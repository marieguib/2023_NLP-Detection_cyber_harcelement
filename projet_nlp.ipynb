{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Classification des avis sur des vêtements de femmes vendus dans le e-commerce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Est ce que les avis que l'on a des vêtements sont représentatifs de la note qui est attribuée ?\n",
    "\n",
    "Idées :\n",
    "- visualisation de données : quels types de vêtements ont les notes les plus élevées ?\n",
    "- nb d'avis donné selon l'âge des clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Import des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps, nous allons importer nos données. Notre base de données contient des informations sur des avis de vêtements femmes vendus sur internet. Ces données sont issus d'un processus de webscrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>767</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Absolutely wonderful - silky and sexy and comf...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Initmates</td>\n",
       "      <td>Intimate</td>\n",
       "      <td>Intimates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1080</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1077</td>\n",
       "      <td>60</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1049</td>\n",
       "      <td>50</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>847</td>\n",
       "      <td>47</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  Clothing ID  Age                    Title  \\\n",
       "0   0          767   33                      NaN   \n",
       "1   1         1080   34                      NaN   \n",
       "2   2         1077   60  Some major design flaws   \n",
       "3   3         1049   50         My favorite buy!   \n",
       "4   4          847   47         Flattering shirt   \n",
       "\n",
       "                                         Review Text  Rating  Recommended IND  \\\n",
       "0  Absolutely wonderful - silky and sexy and comf...       4                1   \n",
       "1  Love this dress!  it's sooo pretty.  i happene...       5                1   \n",
       "2  I had such high hopes for this dress and reall...       3                0   \n",
       "3  I love, love, love this jumpsuit. it's fun, fl...       5                1   \n",
       "4  This shirt is very flattering to all due to th...       5                1   \n",
       "\n",
       "   Positive Feedback Count   Division Name Department Name Class Name  \n",
       "0                        0       Initmates        Intimate  Intimates  \n",
       "1                        4         General         Dresses    Dresses  \n",
       "2                        0         General         Dresses    Dresses  \n",
       "3                        0  General Petite         Bottoms      Pants  \n",
       "4                        6         General            Tops    Blouses  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import des données\n",
    "data = pd.read_csv(\"Womens Clothing E-Commerce Reviews.csv\", sep = \",\")\n",
    "\n",
    "# Renomage première colonne pour pouvoir l'utiliser comme id par la suite\n",
    "data = data.rename(columns = {\"Unnamed: 0\" : \"id\"})\n",
    "\n",
    "# Affichage des 5 premières lignes\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23486, 11)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre jeu de données contient 23 486 avis et 11 colonnes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Pré-traitement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour avoir des données plus propres, nous allons effectuer divers pré-traitements.\n",
    "\n",
    "Pour commencer, nous allons supprimer les lignes où nous avons des valeurs manquantes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19662, 11)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Suppression des valeurs manquantes\n",
    "data = data.dropna()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suite à cette manipulation, nous supprimons environ 4 000 lignes pour pouvoir avoir des données complètes pour poursuivre notre analyse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération des données pour notre étude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traitement de la casse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- suppression des valeurs manquantes => pas d'avis : inutile\n",
    "- suppression des caractères spéciaux \n",
    "- suppression des majuscules\n",
    "- suppression des mots vides\n",
    "- lemmatisation \n",
    "- affiche du nombre de mots par étiquette grammaticale\n",
    "- extraction des mots (groupes de mots) les plus fréquents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps, nous allons récupérer les avis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2        I had such high hopes for this dress and reall...\n",
       "3        I love, love, love this jumpsuit. it's fun, fl...\n",
       "4        This shirt is very flattering to all due to th...\n",
       "5        I love tracy reese dresses, but this one is no...\n",
       "6        I aded this in my basket at hte last mintue to...\n",
       "                               ...                        \n",
       "23481    I was very happy to snag this dress at such a ...\n",
       "23482    It reminds me of maternity clothes. soft, stre...\n",
       "23483    This fit well, but the top was very see throug...\n",
       "23484    I bought this dress for a wedding i have this ...\n",
       "23485    This dress in a lovely platinum is feminine an...\n",
       "Name: Review Text, Length: 19662, dtype: object"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avis = data[\"Review Text\"]\n",
    "avis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(avis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons récupérer seulement la partie textuelle de l'avis, cela nous permet de ne avoir un objet Pandas.Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I had such high hopes for this dress and really wanted it to work for me. i '\n",
      " 'initially ordered the petite small (my usual size) but i found this to be '\n",
      " 'outrageously small. so small in fact that i could not zip it up! i reordered '\n",
      " 'it in petite medium, which was just ok. overall, the top half was '\n",
      " 'comfortable and fit nicely, but the bottom half had a very tight under layer '\n",
      " 'and several somewhat cheap (net) over layers. imo, a major design flaw was '\n",
      " 'the net over layer sewn directly into the zipper - it c',\n",
      " \"I love, love, love this jumpsuit. it's fun, flirty, and fabulous! every time \"\n",
      " 'i wear it, i get nothing but great compliments!',\n",
      " 'This shirt is very flattering to all due to the adjustable front tie. it is '\n",
      " 'the perfect length to wear with leggings and it is sleeveless so it pairs '\n",
      " 'well with any cardigan. love this shirt!!!',\n",
      " 'I love tracy reese dresses, but this one is not for the very petite. i am '\n",
      " 'just under 5 feet tall and usually wear a 0p in this brand. this dress was '\n",
      " 'very pretty out of the package but its a lot of dress. the skirt is long and '\n",
      " 'very full so it overwhelmed my small frame. not a stranger to alterations, '\n",
      " 'shortening and narrowing the skirt would take away from the embellishment of '\n",
      " 'the garment. i love the color and the idea of the style but it just did not '\n",
      " 'work on me. i returned this dress.',\n",
      " 'I aded this in my basket at hte last mintue to see what it would look like '\n",
      " 'in person. (store pick up). i went with teh darkler color only because i am '\n",
      " 'so pale :-) hte color is really gorgeous, and turns out it mathced '\n",
      " 'everythiing i was trying on with it prefectly. it is a little baggy on me '\n",
      " 'and hte xs is hte msallet size (bummer, no petite). i decided to jkeep it '\n",
      " 'though, because as i said, it matvehd everything. my ejans, pants, and the 3 '\n",
      " 'skirts i waas trying on (of which i ]kept all ) oops.',\n",
      " 'I ordered this in carbon for store pick up, and had a ton of stuff (as '\n",
      " 'always) to try on and used this top to pair (skirts and pants). everything '\n",
      " 'went with it. the color is really nice charcoal with shimmer, and went well '\n",
      " 'with pencil skirts, flare pants, etc. my only compaint is it is a bit big, '\n",
      " \"sleeves are long and it doesn't go in petite. also a bit loose for me, but \"\n",
      " 'no xxs... so i kept it and wil ldecide later since the light color is '\n",
      " 'already sold out in hte smallest size...',\n",
      " 'I love this dress. i usually get an xs but it runs a little snug in bust so '\n",
      " 'i ordered up a size. very flattering and feminine with the usual retailer '\n",
      " 'flair for style.',\n",
      " 'I\\'m 5\"5\\' and 125 lbs. i ordered the s petite to make sure the length '\n",
      " \"wasn't too long. i typically wear an xs regular in retailer dresses. if \"\n",
      " \"you're less busty (34b cup or smaller), a s petite will fit you perfectly \"\n",
      " '(snug, but not tight). i love that i could dress it up for a party, or down '\n",
      " 'for work. i love that the tulle is longer then the fabric underneath.',\n",
      " 'Dress runs small esp where the zipper area runs. i ordered the sp which '\n",
      " 'typically fits me and it was very tight! the material on the top looks and '\n",
      " 'feels very cheap that even just pulling on it will cause it to rip the '\n",
      " 'fabric. pretty disappointed as it was going to be my christmas dress this '\n",
      " 'year! needless to say it will be going back.',\n",
      " 'More and more i find myself reliant on the reviews written by savvy shoppers '\n",
      " 'before me and for the most past, they are right on in their estimation of '\n",
      " 'the product. in the case of this dress-if it had not been for the reveiws-i '\n",
      " 'doubt i would have even tried this. the dress is beautifully made, lined and '\n",
      " 'reminiscent of the old retailer quality. it is lined in the solid '\n",
      " 'periwinkle-colored fabric that matches the outer fabric print. tts and very '\n",
      " 'form-fitting. falls just above the knee and does not rid']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "liste_avis = data[\"Review Text\"].values.tolist()\n",
    "pprint(liste_avis[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grâce à cette manipulation, chaque avis est élément d'une liste d'avis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous allons découper les avis en liste de mots et les mettre en minuscules pour pouvoir les analyser plus facilement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i had such high hopes for this dress and really wanted it to work for me. i initially ordered the petite small (my usual size) but i found this to be outrageously small. so small in fact that i could not zip it up! i reordered it in petite medium, which was just ok. overall, the top half was comfortable and fit nicely, but the bottom half had a very tight under layer and several somewhat cheap (net) over layers. imo, a major design flaw was the net over layer sewn directly into the zipper - it c',\n",
       " \"i love, love, love this jumpsuit. it's fun, flirty, and fabulous! every time i wear it, i get nothing but great compliments!\",\n",
       " 'this shirt is very flattering to all due to the adjustable front tie. it is the perfect length to wear with leggings and it is sleeveless so it pairs well with any cardigan. love this shirt!!!',\n",
       " 'i love tracy reese dresses, but this one is not for the very petite. i am just under 5 feet tall and usually wear a 0p in this brand. this dress was very pretty out of the package but its a lot of dress. the skirt is long and very full so it overwhelmed my small frame. not a stranger to alterations, shortening and narrowing the skirt would take away from the embellishment of the garment. i love the color and the idea of the style but it just did not work on me. i returned this dress.',\n",
       " 'i aded this in my basket at hte last mintue to see what it would look like in person. (store pick up). i went with teh darkler color only because i am so pale :-) hte color is really gorgeous, and turns out it mathced everythiing i was trying on with it prefectly. it is a little baggy on me and hte xs is hte msallet size (bummer, no petite). i decided to jkeep it though, because as i said, it matvehd everything. my ejans, pants, and the 3 skirts i waas trying on (of which i ]kept all ) oops.',\n",
       " \"i ordered this in carbon for store pick up, and had a ton of stuff (as always) to try on and used this top to pair (skirts and pants). everything went with it. the color is really nice charcoal with shimmer, and went well with pencil skirts, flare pants, etc. my only compaint is it is a bit big, sleeves are long and it doesn't go in petite. also a bit loose for me, but no xxs... so i kept it and wil ldecide later since the light color is already sold out in hte smallest size...\",\n",
       " 'i love this dress. i usually get an xs but it runs a little snug in bust so i ordered up a size. very flattering and feminine with the usual retailer flair for style.',\n",
       " 'i\\'m 5\"5\\' and 125 lbs. i ordered the s petite to make sure the length wasn\\'t too long. i typically wear an xs regular in retailer dresses. if you\\'re less busty (34b cup or smaller), a s petite will fit you perfectly (snug, but not tight). i love that i could dress it up for a party, or down for work. i love that the tulle is longer then the fabric underneath.',\n",
       " 'dress runs small esp where the zipper area runs. i ordered the sp which typically fits me and it was very tight! the material on the top looks and feels very cheap that even just pulling on it will cause it to rip the fabric. pretty disappointed as it was going to be my christmas dress this year! needless to say it will be going back.',\n",
       " 'more and more i find myself reliant on the reviews written by savvy shoppers before me and for the most past, they are right on in their estimation of the product. in the case of this dress-if it had not been for the reveiws-i doubt i would have even tried this. the dress is beautifully made, lined and reminiscent of the old retailer quality. it is lined in the solid periwinkle-colored fabric that matches the outer fabric print. tts and very form-fitting. falls just above the knee and does not rid']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Découpage des avis en mots\n",
    "\n",
    "liste_avis_clean = []\n",
    "\n",
    "for avis in liste_avis : \n",
    "    avis = str(avis)\n",
    "    avis_clean = avis.split()\n",
    "    avis_clean = avis.lower()\n",
    "    liste_avis_clean.append(avis_clean)\n",
    "\n",
    "liste_avis_clean[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis, nous allons tockeniser notre texte. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i',\n",
       "  'had',\n",
       "  'such',\n",
       "  'high',\n",
       "  'hopes',\n",
       "  'for',\n",
       "  'this',\n",
       "  'dress',\n",
       "  'and',\n",
       "  'really',\n",
       "  'wanted',\n",
       "  'it',\n",
       "  'to',\n",
       "  'work',\n",
       "  'for',\n",
       "  'me',\n",
       "  'i',\n",
       "  'initially',\n",
       "  'ordered',\n",
       "  'the',\n",
       "  'petite',\n",
       "  'small',\n",
       "  'my',\n",
       "  'usual',\n",
       "  'size',\n",
       "  'but',\n",
       "  'i',\n",
       "  'found',\n",
       "  'this',\n",
       "  'to',\n",
       "  'be',\n",
       "  'outrageously',\n",
       "  'small',\n",
       "  'so',\n",
       "  'small',\n",
       "  'in',\n",
       "  'fact',\n",
       "  'that',\n",
       "  'i',\n",
       "  'could',\n",
       "  'not',\n",
       "  'zip',\n",
       "  'it',\n",
       "  'up',\n",
       "  'i',\n",
       "  'reordered',\n",
       "  'it',\n",
       "  'in',\n",
       "  'petite',\n",
       "  'medium',\n",
       "  'which',\n",
       "  'was',\n",
       "  'just',\n",
       "  'ok',\n",
       "  'overall',\n",
       "  'the',\n",
       "  'top',\n",
       "  'half',\n",
       "  'was',\n",
       "  'comfortable',\n",
       "  'and',\n",
       "  'fit',\n",
       "  'nicely',\n",
       "  'but',\n",
       "  'the',\n",
       "  'bottom',\n",
       "  'half',\n",
       "  'had',\n",
       "  'a',\n",
       "  'very',\n",
       "  'tight',\n",
       "  'under',\n",
       "  'layer',\n",
       "  'and',\n",
       "  'several',\n",
       "  'somewhat',\n",
       "  'cheap',\n",
       "  'net',\n",
       "  'over',\n",
       "  'layers',\n",
       "  'imo',\n",
       "  'a',\n",
       "  'major',\n",
       "  'design',\n",
       "  'flaw',\n",
       "  'was',\n",
       "  'the',\n",
       "  'net',\n",
       "  'over',\n",
       "  'layer',\n",
       "  'sewn',\n",
       "  'directly',\n",
       "  'into',\n",
       "  'the',\n",
       "  'zipper',\n",
       "  'it',\n",
       "  'c'],\n",
       " ['i',\n",
       "  'love',\n",
       "  'love',\n",
       "  'love',\n",
       "  'this',\n",
       "  'jumpsuit',\n",
       "  'it',\n",
       "  's',\n",
       "  'fun',\n",
       "  'flirty',\n",
       "  'and',\n",
       "  'fabulous',\n",
       "  'every',\n",
       "  'time',\n",
       "  'i',\n",
       "  'wear',\n",
       "  'it',\n",
       "  'i',\n",
       "  'get',\n",
       "  'nothing',\n",
       "  'but',\n",
       "  'great',\n",
       "  'compliments'],\n",
       " ['this',\n",
       "  'shirt',\n",
       "  'is',\n",
       "  'very',\n",
       "  'flattering',\n",
       "  'to',\n",
       "  'all',\n",
       "  'due',\n",
       "  'to',\n",
       "  'the',\n",
       "  'adjustable',\n",
       "  'front',\n",
       "  'tie',\n",
       "  'it',\n",
       "  'is',\n",
       "  'the',\n",
       "  'perfect',\n",
       "  'length',\n",
       "  'to',\n",
       "  'wear',\n",
       "  'with',\n",
       "  'leggings',\n",
       "  'and',\n",
       "  'it',\n",
       "  'is',\n",
       "  'sleeveless',\n",
       "  'so',\n",
       "  'it',\n",
       "  'pairs',\n",
       "  'well',\n",
       "  'with',\n",
       "  'any',\n",
       "  'cardigan',\n",
       "  'love',\n",
       "  'this',\n",
       "  'shirt'],\n",
       " ['i',\n",
       "  'love',\n",
       "  'tracy',\n",
       "  'reese',\n",
       "  'dresses',\n",
       "  'but',\n",
       "  'this',\n",
       "  'one',\n",
       "  'is',\n",
       "  'not',\n",
       "  'for',\n",
       "  'the',\n",
       "  'very',\n",
       "  'petite',\n",
       "  'i',\n",
       "  'am',\n",
       "  'just',\n",
       "  'under',\n",
       "  '5',\n",
       "  'feet',\n",
       "  'tall',\n",
       "  'and',\n",
       "  'usually',\n",
       "  'wear',\n",
       "  'a',\n",
       "  '0p',\n",
       "  'in',\n",
       "  'this',\n",
       "  'brand',\n",
       "  'this',\n",
       "  'dress',\n",
       "  'was',\n",
       "  'very',\n",
       "  'pretty',\n",
       "  'out',\n",
       "  'of',\n",
       "  'the',\n",
       "  'package',\n",
       "  'but',\n",
       "  'its',\n",
       "  'a',\n",
       "  'lot',\n",
       "  'of',\n",
       "  'dress',\n",
       "  'the',\n",
       "  'skirt',\n",
       "  'is',\n",
       "  'long',\n",
       "  'and',\n",
       "  'very',\n",
       "  'full',\n",
       "  'so',\n",
       "  'it',\n",
       "  'overwhelmed',\n",
       "  'my',\n",
       "  'small',\n",
       "  'frame',\n",
       "  'not',\n",
       "  'a',\n",
       "  'stranger',\n",
       "  'to',\n",
       "  'alterations',\n",
       "  'shortening',\n",
       "  'and',\n",
       "  'narrowing',\n",
       "  'the',\n",
       "  'skirt',\n",
       "  'would',\n",
       "  'take',\n",
       "  'away',\n",
       "  'from',\n",
       "  'the',\n",
       "  'embellishment',\n",
       "  'of',\n",
       "  'the',\n",
       "  'garment',\n",
       "  'i',\n",
       "  'love',\n",
       "  'the',\n",
       "  'color',\n",
       "  'and',\n",
       "  'the',\n",
       "  'idea',\n",
       "  'of',\n",
       "  'the',\n",
       "  'style',\n",
       "  'but',\n",
       "  'it',\n",
       "  'just',\n",
       "  'did',\n",
       "  'not',\n",
       "  'work',\n",
       "  'on',\n",
       "  'me',\n",
       "  'i',\n",
       "  'returned',\n",
       "  'this',\n",
       "  'dress'],\n",
       " ['i',\n",
       "  'aded',\n",
       "  'this',\n",
       "  'in',\n",
       "  'my',\n",
       "  'basket',\n",
       "  'at',\n",
       "  'hte',\n",
       "  'last',\n",
       "  'mintue',\n",
       "  'to',\n",
       "  'see',\n",
       "  'what',\n",
       "  'it',\n",
       "  'would',\n",
       "  'look',\n",
       "  'like',\n",
       "  'in',\n",
       "  'person',\n",
       "  'store',\n",
       "  'pick',\n",
       "  'up',\n",
       "  'i',\n",
       "  'went',\n",
       "  'with',\n",
       "  'teh',\n",
       "  'darkler',\n",
       "  'color',\n",
       "  'only',\n",
       "  'because',\n",
       "  'i',\n",
       "  'am',\n",
       "  'so',\n",
       "  'pale',\n",
       "  'hte',\n",
       "  'color',\n",
       "  'is',\n",
       "  'really',\n",
       "  'gorgeous',\n",
       "  'and',\n",
       "  'turns',\n",
       "  'out',\n",
       "  'it',\n",
       "  'mathced',\n",
       "  'everythiing',\n",
       "  'i',\n",
       "  'was',\n",
       "  'trying',\n",
       "  'on',\n",
       "  'with',\n",
       "  'it',\n",
       "  'prefectly',\n",
       "  'it',\n",
       "  'is',\n",
       "  'a',\n",
       "  'little',\n",
       "  'baggy',\n",
       "  'on',\n",
       "  'me',\n",
       "  'and',\n",
       "  'hte',\n",
       "  'xs',\n",
       "  'is',\n",
       "  'hte',\n",
       "  'msallet',\n",
       "  'size',\n",
       "  'bummer',\n",
       "  'no',\n",
       "  'petite',\n",
       "  'i',\n",
       "  'decided',\n",
       "  'to',\n",
       "  'jkeep',\n",
       "  'it',\n",
       "  'though',\n",
       "  'because',\n",
       "  'as',\n",
       "  'i',\n",
       "  'said',\n",
       "  'it',\n",
       "  'matvehd',\n",
       "  'everything',\n",
       "  'my',\n",
       "  'ejans',\n",
       "  'pants',\n",
       "  'and',\n",
       "  'the',\n",
       "  '3',\n",
       "  'skirts',\n",
       "  'i',\n",
       "  'waas',\n",
       "  'trying',\n",
       "  'on',\n",
       "  'of',\n",
       "  'which',\n",
       "  'i',\n",
       "  'kept',\n",
       "  'all',\n",
       "  'oops'],\n",
       " ['i',\n",
       "  'ordered',\n",
       "  'this',\n",
       "  'in',\n",
       "  'carbon',\n",
       "  'for',\n",
       "  'store',\n",
       "  'pick',\n",
       "  'up',\n",
       "  'and',\n",
       "  'had',\n",
       "  'a',\n",
       "  'ton',\n",
       "  'of',\n",
       "  'stuff',\n",
       "  'as',\n",
       "  'always',\n",
       "  'to',\n",
       "  'try',\n",
       "  'on',\n",
       "  'and',\n",
       "  'used',\n",
       "  'this',\n",
       "  'top',\n",
       "  'to',\n",
       "  'pair',\n",
       "  'skirts',\n",
       "  'and',\n",
       "  'pants',\n",
       "  'everything',\n",
       "  'went',\n",
       "  'with',\n",
       "  'it',\n",
       "  'the',\n",
       "  'color',\n",
       "  'is',\n",
       "  'really',\n",
       "  'nice',\n",
       "  'charcoal',\n",
       "  'with',\n",
       "  'shimmer',\n",
       "  'and',\n",
       "  'went',\n",
       "  'well',\n",
       "  'with',\n",
       "  'pencil',\n",
       "  'skirts',\n",
       "  'flare',\n",
       "  'pants',\n",
       "  'etc',\n",
       "  'my',\n",
       "  'only',\n",
       "  'compaint',\n",
       "  'is',\n",
       "  'it',\n",
       "  'is',\n",
       "  'a',\n",
       "  'bit',\n",
       "  'big',\n",
       "  'sleeves',\n",
       "  'are',\n",
       "  'long',\n",
       "  'and',\n",
       "  'it',\n",
       "  'doesn',\n",
       "  't',\n",
       "  'go',\n",
       "  'in',\n",
       "  'petite',\n",
       "  'also',\n",
       "  'a',\n",
       "  'bit',\n",
       "  'loose',\n",
       "  'for',\n",
       "  'me',\n",
       "  'but',\n",
       "  'no',\n",
       "  'xxs',\n",
       "  'so',\n",
       "  'i',\n",
       "  'kept',\n",
       "  'it',\n",
       "  'and',\n",
       "  'wil',\n",
       "  'ldecide',\n",
       "  'later',\n",
       "  'since',\n",
       "  'the',\n",
       "  'light',\n",
       "  'color',\n",
       "  'is',\n",
       "  'already',\n",
       "  'sold',\n",
       "  'out',\n",
       "  'in',\n",
       "  'hte',\n",
       "  'smallest',\n",
       "  'size'],\n",
       " ['i',\n",
       "  'love',\n",
       "  'this',\n",
       "  'dress',\n",
       "  'i',\n",
       "  'usually',\n",
       "  'get',\n",
       "  'an',\n",
       "  'xs',\n",
       "  'but',\n",
       "  'it',\n",
       "  'runs',\n",
       "  'a',\n",
       "  'little',\n",
       "  'snug',\n",
       "  'in',\n",
       "  'bust',\n",
       "  'so',\n",
       "  'i',\n",
       "  'ordered',\n",
       "  'up',\n",
       "  'a',\n",
       "  'size',\n",
       "  'very',\n",
       "  'flattering',\n",
       "  'and',\n",
       "  'feminine',\n",
       "  'with',\n",
       "  'the',\n",
       "  'usual',\n",
       "  'retailer',\n",
       "  'flair',\n",
       "  'for',\n",
       "  'style'],\n",
       " ['i',\n",
       "  'm',\n",
       "  '5',\n",
       "  '5',\n",
       "  'and',\n",
       "  '125',\n",
       "  'lbs',\n",
       "  'i',\n",
       "  'ordered',\n",
       "  'the',\n",
       "  's',\n",
       "  'petite',\n",
       "  'to',\n",
       "  'make',\n",
       "  'sure',\n",
       "  'the',\n",
       "  'length',\n",
       "  'wasn',\n",
       "  't',\n",
       "  'too',\n",
       "  'long',\n",
       "  'i',\n",
       "  'typically',\n",
       "  'wear',\n",
       "  'an',\n",
       "  'xs',\n",
       "  'regular',\n",
       "  'in',\n",
       "  'retailer',\n",
       "  'dresses',\n",
       "  'if',\n",
       "  'you',\n",
       "  're',\n",
       "  'less',\n",
       "  'busty',\n",
       "  '34b',\n",
       "  'cup',\n",
       "  'or',\n",
       "  'smaller',\n",
       "  'a',\n",
       "  's',\n",
       "  'petite',\n",
       "  'will',\n",
       "  'fit',\n",
       "  'you',\n",
       "  'perfectly',\n",
       "  'snug',\n",
       "  'but',\n",
       "  'not',\n",
       "  'tight',\n",
       "  'i',\n",
       "  'love',\n",
       "  'that',\n",
       "  'i',\n",
       "  'could',\n",
       "  'dress',\n",
       "  'it',\n",
       "  'up',\n",
       "  'for',\n",
       "  'a',\n",
       "  'party',\n",
       "  'or',\n",
       "  'down',\n",
       "  'for',\n",
       "  'work',\n",
       "  'i',\n",
       "  'love',\n",
       "  'that',\n",
       "  'the',\n",
       "  'tulle',\n",
       "  'is',\n",
       "  'longer',\n",
       "  'then',\n",
       "  'the',\n",
       "  'fabric',\n",
       "  'underneath'],\n",
       " ['dress',\n",
       "  'runs',\n",
       "  'small',\n",
       "  'esp',\n",
       "  'where',\n",
       "  'the',\n",
       "  'zipper',\n",
       "  'area',\n",
       "  'runs',\n",
       "  'i',\n",
       "  'ordered',\n",
       "  'the',\n",
       "  'sp',\n",
       "  'which',\n",
       "  'typically',\n",
       "  'fits',\n",
       "  'me',\n",
       "  'and',\n",
       "  'it',\n",
       "  'was',\n",
       "  'very',\n",
       "  'tight',\n",
       "  'the',\n",
       "  'material',\n",
       "  'on',\n",
       "  'the',\n",
       "  'top',\n",
       "  'looks',\n",
       "  'and',\n",
       "  'feels',\n",
       "  'very',\n",
       "  'cheap',\n",
       "  'that',\n",
       "  'even',\n",
       "  'just',\n",
       "  'pulling',\n",
       "  'on',\n",
       "  'it',\n",
       "  'will',\n",
       "  'cause',\n",
       "  'it',\n",
       "  'to',\n",
       "  'rip',\n",
       "  'the',\n",
       "  'fabric',\n",
       "  'pretty',\n",
       "  'disappointed',\n",
       "  'as',\n",
       "  'it',\n",
       "  'was',\n",
       "  'going',\n",
       "  'to',\n",
       "  'be',\n",
       "  'my',\n",
       "  'christmas',\n",
       "  'dress',\n",
       "  'this',\n",
       "  'year',\n",
       "  'needless',\n",
       "  'to',\n",
       "  'say',\n",
       "  'it',\n",
       "  'will',\n",
       "  'be',\n",
       "  'going',\n",
       "  'back'],\n",
       " ['more',\n",
       "  'and',\n",
       "  'more',\n",
       "  'i',\n",
       "  'find',\n",
       "  'myself',\n",
       "  'reliant',\n",
       "  'on',\n",
       "  'the',\n",
       "  'reviews',\n",
       "  'written',\n",
       "  'by',\n",
       "  'savvy',\n",
       "  'shoppers',\n",
       "  'before',\n",
       "  'me',\n",
       "  'and',\n",
       "  'for',\n",
       "  'the',\n",
       "  'most',\n",
       "  'past',\n",
       "  'they',\n",
       "  'are',\n",
       "  'right',\n",
       "  'on',\n",
       "  'in',\n",
       "  'their',\n",
       "  'estimation',\n",
       "  'of',\n",
       "  'the',\n",
       "  'product',\n",
       "  'in',\n",
       "  'the',\n",
       "  'case',\n",
       "  'of',\n",
       "  'this',\n",
       "  'dress',\n",
       "  'if',\n",
       "  'it',\n",
       "  'had',\n",
       "  'not',\n",
       "  'been',\n",
       "  'for',\n",
       "  'the',\n",
       "  'reveiws',\n",
       "  'i',\n",
       "  'doubt',\n",
       "  'i',\n",
       "  'would',\n",
       "  'have',\n",
       "  'even',\n",
       "  'tried',\n",
       "  'this',\n",
       "  'the',\n",
       "  'dress',\n",
       "  'is',\n",
       "  'beautifully',\n",
       "  'made',\n",
       "  'lined',\n",
       "  'and',\n",
       "  'reminiscent',\n",
       "  'of',\n",
       "  'the',\n",
       "  'old',\n",
       "  'retailer',\n",
       "  'quality',\n",
       "  'it',\n",
       "  'is',\n",
       "  'lined',\n",
       "  'in',\n",
       "  'the',\n",
       "  'solid',\n",
       "  'periwinkle',\n",
       "  'colored',\n",
       "  'fabric',\n",
       "  'that',\n",
       "  'matches',\n",
       "  'the',\n",
       "  'outer',\n",
       "  'fabric',\n",
       "  'print',\n",
       "  'tts',\n",
       "  'and',\n",
       "  'very',\n",
       "  'form',\n",
       "  'fitting',\n",
       "  'falls',\n",
       "  'just',\n",
       "  'above',\n",
       "  'the',\n",
       "  'knee',\n",
       "  'and',\n",
       "  'does',\n",
       "  'not',\n",
       "  'rid']]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer('\\w+')\n",
    "liste_avis_clean = [tokenizer.tokenize(str(avis)) for avis in liste_avis_clean]\n",
    "liste_avis_clean[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, on supprime la ponctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i',\n",
       "  'had',\n",
       "  'such',\n",
       "  'high',\n",
       "  'hopes',\n",
       "  'for',\n",
       "  'this',\n",
       "  'dress',\n",
       "  'and',\n",
       "  'really',\n",
       "  'wanted',\n",
       "  'it',\n",
       "  'to',\n",
       "  'work',\n",
       "  'for',\n",
       "  'me',\n",
       "  'i',\n",
       "  'initially',\n",
       "  'ordered',\n",
       "  'the',\n",
       "  'petite',\n",
       "  'small',\n",
       "  'my',\n",
       "  'usual',\n",
       "  'size',\n",
       "  'but',\n",
       "  'i',\n",
       "  'found',\n",
       "  'this',\n",
       "  'to',\n",
       "  'be',\n",
       "  'outrageously',\n",
       "  'small',\n",
       "  'so',\n",
       "  'small',\n",
       "  'in',\n",
       "  'fact',\n",
       "  'that',\n",
       "  'i',\n",
       "  'could',\n",
       "  'not',\n",
       "  'zip',\n",
       "  'it',\n",
       "  'up',\n",
       "  'i',\n",
       "  'reordered',\n",
       "  'it',\n",
       "  'in',\n",
       "  'petite',\n",
       "  'medium',\n",
       "  'which',\n",
       "  'was',\n",
       "  'just',\n",
       "  'ok',\n",
       "  'overall',\n",
       "  'the',\n",
       "  'top',\n",
       "  'half',\n",
       "  'was',\n",
       "  'comfortable',\n",
       "  'and',\n",
       "  'fit',\n",
       "  'nicely',\n",
       "  'but',\n",
       "  'the',\n",
       "  'bottom',\n",
       "  'half',\n",
       "  'had',\n",
       "  'a',\n",
       "  'very',\n",
       "  'tight',\n",
       "  'under',\n",
       "  'layer',\n",
       "  'and',\n",
       "  'several',\n",
       "  'somewhat',\n",
       "  'cheap',\n",
       "  'net',\n",
       "  'over',\n",
       "  'layers',\n",
       "  'imo',\n",
       "  'a',\n",
       "  'major',\n",
       "  'design',\n",
       "  'flaw',\n",
       "  'was',\n",
       "  'the',\n",
       "  'net',\n",
       "  'over',\n",
       "  'layer',\n",
       "  'sewn',\n",
       "  'directly',\n",
       "  'into',\n",
       "  'the',\n",
       "  'zipper',\n",
       "  'it',\n",
       "  'c'],\n",
       " ['i',\n",
       "  'love',\n",
       "  'love',\n",
       "  'love',\n",
       "  'this',\n",
       "  'jumpsuit',\n",
       "  'it',\n",
       "  's',\n",
       "  'fun',\n",
       "  'flirty',\n",
       "  'and',\n",
       "  'fabulous',\n",
       "  'every',\n",
       "  'time',\n",
       "  'i',\n",
       "  'wear',\n",
       "  'it',\n",
       "  'i',\n",
       "  'get',\n",
       "  'nothing',\n",
       "  'but',\n",
       "  'great',\n",
       "  'compliments'],\n",
       " ['this',\n",
       "  'shirt',\n",
       "  'is',\n",
       "  'very',\n",
       "  'flattering',\n",
       "  'to',\n",
       "  'all',\n",
       "  'due',\n",
       "  'to',\n",
       "  'the',\n",
       "  'adjustable',\n",
       "  'front',\n",
       "  'tie',\n",
       "  'it',\n",
       "  'is',\n",
       "  'the',\n",
       "  'perfect',\n",
       "  'length',\n",
       "  'to',\n",
       "  'wear',\n",
       "  'with',\n",
       "  'leggings',\n",
       "  'and',\n",
       "  'it',\n",
       "  'is',\n",
       "  'sleeveless',\n",
       "  'so',\n",
       "  'it',\n",
       "  'pairs',\n",
       "  'well',\n",
       "  'with',\n",
       "  'any',\n",
       "  'cardigan',\n",
       "  'love',\n",
       "  'this',\n",
       "  'shirt'],\n",
       " ['i',\n",
       "  'love',\n",
       "  'tracy',\n",
       "  'reese',\n",
       "  'dresses',\n",
       "  'but',\n",
       "  'this',\n",
       "  'one',\n",
       "  'is',\n",
       "  'not',\n",
       "  'for',\n",
       "  'the',\n",
       "  'very',\n",
       "  'petite',\n",
       "  'i',\n",
       "  'am',\n",
       "  'just',\n",
       "  'under',\n",
       "  '5',\n",
       "  'feet',\n",
       "  'tall',\n",
       "  'and',\n",
       "  'usually',\n",
       "  'wear',\n",
       "  'a',\n",
       "  '0p',\n",
       "  'in',\n",
       "  'this',\n",
       "  'brand',\n",
       "  'this',\n",
       "  'dress',\n",
       "  'was',\n",
       "  'very',\n",
       "  'pretty',\n",
       "  'out',\n",
       "  'of',\n",
       "  'the',\n",
       "  'package',\n",
       "  'but',\n",
       "  'its',\n",
       "  'a',\n",
       "  'lot',\n",
       "  'of',\n",
       "  'dress',\n",
       "  'the',\n",
       "  'skirt',\n",
       "  'is',\n",
       "  'long',\n",
       "  'and',\n",
       "  'very',\n",
       "  'full',\n",
       "  'so',\n",
       "  'it',\n",
       "  'overwhelmed',\n",
       "  'my',\n",
       "  'small',\n",
       "  'frame',\n",
       "  'not',\n",
       "  'a',\n",
       "  'stranger',\n",
       "  'to',\n",
       "  'alterations',\n",
       "  'shortening',\n",
       "  'and',\n",
       "  'narrowing',\n",
       "  'the',\n",
       "  'skirt',\n",
       "  'would',\n",
       "  'take',\n",
       "  'away',\n",
       "  'from',\n",
       "  'the',\n",
       "  'embellishment',\n",
       "  'of',\n",
       "  'the',\n",
       "  'garment',\n",
       "  'i',\n",
       "  'love',\n",
       "  'the',\n",
       "  'color',\n",
       "  'and',\n",
       "  'the',\n",
       "  'idea',\n",
       "  'of',\n",
       "  'the',\n",
       "  'style',\n",
       "  'but',\n",
       "  'it',\n",
       "  'just',\n",
       "  'did',\n",
       "  'not',\n",
       "  'work',\n",
       "  'on',\n",
       "  'me',\n",
       "  'i',\n",
       "  'returned',\n",
       "  'this',\n",
       "  'dress'],\n",
       " ['i',\n",
       "  'aded',\n",
       "  'this',\n",
       "  'in',\n",
       "  'my',\n",
       "  'basket',\n",
       "  'at',\n",
       "  'hte',\n",
       "  'last',\n",
       "  'mintue',\n",
       "  'to',\n",
       "  'see',\n",
       "  'what',\n",
       "  'it',\n",
       "  'would',\n",
       "  'look',\n",
       "  'like',\n",
       "  'in',\n",
       "  'person',\n",
       "  'store',\n",
       "  'pick',\n",
       "  'up',\n",
       "  'i',\n",
       "  'went',\n",
       "  'with',\n",
       "  'teh',\n",
       "  'darkler',\n",
       "  'color',\n",
       "  'only',\n",
       "  'because',\n",
       "  'i',\n",
       "  'am',\n",
       "  'so',\n",
       "  'pale',\n",
       "  'hte',\n",
       "  'color',\n",
       "  'is',\n",
       "  'really',\n",
       "  'gorgeous',\n",
       "  'and',\n",
       "  'turns',\n",
       "  'out',\n",
       "  'it',\n",
       "  'mathced',\n",
       "  'everythiing',\n",
       "  'i',\n",
       "  'was',\n",
       "  'trying',\n",
       "  'on',\n",
       "  'with',\n",
       "  'it',\n",
       "  'prefectly',\n",
       "  'it',\n",
       "  'is',\n",
       "  'a',\n",
       "  'little',\n",
       "  'baggy',\n",
       "  'on',\n",
       "  'me',\n",
       "  'and',\n",
       "  'hte',\n",
       "  'xs',\n",
       "  'is',\n",
       "  'hte',\n",
       "  'msallet',\n",
       "  'size',\n",
       "  'bummer',\n",
       "  'no',\n",
       "  'petite',\n",
       "  'i',\n",
       "  'decided',\n",
       "  'to',\n",
       "  'jkeep',\n",
       "  'it',\n",
       "  'though',\n",
       "  'because',\n",
       "  'as',\n",
       "  'i',\n",
       "  'said',\n",
       "  'it',\n",
       "  'matvehd',\n",
       "  'everything',\n",
       "  'my',\n",
       "  'ejans',\n",
       "  'pants',\n",
       "  'and',\n",
       "  'the',\n",
       "  '3',\n",
       "  'skirts',\n",
       "  'i',\n",
       "  'waas',\n",
       "  'trying',\n",
       "  'on',\n",
       "  'of',\n",
       "  'which',\n",
       "  'i',\n",
       "  'kept',\n",
       "  'all',\n",
       "  'oops'],\n",
       " ['i',\n",
       "  'ordered',\n",
       "  'this',\n",
       "  'in',\n",
       "  'carbon',\n",
       "  'for',\n",
       "  'store',\n",
       "  'pick',\n",
       "  'up',\n",
       "  'and',\n",
       "  'had',\n",
       "  'a',\n",
       "  'ton',\n",
       "  'of',\n",
       "  'stuff',\n",
       "  'as',\n",
       "  'always',\n",
       "  'to',\n",
       "  'try',\n",
       "  'on',\n",
       "  'and',\n",
       "  'used',\n",
       "  'this',\n",
       "  'top',\n",
       "  'to',\n",
       "  'pair',\n",
       "  'skirts',\n",
       "  'and',\n",
       "  'pants',\n",
       "  'everything',\n",
       "  'went',\n",
       "  'with',\n",
       "  'it',\n",
       "  'the',\n",
       "  'color',\n",
       "  'is',\n",
       "  'really',\n",
       "  'nice',\n",
       "  'charcoal',\n",
       "  'with',\n",
       "  'shimmer',\n",
       "  'and',\n",
       "  'went',\n",
       "  'well',\n",
       "  'with',\n",
       "  'pencil',\n",
       "  'skirts',\n",
       "  'flare',\n",
       "  'pants',\n",
       "  'etc',\n",
       "  'my',\n",
       "  'only',\n",
       "  'compaint',\n",
       "  'is',\n",
       "  'it',\n",
       "  'is',\n",
       "  'a',\n",
       "  'bit',\n",
       "  'big',\n",
       "  'sleeves',\n",
       "  'are',\n",
       "  'long',\n",
       "  'and',\n",
       "  'it',\n",
       "  'doesn',\n",
       "  't',\n",
       "  'go',\n",
       "  'in',\n",
       "  'petite',\n",
       "  'also',\n",
       "  'a',\n",
       "  'bit',\n",
       "  'loose',\n",
       "  'for',\n",
       "  'me',\n",
       "  'but',\n",
       "  'no',\n",
       "  'xxs',\n",
       "  'so',\n",
       "  'i',\n",
       "  'kept',\n",
       "  'it',\n",
       "  'and',\n",
       "  'wil',\n",
       "  'ldecide',\n",
       "  'later',\n",
       "  'since',\n",
       "  'the',\n",
       "  'light',\n",
       "  'color',\n",
       "  'is',\n",
       "  'already',\n",
       "  'sold',\n",
       "  'out',\n",
       "  'in',\n",
       "  'hte',\n",
       "  'smallest',\n",
       "  'size'],\n",
       " ['i',\n",
       "  'love',\n",
       "  'this',\n",
       "  'dress',\n",
       "  'i',\n",
       "  'usually',\n",
       "  'get',\n",
       "  'an',\n",
       "  'xs',\n",
       "  'but',\n",
       "  'it',\n",
       "  'runs',\n",
       "  'a',\n",
       "  'little',\n",
       "  'snug',\n",
       "  'in',\n",
       "  'bust',\n",
       "  'so',\n",
       "  'i',\n",
       "  'ordered',\n",
       "  'up',\n",
       "  'a',\n",
       "  'size',\n",
       "  'very',\n",
       "  'flattering',\n",
       "  'and',\n",
       "  'feminine',\n",
       "  'with',\n",
       "  'the',\n",
       "  'usual',\n",
       "  'retailer',\n",
       "  'flair',\n",
       "  'for',\n",
       "  'style'],\n",
       " ['i',\n",
       "  'm',\n",
       "  '5',\n",
       "  '5',\n",
       "  'and',\n",
       "  '125',\n",
       "  'lbs',\n",
       "  'i',\n",
       "  'ordered',\n",
       "  'the',\n",
       "  's',\n",
       "  'petite',\n",
       "  'to',\n",
       "  'make',\n",
       "  'sure',\n",
       "  'the',\n",
       "  'length',\n",
       "  'wasn',\n",
       "  't',\n",
       "  'too',\n",
       "  'long',\n",
       "  'i',\n",
       "  'typically',\n",
       "  'wear',\n",
       "  'an',\n",
       "  'xs',\n",
       "  'regular',\n",
       "  'in',\n",
       "  'retailer',\n",
       "  'dresses',\n",
       "  'if',\n",
       "  'you',\n",
       "  're',\n",
       "  'less',\n",
       "  'busty',\n",
       "  '34b',\n",
       "  'cup',\n",
       "  'or',\n",
       "  'smaller',\n",
       "  'a',\n",
       "  's',\n",
       "  'petite',\n",
       "  'will',\n",
       "  'fit',\n",
       "  'you',\n",
       "  'perfectly',\n",
       "  'snug',\n",
       "  'but',\n",
       "  'not',\n",
       "  'tight',\n",
       "  'i',\n",
       "  'love',\n",
       "  'that',\n",
       "  'i',\n",
       "  'could',\n",
       "  'dress',\n",
       "  'it',\n",
       "  'up',\n",
       "  'for',\n",
       "  'a',\n",
       "  'party',\n",
       "  'or',\n",
       "  'down',\n",
       "  'for',\n",
       "  'work',\n",
       "  'i',\n",
       "  'love',\n",
       "  'that',\n",
       "  'the',\n",
       "  'tulle',\n",
       "  'is',\n",
       "  'longer',\n",
       "  'then',\n",
       "  'the',\n",
       "  'fabric',\n",
       "  'underneath'],\n",
       " ['dress',\n",
       "  'runs',\n",
       "  'small',\n",
       "  'esp',\n",
       "  'where',\n",
       "  'the',\n",
       "  'zipper',\n",
       "  'area',\n",
       "  'runs',\n",
       "  'i',\n",
       "  'ordered',\n",
       "  'the',\n",
       "  'sp',\n",
       "  'which',\n",
       "  'typically',\n",
       "  'fits',\n",
       "  'me',\n",
       "  'and',\n",
       "  'it',\n",
       "  'was',\n",
       "  'very',\n",
       "  'tight',\n",
       "  'the',\n",
       "  'material',\n",
       "  'on',\n",
       "  'the',\n",
       "  'top',\n",
       "  'looks',\n",
       "  'and',\n",
       "  'feels',\n",
       "  'very',\n",
       "  'cheap',\n",
       "  'that',\n",
       "  'even',\n",
       "  'just',\n",
       "  'pulling',\n",
       "  'on',\n",
       "  'it',\n",
       "  'will',\n",
       "  'cause',\n",
       "  'it',\n",
       "  'to',\n",
       "  'rip',\n",
       "  'the',\n",
       "  'fabric',\n",
       "  'pretty',\n",
       "  'disappointed',\n",
       "  'as',\n",
       "  'it',\n",
       "  'was',\n",
       "  'going',\n",
       "  'to',\n",
       "  'be',\n",
       "  'my',\n",
       "  'christmas',\n",
       "  'dress',\n",
       "  'this',\n",
       "  'year',\n",
       "  'needless',\n",
       "  'to',\n",
       "  'say',\n",
       "  'it',\n",
       "  'will',\n",
       "  'be',\n",
       "  'going',\n",
       "  'back'],\n",
       " ['more',\n",
       "  'and',\n",
       "  'more',\n",
       "  'i',\n",
       "  'find',\n",
       "  'myself',\n",
       "  'reliant',\n",
       "  'on',\n",
       "  'the',\n",
       "  'reviews',\n",
       "  'written',\n",
       "  'by',\n",
       "  'savvy',\n",
       "  'shoppers',\n",
       "  'before',\n",
       "  'me',\n",
       "  'and',\n",
       "  'for',\n",
       "  'the',\n",
       "  'most',\n",
       "  'past',\n",
       "  'they',\n",
       "  'are',\n",
       "  'right',\n",
       "  'on',\n",
       "  'in',\n",
       "  'their',\n",
       "  'estimation',\n",
       "  'of',\n",
       "  'the',\n",
       "  'product',\n",
       "  'in',\n",
       "  'the',\n",
       "  'case',\n",
       "  'of',\n",
       "  'this',\n",
       "  'dress',\n",
       "  'if',\n",
       "  'it',\n",
       "  'had',\n",
       "  'not',\n",
       "  'been',\n",
       "  'for',\n",
       "  'the',\n",
       "  'reveiws',\n",
       "  'i',\n",
       "  'doubt',\n",
       "  'i',\n",
       "  'would',\n",
       "  'have',\n",
       "  'even',\n",
       "  'tried',\n",
       "  'this',\n",
       "  'the',\n",
       "  'dress',\n",
       "  'is',\n",
       "  'beautifully',\n",
       "  'made',\n",
       "  'lined',\n",
       "  'and',\n",
       "  'reminiscent',\n",
       "  'of',\n",
       "  'the',\n",
       "  'old',\n",
       "  'retailer',\n",
       "  'quality',\n",
       "  'it',\n",
       "  'is',\n",
       "  'lined',\n",
       "  'in',\n",
       "  'the',\n",
       "  'solid',\n",
       "  'periwinkle',\n",
       "  'colored',\n",
       "  'fabric',\n",
       "  'that',\n",
       "  'matches',\n",
       "  'the',\n",
       "  'outer',\n",
       "  'fabric',\n",
       "  'print',\n",
       "  'tts',\n",
       "  'and',\n",
       "  'very',\n",
       "  'form',\n",
       "  'fitting',\n",
       "  'falls',\n",
       "  'just',\n",
       "  'above',\n",
       "  'the',\n",
       "  'knee',\n",
       "  'and',\n",
       "  'does',\n",
       "  'not',\n",
       "  'rid']]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "punct = string.punctuation\n",
    "\n",
    "# Pour chaque token dans chaque avis, si le token n'est pas dans la liste des ponctuations, on le garde\n",
    "liste_avis_clean = [[token for token in avis if token not in punct] for avis in liste_avis_clean]\n",
    "liste_avis_clean[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traitement et séparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie, nous allons chercher à classifier les avis en fonction de leur note. \n",
    "\n",
    "Nous allons utiliser la colonne \"Rating\" comme étiquettes et \"Review Text\" comme valeurs. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sélection des informations dans notre dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons sélectionner les trois colonnes qui vont nous servir pour la classification dans un objectif d'optimiser les temps de calculs et de ne pas avoir d'informations superflus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>I love tracy reese dresses, but this one is no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>I aded this in my basket at hte last mintue to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  Rating                                        Review Text\n",
       "2   2       3  I had such high hopes for this dress and reall...\n",
       "3   3       5  I love, love, love this jumpsuit. it's fun, fl...\n",
       "4   4       5  This shirt is very flattering to all due to th...\n",
       "5   5       2  I love tracy reese dresses, but this one is no...\n",
       "6   6       5  I aded this in my basket at hte last mintue to..."
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On récupère la colonne id, Rating et Review Text\n",
    "data = data[[\"id\", \"Rating\", \"Review Text\"]]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse de la colonne \"Rating\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rating\n",
       "5    10858\n",
       "4     4289\n",
       "3     2464\n",
       "2     1360\n",
       "1      691\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyse de la colonne \"Rating\"\n",
    "data[\"Rating\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons ici des notes allant de 1 à 5. Nous allons diviser ces valeurs en 3 catégories : \n",
    "- -1 pour les notes allant de 1 à 2\n",
    "- 0 pour les notes égales à 3 \n",
    "- 1 pour les plus élevées (4 et 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse de la colonne \"Review Text\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre colonne correspondant aux valeurs est \"Review Text\". \\\n",
    "Cette colonne contient tous les avis laissés par les internautes sur les différents vêtements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changement des étiquettes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour réaliser notre classification, nous allons donc modifier les étiquettes comme précisé ci-dessus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_label_to_numeric(label):\n",
    "    return 1 if label == 5 else 0 if label == 3 or label == 4 else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(data):\n",
    "    labels = data[[\"id\",\"Rating\"]]\n",
    "    labels['Rating'] = labels['Rating'].apply(map_label_to_numeric)\n",
    "    labels.set_index('id', inplace=True)\n",
    "    \n",
    "    # ajouter les labels dans data selon l'id\n",
    "    data['score_avis'] = labels\n",
    "\n",
    "    # data['score_avis'] = labels\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_7908\\29256686.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labels['Rating'] = labels['Rating'].apply(map_label_to_numeric)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>score_avis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>I love tracy reese dresses, but this one is no...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>I aded this in my basket at hte last mintue to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  Rating                                        Review Text  score_avis\n",
       "2   2       3  I had such high hopes for this dress and reall...           0\n",
       "3   3       5  I love, love, love this jumpsuit. it's fun, fl...           1\n",
       "4   4       5  This shirt is very flattering to all due to th...           1\n",
       "5   5       2  I love tracy reese dresses, but this one is no...          -1\n",
       "6   6       5  I aded this in my basket at hte last mintue to...           1"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = get_labels(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score_avis\n",
       " 1    10858\n",
       " 0     6753\n",
       "-1     2051\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On analyse la nouvelle colonne \"score_avis\"\n",
    "data[\"score_avis\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grâce à cette manipulation, nous pouvons observer que les avis ayant la note de 5 sont majoritaires dans notre jeu de données puisque cela correspond à la note de 1. Les avis compris entre 1 et 2 ont une proportion plus faible (-1). \n",
    "\n",
    "A présent, nous n'avons plus besoin de la colonne Rating, nous pouvons donc la supprimer du dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>score_avis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>I love tracy reese dresses, but this one is no...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>I aded this in my basket at hte last mintue to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                        Review Text  score_avis\n",
       "2   2  I had such high hopes for this dress and reall...           0\n",
       "3   3  I love, love, love this jumpsuit. it's fun, fl...           1\n",
       "4   4  This shirt is very flattering to all due to th...           1\n",
       "5   5  I love tracy reese dresses, but this one is no...          -1\n",
       "6   6  I aded this in my basket at hte last mintue to...           1"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[[\"id\", \"Review Text\", \"score_avis\"]]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Division de notre dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour réaliser notre classification, nous avons besoin de séparer notre jeu de données en un jeu d'apprentissage, de validation et de test.\n",
    "\n",
    "Note :\n",
    "Les données en apprentissage automatique sont généralement séparées en trois jeux :\n",
    "+ **entraînement** : données destinées à l'apprentissage du modèle ;\n",
    "+ **validation** : données destinées à une évaluation intermédiaire du modèle pour permettre l'ajustement de ses hyperparamètres. Une fois les hyperparamètres du modèle arrêtés, on peut le ré-entraîner sur l'ensemble des données (entraînement + validation) avant de le tester sur le jeu de test ;\n",
    "+ **test** : données destinées EXCLUSIVEMENT à l'évaluation FINALE (à réaliser une fois uniquement !) du modèle choisi finalement. Elles ne doivent sous aucune forme servir à la conception du modèle. Il est donc interdit aussi bien de les examiner que d'évaluer le modèle en cours de développement sur ce jeu de données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour créer l'ensemble de validation, nous allons effectuer la manipulation à la fin du pré-traitement réalisé lors de la classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour diviser de notre jeu de données en 2 : train et test\n",
    "def split_data(data, train_ratio):\n",
    "    data_train = data.sample(frac = train_ratio)\n",
    "    data_test = data.drop(data_train.index)\n",
    "    return data_train, data_test\n",
    "\n",
    "# Diviser notre jeu de données en 2 : train et test\n",
    "data_train, data_test = split_data(data, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11797, 3), (7865, 3))"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape, data_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans notre cas :\n",
    "+ entraînement (appelé *Train*) contenant 11797 observations ;\n",
    "+ validation (appelé *Validation*) contenant 5243 observations ;\n",
    "+ test (appelé *Test*), contenant 2622 observations, soit environ 22% de la taille du jeu d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>score_avis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10556</th>\n",
       "      <td>10556</td>\n",
       "      <td>I love this top! it's so hard for me to find c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3282</th>\n",
       "      <td>3282</td>\n",
       "      <td>I absolutely fell in love with the design of t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14016</th>\n",
       "      <td>14016</td>\n",
       "      <td>This dress is beautiful--lovely colors and des...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19217</th>\n",
       "      <td>19217</td>\n",
       "      <td>Cute top. definitely sheer so need an undershi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14821</th>\n",
       "      <td>14821</td>\n",
       "      <td>I was just at the store and purchased this cut...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                        Review Text  score_avis\n",
       "10556  10556  I love this top! it's so hard for me to find c...           1\n",
       "3282    3282  I absolutely fell in love with the design of t...           0\n",
       "14016  14016  This dress is beautiful--lovely colors and des...           0\n",
       "19217  19217  Cute top. definitely sheer so need an undershi...           0\n",
       "14821  14821  I was just at the store and purchased this cut...           1"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>score_avis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>I love tracy reese dresses, but this one is no...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>I aded this in my basket at hte last mintue to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>I ordered this in carbon for store pick up, an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                        Review Text  score_avis\n",
       "3   3  I love, love, love this jumpsuit. it's fun, fl...           1\n",
       "4   4  This shirt is very flattering to all due to th...           1\n",
       "5   5  I love tracy reese dresses, but this one is no...          -1\n",
       "6   6  I aded this in my basket at hte last mintue to...           1\n",
       "7   7  I ordered this in carbon for store pick up, an...           0"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution des classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est important de connaître la répartition des classes dans les données d'entraînement pour pouvoir procéder à notre classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_avis\n",
      " 1    6497\n",
      " 0    4075\n",
      "-1    1225\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "score_avis\n",
       " 1    0.550733\n",
       " 0    0.345427\n",
       "-1    0.103840\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyse de la colonne \"score_avis\" de notre jeu de données d'entrainement\n",
    "print(data_train[\"score_avis\"].value_counts())\n",
    "\n",
    "# Calcul des proportions de chaque classe dans notre jeu de données d'entrainement\n",
    "data_train[\"score_avis\"].value_counts()/len(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons observer que les classes de scores sont réparties de manière aléatoire dans notre jeu d'apprentissage. Nous pouvons noter plus de 50% d'avis très favorables, correspondant à la note de 5/5. Les avis négatifs sont en minorité dans notre jeu d'entraînement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration du texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour se faire une idée des textes auxquels nous avons affaire, nous allons les afficher pour savoir quels pré-traitements sont nécessaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"I love this top! it's so hard for me to find conservative, comfortable and yet not mom-isn swimsuits. this top is amazing. it's comfortable and soft and great quality. i'm excited to use this suit this season!\",\n",
       "       'I absolutely fell in love with the design of this dress! the colors and cut are easy to wear but unique enough to stand out. i was between the medium and the small and ultimately went with the medium. the dress hits right above my knee, fits nicely at my waist, and is well made. the top does seem a bit loose (as pictured on the model) but i prefer a slightly more fitted look. a padded bra would probably have filled out the extra fabric but it was an easy alteration and will allow me to wear the',\n",
       "       \"This dress is beautiful--lovely colors and design--but it's huuuge. i'm usually a s/m in tops and dresses, but i got an xs after trying it on in the store. i'm busty and have hips and still have a bit of room. the length is good--a little above the knee but still work appropriate. with my curves it's not my most flattering dress since it pretty much falls straight down, but i think it's too pretty not to keep.\",\n",
       "       \"Cute top. definitely sheer so need an undershirt. really love details on it, especially back and fringe at bottom. 5'4, 135 lbs, ordered s.\",\n",
       "       \"I was just at the store and purchased this cute top which runs tts for me. i wasn't expecting to find anything at the store but the beautiful lace and adorable tie at the front caught my eye so i bought the small which is my usual size at retailer (34d-27-35) and it fits perfectly. the material is a touch thin but that's okay with me since i don't want heavy clothes for the warmer months anyway. i also don't find this top particularly see through as it's black and will wear a full coverage bra wit\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Affichage des 5 premiers avis\n",
    "data_train[\"Review Text\"].values[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_examples</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       num_examples\n",
       "class              \n",
       " 1             6497\n",
       " 0             4075\n",
       "-1             1225"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "class_distribution = (pd.DataFrame.from_dict(Counter(data_train.score_avis.values),\n",
    "                                             orient='index')\n",
    "                                  .rename(columns={0: 'num_examples'}))\n",
    "class_distribution.index.name = 'class'\n",
    "class_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_examples</th>\n",
       "      <th>perc_examples</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6497</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4075</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>1225</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       num_examples  perc_examples\n",
       "class                             \n",
       " 1             6497           0.55\n",
       " 0             4075           0.35\n",
       "-1             1225           0.10"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class_distribution['perc_examples'] = np.around(class_distribution.num_examples /\n",
    "                                                np.sum(class_distribution.num_examples),\n",
    "                                                2)\n",
    "class_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Représentation des textes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sélection de descripteurs : prétraitements textuels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemple sur un avis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Short of calling it dark gray- which  isn't dark, but that's fine because i saw the picture. it's everything i wanted it to be. i just wish it was chiller here in georgia so i can wear it more often\""
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw = data_train['Review Text'].iloc[100]\n",
    "tw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour effectuer nos tâches de traitements de langage, nous allons transformer le 1er avis en utilisant spacy. En effet, spacy a plusieurs fonctionnalités : \n",
    "- permet de tokeniser directement notre texte\n",
    "- peut lemmatiser les mots\n",
    "- identifier et classer les entités nommées\n",
    "- analyser les dépendances syntaxiques entre les mots\n",
    "- représenter les mots sous forme de vecteurs (embedding)\n",
    "- etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\asus\\anaconda3\\lib\\site-packages (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (0.3.3)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "\n",
      "⠙ Loading compatibility table...\n",
      "⠹ Loading compatibility table...\n",
      "⠸ Loading compatibility table...\n",
      "⠼ Loading compatibility table...\n",
      "\u001b[2K\u001b[38;5;2m✔ Loaded compatibility table\u001b[0m\n",
      "\u001b[1m\n",
      "================= Installed pipeline packages (spaCy v3.7.2) =================\u001b[0m\n",
      "\u001b[38;5;4mℹ spaCy installation:\n",
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\spacy\u001b[0m\n",
      "\n",
      "NAME              SPACY            VERSION                            \n",
      "en_core_web_sm    >=3.7.2,<3.8.0   \u001b[38;5;2m3.7.1\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "fr_core_news_sm   >=3.7.0,<3.8.0   \u001b[38;5;2m3.7.0\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 682.7 kB/s eta 0:00:19\n",
      "     --------------------------------------- 0.0/12.8 MB 495.5 kB/s eta 0:00:26\n",
      "      --------------------------------------- 0.2/12.8 MB 1.4 MB/s eta 0:00:09\n",
      "     - -------------------------------------- 0.4/12.8 MB 2.4 MB/s eta 0:00:06\n",
      "     -- ------------------------------------- 0.7/12.8 MB 3.1 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 1.1/12.8 MB 4.0 MB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 1.3/12.8 MB 4.2 MB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 1.6/12.8 MB 4.3 MB/s eta 0:00:03\n",
      "     ------ --------------------------------- 2.0/12.8 MB 4.8 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 2.3/12.8 MB 5.0 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 2.5/12.8 MB 5.2 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 2.9/12.8 MB 5.3 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 3.3/12.8 MB 5.5 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 4.1/12.8 MB 5.8 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 4.5/12.8 MB 6.1 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 4.8/12.8 MB 6.1 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 5.1/12.8 MB 6.1 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 5.3/12.8 MB 6.0 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 5.7/12.8 MB 6.2 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 6.1/12.8 MB 6.2 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 6.4/12.8 MB 6.2 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 6.6/12.8 MB 6.1 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 7.0/12.8 MB 6.2 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 7.5/12.8 MB 6.3 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 7.8/12.8 MB 6.3 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 8.3/12.8 MB 6.5 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.7/12.8 MB 6.6 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 9.1/12.8 MB 6.6 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.6/12.8 MB 6.8 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.0/12.8 MB 6.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.4/12.8 MB 7.4 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.9/12.8 MB 7.5 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.4/12.8 MB 7.7 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.6/12.8 MB 7.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 11.8/12.8 MB 7.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 11.9/12.8 MB 7.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.2/12.8 MB 7.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 7.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 7.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 7.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.3)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Pour la langue anglaise\n",
    "!pip install -U spacy\n",
    "! python -m spacy validate\n",
    "\n",
    "import spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "This dress in a lovely platinum is feminine and fits perfectly, easy to wear and comfy, too! highly recommend!"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformation du premier avis en objet spacy\n",
    "avis_nlp = nlp(avis) \n",
    "avis_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(avis_nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a bien un objet de type spacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On affiche chaque token de notre objet spacy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "dress\n",
      "in\n",
      "a\n",
      "lovely\n",
      "platinum\n",
      "is\n",
      "feminine\n",
      "and\n",
      "fits\n",
      "perfectly\n",
      ",\n",
      "easy\n",
      "to\n",
      "wear\n",
      "and\n",
      "comfy\n",
      ",\n",
      "too\n",
      "!\n",
      "highly\n",
      "recommend\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for token in avis_nlp:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous allons pouvoir afficher les lemmes de chaque token. Grâce à cette étape, nous allons pouvoir simplifier les mots pour faciliter notre analyse textuelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this\n",
      "dress\n",
      "in\n",
      "a\n",
      "lovely\n",
      "platinum\n",
      "be\n",
      "feminine\n",
      "and\n",
      "fit\n",
      "perfectly\n",
      ",\n",
      "easy\n",
      "to\n",
      "wear\n",
      "and\n",
      "comfy\n",
      ",\n",
      "too\n",
      "!\n",
      "highly\n",
      "recommend\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "# Lemmatisation\n",
    "for token in avis_nlp:\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Généralisation de la lemmatisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour lemmatiser notre texte, nous allons définir une fonction. Cette étape est indispensable pour récupérer les lemmes des mots de notre texte d'origine. Nous allons simplifier notre texte grâce à cette fonction.\n",
    "\n",
    "Cette fonction va nous permettre de généraliser par la suite nos manipulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatise_text(text):\n",
    "    text = nlp(text) # on transforme le texte en objet spacy\n",
    "    lemmas = [token.lemma_ for token in text] # on récupère les lemmes\n",
    "    return ' '.join(lemmas) # on retourne les lemmes sous forme de texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avis initial :  I had such high hopes for this dress and really wanted it to work for me. i initially ordered the petite small (my usual size) but i found this to be outrageously small. so small in fact that i could not zip it up! i reordered it in petite medium, which was just ok. overall, the top half was comfortable and fit nicely, but the bottom half had a very tight under layer and several somewhat cheap (net) over layers. imo, a major design flaw was the net over layer sewn directly into the zipper - it c\n",
      "Avis lemmatisé :  I have such high hope for this dress and really want it to work for I . I initially order the petite small ( my usual size ) but I find this to be outrageously small . so small in fact that I could not zip it up ! I reorder it in petite medium , which be just ok . overall , the top half be comfortable and fit nicely , but the bottom half have a very tight under layer and several somewhat cheap ( net ) over layer . imo , a major design flaw be the net over layer sew directly into the zipper - it c\n",
      "Avis initial :  I love, love, love this jumpsuit. it's fun, flirty, and fabulous! every time i wear it, i get nothing but great compliments!\n",
      "Avis lemmatisé :  I love , love , love this jumpsuit . it be fun , flirty , and fabulous ! every time I wear it , I get nothing but great compliment !\n",
      "Avis initial :  This shirt is very flattering to all due to the adjustable front tie. it is the perfect length to wear with leggings and it is sleeveless so it pairs well with any cardigan. love this shirt!!!\n",
      "Avis lemmatisé :  this shirt be very flattering to all due to the adjustable front tie . it be the perfect length to wear with legging and it be sleeveless so it pair well with any cardigan . love this shirt ! ! !\n"
     ]
    }
   ],
   "source": [
    "# On teste sur 3 avis \n",
    "for avis in liste_avis[:3]:\n",
    "    print(\"Avis initial : \", avis) # On affiche l'avis initial\n",
    "    print(\"Avis lemmatisé : \", lemmatise_text(avis)) # On applique la fonction à notre avis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons ensuite ajouter une colonne avec la fonction de **lemmatisation** appliquée à nos avis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['lemmas'] = data_train['Review Text'].apply(lemmatise_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>score_avis</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10556</th>\n",
       "      <td>10556</td>\n",
       "      <td>I love this top! it's so hard for me to find c...</td>\n",
       "      <td>1</td>\n",
       "      <td>I love this top ! it be so hard for I to find ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3282</th>\n",
       "      <td>3282</td>\n",
       "      <td>I absolutely fell in love with the design of t...</td>\n",
       "      <td>0</td>\n",
       "      <td>I absolutely fall in love with the design of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14016</th>\n",
       "      <td>14016</td>\n",
       "      <td>This dress is beautiful--lovely colors and des...</td>\n",
       "      <td>0</td>\n",
       "      <td>this dress be beautiful -- lovely color and de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19217</th>\n",
       "      <td>19217</td>\n",
       "      <td>Cute top. definitely sheer so need an undershi...</td>\n",
       "      <td>0</td>\n",
       "      <td>cute top . definitely sheer so need an undersh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14821</th>\n",
       "      <td>14821</td>\n",
       "      <td>I was just at the store and purchased this cut...</td>\n",
       "      <td>1</td>\n",
       "      <td>I be just at the store and purchase this cute ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                        Review Text  score_avis  \\\n",
       "10556  10556  I love this top! it's so hard for me to find c...           1   \n",
       "3282    3282  I absolutely fell in love with the design of t...           0   \n",
       "14016  14016  This dress is beautiful--lovely colors and des...           0   \n",
       "19217  19217  Cute top. definitely sheer so need an undershi...           0   \n",
       "14821  14821  I was just at the store and purchased this cut...           1   \n",
       "\n",
       "                                                  lemmas  \n",
       "10556  I love this top ! it be so hard for I to find ...  \n",
       "3282   I absolutely fall in love with the design of t...  \n",
       "14016  this dress be beautiful -- lovely color and de...  \n",
       "19217  cute top . definitely sheer so need an undersh...  \n",
       "14821  I be just at the store and purchase this cute ...  "
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On effectue la même manipulation sur l'ensemble de test ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['lemmas'] = data_test['Review Text'].apply(lemmatise_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7865, 4)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde\n",
    "data_train.to_pickle('train.pkl')\n",
    "data_test.to_pickle('test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Racines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour réduire les mots à leur forme de base, nous allons utiliser SnowballStemmer sur notre texte. Pour cela, nous allons créer une fonction et l'appliquer à nos avis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text(text):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    stems = [stemmer.stem(token) for token in tokenizer.tokenize(text)]\n",
    "    return ' '.join(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this dress in a love platinum is feminin and fit perfect easi to wear and comfi too high recommend'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_text(avis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On applique la fonction à notre dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['stems'] = data_train['Review Text'].apply(stem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>score_avis</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>stems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10556</th>\n",
       "      <td>10556</td>\n",
       "      <td>I love this top! it's so hard for me to find c...</td>\n",
       "      <td>1</td>\n",
       "      <td>I love this top ! it be so hard for I to find ...</td>\n",
       "      <td>i love this top it s so hard for me to find co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3282</th>\n",
       "      <td>3282</td>\n",
       "      <td>I absolutely fell in love with the design of t...</td>\n",
       "      <td>0</td>\n",
       "      <td>I absolutely fall in love with the design of t...</td>\n",
       "      <td>i absolut fell in love with the design of this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14016</th>\n",
       "      <td>14016</td>\n",
       "      <td>This dress is beautiful--lovely colors and des...</td>\n",
       "      <td>0</td>\n",
       "      <td>this dress be beautiful -- lovely color and de...</td>\n",
       "      <td>this dress is beauti love color and design but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19217</th>\n",
       "      <td>19217</td>\n",
       "      <td>Cute top. definitely sheer so need an undershi...</td>\n",
       "      <td>0</td>\n",
       "      <td>cute top . definitely sheer so need an undersh...</td>\n",
       "      <td>cute top definit sheer so need an undershirt r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14821</th>\n",
       "      <td>14821</td>\n",
       "      <td>I was just at the store and purchased this cut...</td>\n",
       "      <td>1</td>\n",
       "      <td>I be just at the store and purchase this cute ...</td>\n",
       "      <td>i was just at the store and purchas this cute ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                        Review Text  score_avis  \\\n",
       "10556  10556  I love this top! it's so hard for me to find c...           1   \n",
       "3282    3282  I absolutely fell in love with the design of t...           0   \n",
       "14016  14016  This dress is beautiful--lovely colors and des...           0   \n",
       "19217  19217  Cute top. definitely sheer so need an undershi...           0   \n",
       "14821  14821  I was just at the store and purchased this cut...           1   \n",
       "\n",
       "                                                  lemmas  \\\n",
       "10556  I love this top ! it be so hard for I to find ...   \n",
       "3282   I absolutely fall in love with the design of t...   \n",
       "14016  this dress be beautiful -- lovely color and de...   \n",
       "19217  cute top . definitely sheer so need an undersh...   \n",
       "14821  I be just at the store and purchase this cute ...   \n",
       "\n",
       "                                                   stems  \n",
       "10556  i love this top it s so hard for me to find co...  \n",
       "3282   i absolut fell in love with the design of this...  \n",
       "14016  this dress is beauti love color and design but...  \n",
       "19217  cute top definit sheer so need an undershirt r...  \n",
       "14821  i was just at the store and purchas this cute ...  "
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fait de même sur l'ensemble de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['stems'] = data_test['Review Text'].apply(stem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7865, 5)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde\n",
    "data_train.to_pickle('train.pkl')\n",
    "data_test.to_pickle('test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Étiquettes morphosyntaxiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis, nous allons analyser notre texte et renvoyer chaque mot remplacé par sa catégorie grammaticale pour continuer l'étude des avis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_words_with_pos_tag(text):\n",
    "    text = nlp(text)\n",
    "    return ' '.join([token.pos_ for token in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On teste la fonction sur un avis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DET NOUN ADP DET ADJ NOUN AUX ADJ CCONJ VERB ADV PUNCT ADJ PART VERB CCONJ ADJ PUNCT ADV PUNCT ADV VERB PUNCT'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_words_with_pos_tag(avis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On applique la fonction à notre ensemble d'entrainement et on ajoute une colonne à notre dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['pos'] = data_train['Review Text'].apply(replace_words_with_pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>score_avis</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>stems</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10556</th>\n",
       "      <td>10556</td>\n",
       "      <td>I love this top! it's so hard for me to find c...</td>\n",
       "      <td>1</td>\n",
       "      <td>I love this top ! it be so hard for I to find ...</td>\n",
       "      <td>i love this top it s so hard for me to find co...</td>\n",
       "      <td>PRON VERB DET NOUN PUNCT PRON AUX ADV ADJ SCON...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3282</th>\n",
       "      <td>3282</td>\n",
       "      <td>I absolutely fell in love with the design of t...</td>\n",
       "      <td>0</td>\n",
       "      <td>I absolutely fall in love with the design of t...</td>\n",
       "      <td>i absolut fell in love with the design of this...</td>\n",
       "      <td>PRON ADV VERB ADP NOUN ADP DET NOUN ADP DET NO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14016</th>\n",
       "      <td>14016</td>\n",
       "      <td>This dress is beautiful--lovely colors and des...</td>\n",
       "      <td>0</td>\n",
       "      <td>this dress be beautiful -- lovely color and de...</td>\n",
       "      <td>this dress is beauti love color and design but...</td>\n",
       "      <td>DET NOUN AUX ADJ PUNCT ADJ NOUN CCONJ NOUN PUN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19217</th>\n",
       "      <td>19217</td>\n",
       "      <td>Cute top. definitely sheer so need an undershi...</td>\n",
       "      <td>0</td>\n",
       "      <td>cute top . definitely sheer so need an undersh...</td>\n",
       "      <td>cute top definit sheer so need an undershirt r...</td>\n",
       "      <td>ADJ NOUN PUNCT ADV ADJ ADV VERB DET NOUN PUNCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14821</th>\n",
       "      <td>14821</td>\n",
       "      <td>I was just at the store and purchased this cut...</td>\n",
       "      <td>1</td>\n",
       "      <td>I be just at the store and purchase this cute ...</td>\n",
       "      <td>i was just at the store and purchas this cute ...</td>\n",
       "      <td>PRON AUX ADV ADP DET NOUN CCONJ VERB DET ADJ N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                        Review Text  score_avis  \\\n",
       "10556  10556  I love this top! it's so hard for me to find c...           1   \n",
       "3282    3282  I absolutely fell in love with the design of t...           0   \n",
       "14016  14016  This dress is beautiful--lovely colors and des...           0   \n",
       "19217  19217  Cute top. definitely sheer so need an undershi...           0   \n",
       "14821  14821  I was just at the store and purchased this cut...           1   \n",
       "\n",
       "                                                  lemmas  \\\n",
       "10556  I love this top ! it be so hard for I to find ...   \n",
       "3282   I absolutely fall in love with the design of t...   \n",
       "14016  this dress be beautiful -- lovely color and de...   \n",
       "19217  cute top . definitely sheer so need an undersh...   \n",
       "14821  I be just at the store and purchase this cute ...   \n",
       "\n",
       "                                                   stems  \\\n",
       "10556  i love this top it s so hard for me to find co...   \n",
       "3282   i absolut fell in love with the design of this...   \n",
       "14016  this dress is beauti love color and design but...   \n",
       "19217  cute top definit sheer so need an undershirt r...   \n",
       "14821  i was just at the store and purchas this cute ...   \n",
       "\n",
       "                                                     pos  \n",
       "10556  PRON VERB DET NOUN PUNCT PRON AUX ADV ADJ SCON...  \n",
       "3282   PRON ADV VERB ADP NOUN ADP DET NOUN ADP DET NO...  \n",
       "14016  DET NOUN AUX ADJ PUNCT ADJ NOUN CCONJ NOUN PUN...  \n",
       "19217  ADJ NOUN PUNCT ADV ADJ ADV VERB DET NOUN PUNCT...  \n",
       "14821  PRON AUX ADV ADP DET NOUN CCONJ VERB DET ADJ N...  "
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On effectue la même manipulation sur notre ensemble de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['pos'] = data_test['Review Text'].apply(replace_words_with_pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7865, 6)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde\n",
    "data_train.to_pickle('train.pkl')\n",
    "data_test.to_pickle('test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Classe d'appartenance des entités nommées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour pouvoir effectuer la reconnaissances d'entités nommées sur nos avis, nous allons retourner une version de notre avis ou chaque entité nommée est remplacée par son type d'entité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner(text):\n",
    "\n",
    "    text = nlp(text) # on transforme le texte en objet spacy\n",
    "    \n",
    "    new_text = [] # on crée une liste vide\n",
    "\n",
    "    for token in text: # pour chaque token dans l'avis\n",
    "\n",
    "        # print(token.text, token.ent_iob_, token.ent_type_)\n",
    "        \n",
    "        if token.ent_iob_ == \"O\": # si l'entité ne fait pas partie d'une entité nommée\n",
    "            new_text.append(token.text) # on ajoute le texte du token à la liste\n",
    "        elif token.ent_iob_ == \"B\": # si l'entité fait partie d'une entité nommée\n",
    "            new_text.append(token.ent_type_) # on ajoute le type de l'entité à la liste\n",
    "\n",
    "        # Si l'entité comprend plusieurs mot on ne répète pas l'étiquette\n",
    "        else:\n",
    "            continue\n",
    "    return ' '.join(new_text) # on retourne les étiquettes sous forme de texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On applique la fonction sur un avis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avis initiaux :  This shirt is very flattering to all due to the adjustable front tie. it is the perfect length to wear with leggings and it is sleeveless so it pairs well with any cardigan. love this shirt!!!\n",
      "Avec les entités nommées :  This shirt is very flattering to all due to the adjustable front tie . it is the perfect length to wear with leggings and it is sleeveless so it pairs well with any cardigan . love this shirt ! ! !\n"
     ]
    }
   ],
   "source": [
    "print(\"Avis initiaux : \", avis)\n",
    "print(\"Avec les entités nommées : \", ner(avis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On applique ensuite notre fonction à notre dataframe d'entrainement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['entites_nommees'] = data_train['Review Text'].apply(ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>score_avis</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>stems</th>\n",
       "      <th>pos</th>\n",
       "      <th>entites_nommees</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10556</th>\n",
       "      <td>10556</td>\n",
       "      <td>I love this top! it's so hard for me to find c...</td>\n",
       "      <td>1</td>\n",
       "      <td>I love this top ! it be so hard for I to find ...</td>\n",
       "      <td>i love this top it s so hard for me to find co...</td>\n",
       "      <td>PRON VERB DET NOUN PUNCT PRON AUX ADV ADJ SCON...</td>\n",
       "      <td>I love this top ! it 's so hard for me to find...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3282</th>\n",
       "      <td>3282</td>\n",
       "      <td>I absolutely fell in love with the design of t...</td>\n",
       "      <td>0</td>\n",
       "      <td>I absolutely fall in love with the design of t...</td>\n",
       "      <td>i absolut fell in love with the design of this...</td>\n",
       "      <td>PRON ADV VERB ADP NOUN ADP DET NOUN ADP DET NO...</td>\n",
       "      <td>I absolutely fell in love with the design of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14016</th>\n",
       "      <td>14016</td>\n",
       "      <td>This dress is beautiful--lovely colors and des...</td>\n",
       "      <td>0</td>\n",
       "      <td>this dress be beautiful -- lovely color and de...</td>\n",
       "      <td>this dress is beauti love color and design but...</td>\n",
       "      <td>DET NOUN AUX ADJ PUNCT ADJ NOUN CCONJ NOUN PUN...</td>\n",
       "      <td>This dress is beautiful -- lovely colors and d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19217</th>\n",
       "      <td>19217</td>\n",
       "      <td>Cute top. definitely sheer so need an undershi...</td>\n",
       "      <td>0</td>\n",
       "      <td>cute top . definitely sheer so need an undersh...</td>\n",
       "      <td>cute top definit sheer so need an undershirt r...</td>\n",
       "      <td>ADJ NOUN PUNCT ADV ADJ ADV VERB DET NOUN PUNCT...</td>\n",
       "      <td>Cute top . definitely sheer so need an undersh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14821</th>\n",
       "      <td>14821</td>\n",
       "      <td>I was just at the store and purchased this cut...</td>\n",
       "      <td>1</td>\n",
       "      <td>I be just at the store and purchase this cute ...</td>\n",
       "      <td>i was just at the store and purchas this cute ...</td>\n",
       "      <td>PRON AUX ADV ADP DET NOUN CCONJ VERB DET ADJ N...</td>\n",
       "      <td>I was just at the store and purchased this cut...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                        Review Text  score_avis  \\\n",
       "10556  10556  I love this top! it's so hard for me to find c...           1   \n",
       "3282    3282  I absolutely fell in love with the design of t...           0   \n",
       "14016  14016  This dress is beautiful--lovely colors and des...           0   \n",
       "19217  19217  Cute top. definitely sheer so need an undershi...           0   \n",
       "14821  14821  I was just at the store and purchased this cut...           1   \n",
       "\n",
       "                                                  lemmas  \\\n",
       "10556  I love this top ! it be so hard for I to find ...   \n",
       "3282   I absolutely fall in love with the design of t...   \n",
       "14016  this dress be beautiful -- lovely color and de...   \n",
       "19217  cute top . definitely sheer so need an undersh...   \n",
       "14821  I be just at the store and purchase this cute ...   \n",
       "\n",
       "                                                   stems  \\\n",
       "10556  i love this top it s so hard for me to find co...   \n",
       "3282   i absolut fell in love with the design of this...   \n",
       "14016  this dress is beauti love color and design but...   \n",
       "19217  cute top definit sheer so need an undershirt r...   \n",
       "14821  i was just at the store and purchas this cute ...   \n",
       "\n",
       "                                                     pos  \\\n",
       "10556  PRON VERB DET NOUN PUNCT PRON AUX ADV ADJ SCON...   \n",
       "3282   PRON ADV VERB ADP NOUN ADP DET NOUN ADP DET NO...   \n",
       "14016  DET NOUN AUX ADJ PUNCT ADJ NOUN CCONJ NOUN PUN...   \n",
       "19217  ADJ NOUN PUNCT ADV ADJ ADV VERB DET NOUN PUNCT...   \n",
       "14821  PRON AUX ADV ADP DET NOUN CCONJ VERB DET ADJ N...   \n",
       "\n",
       "                                         entites_nommees  \n",
       "10556  I love this top ! it 's so hard for me to find...  \n",
       "3282   I absolutely fell in love with the design of t...  \n",
       "14016  This dress is beautiful -- lovely colors and d...  \n",
       "19217  Cute top . definitely sheer so need an undersh...  \n",
       "14821  I was just at the store and purchased this cut...  "
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aussi, on applique à notre ensemble de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['entites_nommees'] = data_test['Review Text'].apply(ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7865, 7)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.to_pickle('train.pkl')\n",
    "data_test.to_pickle('test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Calcul des valeurs des descripteurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour procéder aux calculs, nous allons séparer notre jeu de données d'entraînement pour avoir un jeu de données de validation. Les données test nou servirons pour l'évaluation finale des modèles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(data_train['Review Text'],\n",
    "                                                      data_train['score_avis'],\n",
    "                                                      train_size=0.75,\n",
    "                                                      random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8847,), (2950,))"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a donc 8 847 lignes dans notre jeu d'entrainement et 2 950 dans celui de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2955     0\n",
       "21614   -1\n",
       "15031    1\n",
       "5773     1\n",
       "19552   -1\n",
       "        ..\n",
       "9405     0\n",
       "617      0\n",
       "19144   -1\n",
       "11891    0\n",
       "2431     1\n",
       "Name: score_avis, Length: 8847, dtype: int64"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons observer que les sorties à prédire correspondent aux trois étiquettes que nous avons défini plus haut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour évaluer notre modèle, nous initialisons les ensembles de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère les avis et les labels du jeu de données de test\n",
    "X_test, y_test = data_test['Review Text'], data_test['score_avis'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binaire : présence/absence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bin_count = CountVectorizer(binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(binary=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(binary=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(binary=True)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_count.fit(X_train)\n",
    "bin_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8847x9587 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 386829 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized_bin = bin_count.transform(X_train)\n",
    "X_train_vectorized_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_vectorized_bin = bin_count.transform(X_valid)\n",
    "X_test_vectorized_bin = bin_count.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2950x9587 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 128132 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid_vectorized_bin # MEME NOMBRE DE COLONNES QUE X_train_vectorized_bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Numérique discret : décomptes d'occurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons calculer les fréquences d'occurence des termes dans nos avis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_count = CountVectorizer().fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons examiner le vocabulaire de nos avis : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '00p', '0p', '0petite', '0r', '0verall', '10', '100', '1000',\n",
       "       '100lbs', '101', '102', '102lbs', '103', '103lbs', '104', '105',\n",
       "       '105lbs', '106', '106lbs', '107', '107lb', '107lbs', '108', '109',\n",
       "       '109lbs', '10p', '10s', '10th', '11', '110', '110lb', '110lbs',\n",
       "       '111lbs', '112', '112lbs', '112llbs', '113', '113lbs', '114',\n",
       "       '114lb', '114lbs', '115', '115ish', '115lbs', '115llbs', '116',\n",
       "       '116bs', '116lb', '116lbs'], dtype=object)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_count.get_feature_names_out()[:50] # 50 premiers mots (\"types\" du vocabulaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['yepeee', 'yes', 'yesterday', 'yesteryear', 'yet', 'yey', 'yield',\n",
       "       'yikes', 'yippee', 'yo', 'yoga', 'yogis', 'yoke', 'york', 'you',\n",
       "       'young', 'younger', 'your', 'youre', 'yourself', 'youth',\n",
       "       'youthful', 'yr', 'yrs', 'yuck', 'yucky', 'yummiest', 'yummy',\n",
       "       'yup', 'zag', 'zara', 'zero', 'zig', 'zip', 'zipepr', 'ziploc',\n",
       "       'zipped', 'zipper', 'zippered', 'zippers', 'zippie', 'zipping',\n",
       "       'zips', 'zombie', 'zone', 'zoolander', 'zoom', 'zooming', 'zuma',\n",
       "       'ã¼ber'], dtype=object)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_count.get_feature_names_out()[-50:] # 50 derniers mots (\"types\" du vocabulaire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taille de notre vocabulaire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9587"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect_count.get_feature_names_out()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création matrice document-termes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons créer la matrice document-termes avec le même vectoriseur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8847x9587 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 386829 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized_count = vect_count.transform(X_train)\n",
    "X_train_vectorized_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_vectorized_count = vect_count.transform(X_valid)\n",
    "X_test_vectorized_count = vect_count.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A présent, nous allons prendre en compte les bi-grammes dans notre vocabulaire. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_count_bigrams = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\n",
    "X_train_vectorized_count_bigrams = vect_count_bigrams.transform(X_train)\n",
    "X_valid_vectorized_count_bigrams = vect_count_bigrams.transform(X_valid)\n",
    "X_test_vectorized_count_bigrams = vect_count_bigrams.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17321"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect_count_bigrams.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons presque 2 fois plus de vocabulaire avec inclusion des bigrammes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Numérique continu : TF-IDF (ou autres pondérations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons limiter le vocabulaire à des termes qui apparaissent au moins 5 fois dans le document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_tfidf = TfidfVectorizer(min_df=5).fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9587, 3232)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect_count.get_feature_names_out()), len(vect_tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La réduction de la taille du vocadulaire est importante et est due au paramètre min_df=5 : on a quasiment 3 fois moins de termes !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons vectoriser les jeux de données. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorisation des corpus d'entrainement, de validation et de test\n",
    "X_train_vectorized_tfidf = vect_tfidf.transform(X_train)\n",
    "X_valid_vectorized_tfidf = vect_tfidf.transform(X_valid)\n",
    "X_test_vectorized_tfidf = vect_tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification des textes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons réaliser une classification en utilisant plusieurs modèles afin de comparer les performances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèles de référence faibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choix aléatoire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons d'abord choisir un modèle où toutes les classes ont la même probabilité d'être choisies ou bien le prédicteur respecte la disctribution des classes dans les données d'entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prédiction proportionnelle à la distribution des classes dans les données d'entraînement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_prop_class = DummyClassifier(strategy='stratified').fit(X_train_vectorized_tfidf,\n",
    "                                                               y_train)\n",
    "predictions_valid = random_prop_class.predict(X_valid_vectorized_tfidf)\n",
    "conf_mat = confusion_matrix(y_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 29 120 187]\n",
      " [110 337 546]\n",
      " [164 569 888]]\n"
     ]
    }
   ],
   "source": [
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prédiction uniforme : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  1, ...,  1,  0, -1], dtype=int64)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_uniform = DummyClassifier(strategy='uniform').fit(X_train_vectorized_tfidf,\n",
    "                                                         y_train)\n",
    "predictions_valid = random_uniform.predict(X_valid_vectorized_tfidf)\n",
    "predictions_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[106 133  97]\n",
      " [340 320 333]\n",
      " [526 562 533]]\n"
     ]
    }
   ],
   "source": [
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32508474576271185"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.11      0.32      0.16       336\n",
      "           0       0.32      0.32      0.32       993\n",
      "           1       0.55      0.33      0.41      1621\n",
      "\n",
      "    accuracy                           0.33      2950\n",
      "   macro avg       0.33      0.32      0.30      2950\n",
      "weighted avg       0.42      0.33      0.35      2950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, predictions_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prédiction constante de la classe majoritaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons d'abord identifier la répartition des classes dans les données d'entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_examples</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       num_examples\n",
       "class              \n",
       " 1             6497\n",
       " 0             4075\n",
       "-1             1225"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maj = DummyClassifier(strategy='most_frequent').fit(X_train_vectorized_tfidf, y_train)\n",
    "predictions_valid = maj.predict(X_valid_vectorized_tfidf)\n",
    "predictions_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "maj_class = (class_distribution.index[class_distribution.perc_examples ==\n",
    "                                      np.amax(class_distribution.perc_examples)][0])\n",
    "maj_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(predictions_valid == maj_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5494915254237288"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maj.score(X_valid_vectorized_tfidf, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00       336\n",
      "           0       0.00      0.00      0.00       993\n",
      "           1       0.55      1.00      0.71      1621\n",
      "\n",
      "    accuracy                           0.55      2950\n",
      "   macro avg       0.18      0.33      0.24      2950\n",
      "weighted avg       0.30      0.55      0.39      2950\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, predictions_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifieur naïf bayésien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nb = MultinomialNB().fit(X_train_vectorized_tfidf, y_train)\n",
    "predictions_valid = model_nb.predict(X_valid_vectorized_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6823728813559322"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.62      0.01      0.03       336\n",
      "           0       0.57      0.50      0.54       993\n",
      "           1       0.73      0.93      0.82      1621\n",
      "\n",
      "    accuracy                           0.68      2950\n",
      "   macro avg       0.64      0.48      0.46      2950\n",
      "weighted avg       0.66      0.68      0.63      2950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, predictions_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model_lr = LogisticRegression(multi_class='multinomial', solver='lbfgs',\n",
    "                              max_iter=200).fit(X_train_vectorized_count, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_valid = model_lr.predict(X_valid_vectorized_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6752542372881356"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.53      0.42      0.47       336\n",
      "           0       0.55      0.58      0.56       993\n",
      "           1       0.78      0.79      0.78      1621\n",
      "\n",
      "    accuracy                           0.68      2950\n",
      "   macro avg       0.62      0.59      0.60      2950\n",
      "weighted avg       0.67      0.68      0.67      2950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, predictions_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_n_strongly_associated_features(vectoriser, model, n):\n",
    "    feature_names = np.array(vectoriser.get_feature_names_out())\n",
    "\n",
    "    for i in range(3):\n",
    "        class_name = model.classes_[i]\n",
    "        print(\"CLASSE {}\".format(class_name))\n",
    "        idx_coefs_sorted = model.coef_[i].argsort() # ordre croissant\n",
    "        print(\"Les dix variables ayant l'association négative la plus forte \" + \n",
    "              \"avec la classe {} :\\n{}\\n\".format(class_name,\n",
    "                                                 feature_names[idx_coefs_sorted[:n]]))\n",
    "        idx_coefs_sorted = idx_coefs_sorted[::-1] # ordre décroissant\n",
    "        print(\"Les dix variables ayant l'association positive la plus forte \" +\n",
    "              \"avec la classe {} :\\n{}\\n\"\n",
    "              .format(class_name,\n",
    "                      feature_names[idx_coefs_sorted[:n]]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examinons les variables (termes) ayant l'association la plus forte avec chaque classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSE -1\n",
      "Les dix variables ayant l'association négative la plus forte avec la classe -1 :\n",
      "['nude' 'wait' 'compliments' 'easy' 'little' '34' 'amazing' 'fun' 'hoped'\n",
      " 'complaint']\n",
      "\n",
      "Les dix variables ayant l'association positive la plus forte avec la classe -1 :\n",
      "['flow' 'poor' 'weird' 'instructions' 'awful' 'unflattering' 'bummed'\n",
      " 'money' 'returning' 'disappointing']\n",
      "\n",
      "\n",
      "CLASSE 0\n",
      "Les dix variables ayant l'association négative la plus forte avec la classe 0 :\n",
      "['machine' 'bonus' 'risk' '140' 'heat' 'sleeved' 'fixed' 'door' 'string'\n",
      " 'street']\n",
      "\n",
      "Les dix variables ayant l'association positive la plus forte avec la classe 0 :\n",
      "['cap' 'yarn' 'clearance' 'sparkle' 'stitches' 'sundry' 'snatched' 'sleek'\n",
      " 'opposed' 'pulls']\n",
      "\n",
      "\n",
      "CLASSE 1\n",
      "Les dix variables ayant l'association négative la plus forte avec la classe 1 :\n",
      "['disappointing' 'cap' 'idea' 'poor' 'seemed' 'bummed' 'returning'\n",
      " 'disappointed' 'taste' 'strange']\n",
      "\n",
      "Les dix variables ayant l'association positive la plus forte avec la classe 1 :\n",
      "['machine' 'bonus' 'fantastic' 'nude' 'highly' 'stunning' 'heat' 'beauty'\n",
      " 'pleasantly' 'paired']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_n_strongly_associated_features(vect_count, model_lr, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMMENTAIRE A METTRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model_lr = LogisticRegression(multi_class='multinomial',\n",
    "                              solver='lbfgs').fit(X_train_vectorized_tfidf, y_train)\n",
    "predictions_valid = model_lr.predict(X_valid_vectorized_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.703728813559322"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.67      0.28      0.39       336\n",
      "           0       0.59      0.60      0.59       993\n",
      "           1       0.77      0.85      0.81      1621\n",
      "\n",
      "    accuracy                           0.70      2950\n",
      "   macro avg       0.68      0.58      0.60      2950\n",
      "weighted avg       0.70      0.70      0.69      2950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, predictions_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF le moins élevé : ['pros' 'shouldn' 'tighten' 'secondly' 'framed' 'wo' 'xspetite' '30d'\n",
      " 'dingy' 'fo']\n",
      "TF-IDF le plus élevé : ['amp' 'sweet' 'loved' 'simple' 'she' 'cute' 'fits' 'hei' 'decent' 'rough']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vect_tfidf.get_feature_names_out())\n",
    "idx_tfidf_sorted = X_train_vectorized_tfidf.max(0).toarray()[0].argsort()\n",
    "print(\"TF-IDF le moins élevé : {}\".format(feature_names[idx_tfidf_sorted[:10]]))\n",
    "print(\"TF-IDF le plus élevé : {}\".format(feature_names[idx_tfidf_sorted[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous faisons avec les mêmes paramètres mais avec le vectoriseur à unigrammes et bigrammes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression(multi_class='multinomial', solver='lbfgs',max_iter=500).fit(X_train_vectorized_count_bigrams, y_train)\n",
    "predictions_valid = model_lr.predict(X_valid_vectorized_count_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_valid, predictions_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_n_strongly_associated_features(vect_count_bigrams, model_lr, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svm = SVC(kernel='linear', C=0.1).fit(X_train_vectorized_count_bigrams, y_train)\n",
    "predictions_valid = model_svm.predict(X_valid_vectorized_count_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_valid, predictions_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
