{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Classification des avis sur des vêtements de femmes vendus dans le e-commerce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Est ce que les avis que l'on a des vêtements sont représentatifs de la note qui est attribuée ?\n",
    "\n",
    "Idées :\n",
    "- visualisation de données : quels types de vêtements ont les notes les plus élevées ?\n",
    "- nb d'avis donné selon l'âge des clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Import des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps, nous allons importer nos données. Notre base de données contient des informations sur des avis de vêtements femmes vendus sur internet. Ces données sont issus d'un processus de webscrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>767</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Absolutely wonderful - silky and sexy and comf...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Initmates</td>\n",
       "      <td>Intimate</td>\n",
       "      <td>Intimates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1080</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1077</td>\n",
       "      <td>60</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1049</td>\n",
       "      <td>50</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>847</td>\n",
       "      <td>47</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  Clothing ID  Age                    Title  \\\n",
       "0   0          767   33                      NaN   \n",
       "1   1         1080   34                      NaN   \n",
       "2   2         1077   60  Some major design flaws   \n",
       "3   3         1049   50         My favorite buy!   \n",
       "4   4          847   47         Flattering shirt   \n",
       "\n",
       "                                         Review Text  Rating  Recommended IND  \\\n",
       "0  Absolutely wonderful - silky and sexy and comf...       4                1   \n",
       "1  Love this dress!  it's sooo pretty.  i happene...       5                1   \n",
       "2  I had such high hopes for this dress and reall...       3                0   \n",
       "3  I love, love, love this jumpsuit. it's fun, fl...       5                1   \n",
       "4  This shirt is very flattering to all due to th...       5                1   \n",
       "\n",
       "   Positive Feedback Count   Division Name Department Name Class Name  \n",
       "0                        0       Initmates        Intimate  Intimates  \n",
       "1                        4         General         Dresses    Dresses  \n",
       "2                        0         General         Dresses    Dresses  \n",
       "3                        0  General Petite         Bottoms      Pants  \n",
       "4                        6         General            Tops    Blouses  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import des données\n",
    "data = pd.read_csv(\"Womens Clothing E-Commerce Reviews.csv\", sep = \",\")\n",
    "\n",
    "# Renomage première colonne pour pouvoir l'utiliser comme id par la suite\n",
    "data = data.rename(columns = {\"Unnamed: 0\" : \"id\"})\n",
    "\n",
    "# Affichage des 5 premières lignes\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23486, 11)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre jeu de données contient 23 486 avis et 11 colonnes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Pré-traitement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour avoir des données plus propres, nous allons effectuer divers pré-traitements.\n",
    "\n",
    "Pour commencer, nous allons supprimer les lignes où nous avons des valeurs manquantes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19662, 11)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Suppression des valeurs manquantes\n",
    "data = data.dropna()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suite à cette manipulation, nous supprimons environ 4 000 lignes pour pouvoir avoir des données complètes pour poursuivre notre analyse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération des données pour notre étude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traitement de la casse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- suppression des valeurs manquantes => pas d'avis : inutile\n",
    "- suppression des caractères spéciaux \n",
    "- suppression des majuscules\n",
    "- suppression des mots vides\n",
    "- lemmatisation \n",
    "- affiche du nombre de mots par étiquette grammaticale\n",
    "- extraction des mots (groupes de mots) les plus fréquents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps, nous allons récupérer les avis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2        I had such high hopes for this dress and reall...\n",
       "3        I love, love, love this jumpsuit. it's fun, fl...\n",
       "4        This shirt is very flattering to all due to th...\n",
       "5        I love tracy reese dresses, but this one is no...\n",
       "6        I aded this in my basket at hte last mintue to...\n",
       "                               ...                        \n",
       "23481    I was very happy to snag this dress at such a ...\n",
       "23482    It reminds me of maternity clothes. soft, stre...\n",
       "23483    This fit well, but the top was very see throug...\n",
       "23484    I bought this dress for a wedding i have this ...\n",
       "23485    This dress in a lovely platinum is feminine an...\n",
       "Name: Review Text, Length: 19662, dtype: object"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avis = data[\"Review Text\"]\n",
    "avis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(avis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons récupérer seulement la partie textuelle de l'avis, cela nous permet de ne avoir un objet Pandas.Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I had such high hopes for this dress and really wanted it to work for me. i '\n",
      " 'initially ordered the petite small (my usual size) but i found this to be '\n",
      " 'outrageously small. so small in fact that i could not zip it up! i reordered '\n",
      " 'it in petite medium, which was just ok. overall, the top half was '\n",
      " 'comfortable and fit nicely, but the bottom half had a very tight under layer '\n",
      " 'and several somewhat cheap (net) over layers. imo, a major design flaw was '\n",
      " 'the net over layer sewn directly into the zipper - it c',\n",
      " \"I love, love, love this jumpsuit. it's fun, flirty, and fabulous! every time \"\n",
      " 'i wear it, i get nothing but great compliments!',\n",
      " 'This shirt is very flattering to all due to the adjustable front tie. it is '\n",
      " 'the perfect length to wear with leggings and it is sleeveless so it pairs '\n",
      " 'well with any cardigan. love this shirt!!!',\n",
      " 'I love tracy reese dresses, but this one is not for the very petite. i am '\n",
      " 'just under 5 feet tall and usually wear a 0p in this brand. this dress was '\n",
      " 'very pretty out of the package but its a lot of dress. the skirt is long and '\n",
      " 'very full so it overwhelmed my small frame. not a stranger to alterations, '\n",
      " 'shortening and narrowing the skirt would take away from the embellishment of '\n",
      " 'the garment. i love the color and the idea of the style but it just did not '\n",
      " 'work on me. i returned this dress.',\n",
      " 'I aded this in my basket at hte last mintue to see what it would look like '\n",
      " 'in person. (store pick up). i went with teh darkler color only because i am '\n",
      " 'so pale :-) hte color is really gorgeous, and turns out it mathced '\n",
      " 'everythiing i was trying on with it prefectly. it is a little baggy on me '\n",
      " 'and hte xs is hte msallet size (bummer, no petite). i decided to jkeep it '\n",
      " 'though, because as i said, it matvehd everything. my ejans, pants, and the 3 '\n",
      " 'skirts i waas trying on (of which i ]kept all ) oops.',\n",
      " 'I ordered this in carbon for store pick up, and had a ton of stuff (as '\n",
      " 'always) to try on and used this top to pair (skirts and pants). everything '\n",
      " 'went with it. the color is really nice charcoal with shimmer, and went well '\n",
      " 'with pencil skirts, flare pants, etc. my only compaint is it is a bit big, '\n",
      " \"sleeves are long and it doesn't go in petite. also a bit loose for me, but \"\n",
      " 'no xxs... so i kept it and wil ldecide later since the light color is '\n",
      " 'already sold out in hte smallest size...',\n",
      " 'I love this dress. i usually get an xs but it runs a little snug in bust so '\n",
      " 'i ordered up a size. very flattering and feminine with the usual retailer '\n",
      " 'flair for style.',\n",
      " 'I\\'m 5\"5\\' and 125 lbs. i ordered the s petite to make sure the length '\n",
      " \"wasn't too long. i typically wear an xs regular in retailer dresses. if \"\n",
      " \"you're less busty (34b cup or smaller), a s petite will fit you perfectly \"\n",
      " '(snug, but not tight). i love that i could dress it up for a party, or down '\n",
      " 'for work. i love that the tulle is longer then the fabric underneath.',\n",
      " 'Dress runs small esp where the zipper area runs. i ordered the sp which '\n",
      " 'typically fits me and it was very tight! the material on the top looks and '\n",
      " 'feels very cheap that even just pulling on it will cause it to rip the '\n",
      " 'fabric. pretty disappointed as it was going to be my christmas dress this '\n",
      " 'year! needless to say it will be going back.',\n",
      " 'More and more i find myself reliant on the reviews written by savvy shoppers '\n",
      " 'before me and for the most past, they are right on in their estimation of '\n",
      " 'the product. in the case of this dress-if it had not been for the reveiws-i '\n",
      " 'doubt i would have even tried this. the dress is beautifully made, lined and '\n",
      " 'reminiscent of the old retailer quality. it is lined in the solid '\n",
      " 'periwinkle-colored fabric that matches the outer fabric print. tts and very '\n",
      " 'form-fitting. falls just above the knee and does not rid']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "liste_avis = data[\"Review Text\"].values.tolist()\n",
    "pprint(liste_avis[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grâce à cette manipulation, chaque avis est élément d'une liste d'avis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous allons découper les avis en liste de mots et les mettre en minuscules pour pouvoir les analyser plus facilement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i had such high hopes for this dress and really wanted it to work for me. i initially ordered the petite small (my usual size) but i found this to be outrageously small. so small in fact that i could not zip it up! i reordered it in petite medium, which was just ok. overall, the top half was comfortable and fit nicely, but the bottom half had a very tight under layer and several somewhat cheap (net) over layers. imo, a major design flaw was the net over layer sewn directly into the zipper - it c',\n",
       " \"i love, love, love this jumpsuit. it's fun, flirty, and fabulous! every time i wear it, i get nothing but great compliments!\",\n",
       " 'this shirt is very flattering to all due to the adjustable front tie. it is the perfect length to wear with leggings and it is sleeveless so it pairs well with any cardigan. love this shirt!!!',\n",
       " 'i love tracy reese dresses, but this one is not for the very petite. i am just under 5 feet tall and usually wear a 0p in this brand. this dress was very pretty out of the package but its a lot of dress. the skirt is long and very full so it overwhelmed my small frame. not a stranger to alterations, shortening and narrowing the skirt would take away from the embellishment of the garment. i love the color and the idea of the style but it just did not work on me. i returned this dress.',\n",
       " 'i aded this in my basket at hte last mintue to see what it would look like in person. (store pick up). i went with teh darkler color only because i am so pale :-) hte color is really gorgeous, and turns out it mathced everythiing i was trying on with it prefectly. it is a little baggy on me and hte xs is hte msallet size (bummer, no petite). i decided to jkeep it though, because as i said, it matvehd everything. my ejans, pants, and the 3 skirts i waas trying on (of which i ]kept all ) oops.',\n",
       " \"i ordered this in carbon for store pick up, and had a ton of stuff (as always) to try on and used this top to pair (skirts and pants). everything went with it. the color is really nice charcoal with shimmer, and went well with pencil skirts, flare pants, etc. my only compaint is it is a bit big, sleeves are long and it doesn't go in petite. also a bit loose for me, but no xxs... so i kept it and wil ldecide later since the light color is already sold out in hte smallest size...\",\n",
       " 'i love this dress. i usually get an xs but it runs a little snug in bust so i ordered up a size. very flattering and feminine with the usual retailer flair for style.',\n",
       " 'i\\'m 5\"5\\' and 125 lbs. i ordered the s petite to make sure the length wasn\\'t too long. i typically wear an xs regular in retailer dresses. if you\\'re less busty (34b cup or smaller), a s petite will fit you perfectly (snug, but not tight). i love that i could dress it up for a party, or down for work. i love that the tulle is longer then the fabric underneath.',\n",
       " 'dress runs small esp where the zipper area runs. i ordered the sp which typically fits me and it was very tight! the material on the top looks and feels very cheap that even just pulling on it will cause it to rip the fabric. pretty disappointed as it was going to be my christmas dress this year! needless to say it will be going back.',\n",
       " 'more and more i find myself reliant on the reviews written by savvy shoppers before me and for the most past, they are right on in their estimation of the product. in the case of this dress-if it had not been for the reveiws-i doubt i would have even tried this. the dress is beautifully made, lined and reminiscent of the old retailer quality. it is lined in the solid periwinkle-colored fabric that matches the outer fabric print. tts and very form-fitting. falls just above the knee and does not rid']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Découpage des avis en mots\n",
    "\n",
    "liste_avis_clean = []\n",
    "\n",
    "for avis in liste_avis : \n",
    "    avis = str(avis)\n",
    "    avis_clean = avis.split()\n",
    "    avis_clean = avis.lower()\n",
    "    liste_avis_clean.append(avis_clean)\n",
    "\n",
    "liste_avis_clean[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis, nous allons tockeniser notre texte. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i',\n",
       "  'had',\n",
       "  'such',\n",
       "  'high',\n",
       "  'hopes',\n",
       "  'for',\n",
       "  'this',\n",
       "  'dress',\n",
       "  'and',\n",
       "  'really',\n",
       "  'wanted',\n",
       "  'it',\n",
       "  'to',\n",
       "  'work',\n",
       "  'for',\n",
       "  'me',\n",
       "  'i',\n",
       "  'initially',\n",
       "  'ordered',\n",
       "  'the',\n",
       "  'petite',\n",
       "  'small',\n",
       "  'my',\n",
       "  'usual',\n",
       "  'size',\n",
       "  'but',\n",
       "  'i',\n",
       "  'found',\n",
       "  'this',\n",
       "  'to',\n",
       "  'be',\n",
       "  'outrageously',\n",
       "  'small',\n",
       "  'so',\n",
       "  'small',\n",
       "  'in',\n",
       "  'fact',\n",
       "  'that',\n",
       "  'i',\n",
       "  'could',\n",
       "  'not',\n",
       "  'zip',\n",
       "  'it',\n",
       "  'up',\n",
       "  'i',\n",
       "  'reordered',\n",
       "  'it',\n",
       "  'in',\n",
       "  'petite',\n",
       "  'medium',\n",
       "  'which',\n",
       "  'was',\n",
       "  'just',\n",
       "  'ok',\n",
       "  'overall',\n",
       "  'the',\n",
       "  'top',\n",
       "  'half',\n",
       "  'was',\n",
       "  'comfortable',\n",
       "  'and',\n",
       "  'fit',\n",
       "  'nicely',\n",
       "  'but',\n",
       "  'the',\n",
       "  'bottom',\n",
       "  'half',\n",
       "  'had',\n",
       "  'a',\n",
       "  'very',\n",
       "  'tight',\n",
       "  'under',\n",
       "  'layer',\n",
       "  'and',\n",
       "  'several',\n",
       "  'somewhat',\n",
       "  'cheap',\n",
       "  'net',\n",
       "  'over',\n",
       "  'layers',\n",
       "  'imo',\n",
       "  'a',\n",
       "  'major',\n",
       "  'design',\n",
       "  'flaw',\n",
       "  'was',\n",
       "  'the',\n",
       "  'net',\n",
       "  'over',\n",
       "  'layer',\n",
       "  'sewn',\n",
       "  'directly',\n",
       "  'into',\n",
       "  'the',\n",
       "  'zipper',\n",
       "  'it',\n",
       "  'c'],\n",
       " ['i',\n",
       "  'love',\n",
       "  'love',\n",
       "  'love',\n",
       "  'this',\n",
       "  'jumpsuit',\n",
       "  'it',\n",
       "  's',\n",
       "  'fun',\n",
       "  'flirty',\n",
       "  'and',\n",
       "  'fabulous',\n",
       "  'every',\n",
       "  'time',\n",
       "  'i',\n",
       "  'wear',\n",
       "  'it',\n",
       "  'i',\n",
       "  'get',\n",
       "  'nothing',\n",
       "  'but',\n",
       "  'great',\n",
       "  'compliments'],\n",
       " ['this',\n",
       "  'shirt',\n",
       "  'is',\n",
       "  'very',\n",
       "  'flattering',\n",
       "  'to',\n",
       "  'all',\n",
       "  'due',\n",
       "  'to',\n",
       "  'the',\n",
       "  'adjustable',\n",
       "  'front',\n",
       "  'tie',\n",
       "  'it',\n",
       "  'is',\n",
       "  'the',\n",
       "  'perfect',\n",
       "  'length',\n",
       "  'to',\n",
       "  'wear',\n",
       "  'with',\n",
       "  'leggings',\n",
       "  'and',\n",
       "  'it',\n",
       "  'is',\n",
       "  'sleeveless',\n",
       "  'so',\n",
       "  'it',\n",
       "  'pairs',\n",
       "  'well',\n",
       "  'with',\n",
       "  'any',\n",
       "  'cardigan',\n",
       "  'love',\n",
       "  'this',\n",
       "  'shirt'],\n",
       " ['i',\n",
       "  'love',\n",
       "  'tracy',\n",
       "  'reese',\n",
       "  'dresses',\n",
       "  'but',\n",
       "  'this',\n",
       "  'one',\n",
       "  'is',\n",
       "  'not',\n",
       "  'for',\n",
       "  'the',\n",
       "  'very',\n",
       "  'petite',\n",
       "  'i',\n",
       "  'am',\n",
       "  'just',\n",
       "  'under',\n",
       "  '5',\n",
       "  'feet',\n",
       "  'tall',\n",
       "  'and',\n",
       "  'usually',\n",
       "  'wear',\n",
       "  'a',\n",
       "  '0p',\n",
       "  'in',\n",
       "  'this',\n",
       "  'brand',\n",
       "  'this',\n",
       "  'dress',\n",
       "  'was',\n",
       "  'very',\n",
       "  'pretty',\n",
       "  'out',\n",
       "  'of',\n",
       "  'the',\n",
       "  'package',\n",
       "  'but',\n",
       "  'its',\n",
       "  'a',\n",
       "  'lot',\n",
       "  'of',\n",
       "  'dress',\n",
       "  'the',\n",
       "  'skirt',\n",
       "  'is',\n",
       "  'long',\n",
       "  'and',\n",
       "  'very',\n",
       "  'full',\n",
       "  'so',\n",
       "  'it',\n",
       "  'overwhelmed',\n",
       "  'my',\n",
       "  'small',\n",
       "  'frame',\n",
       "  'not',\n",
       "  'a',\n",
       "  'stranger',\n",
       "  'to',\n",
       "  'alterations',\n",
       "  'shortening',\n",
       "  'and',\n",
       "  'narrowing',\n",
       "  'the',\n",
       "  'skirt',\n",
       "  'would',\n",
       "  'take',\n",
       "  'away',\n",
       "  'from',\n",
       "  'the',\n",
       "  'embellishment',\n",
       "  'of',\n",
       "  'the',\n",
       "  'garment',\n",
       "  'i',\n",
       "  'love',\n",
       "  'the',\n",
       "  'color',\n",
       "  'and',\n",
       "  'the',\n",
       "  'idea',\n",
       "  'of',\n",
       "  'the',\n",
       "  'style',\n",
       "  'but',\n",
       "  'it',\n",
       "  'just',\n",
       "  'did',\n",
       "  'not',\n",
       "  'work',\n",
       "  'on',\n",
       "  'me',\n",
       "  'i',\n",
       "  'returned',\n",
       "  'this',\n",
       "  'dress'],\n",
       " ['i',\n",
       "  'aded',\n",
       "  'this',\n",
       "  'in',\n",
       "  'my',\n",
       "  'basket',\n",
       "  'at',\n",
       "  'hte',\n",
       "  'last',\n",
       "  'mintue',\n",
       "  'to',\n",
       "  'see',\n",
       "  'what',\n",
       "  'it',\n",
       "  'would',\n",
       "  'look',\n",
       "  'like',\n",
       "  'in',\n",
       "  'person',\n",
       "  'store',\n",
       "  'pick',\n",
       "  'up',\n",
       "  'i',\n",
       "  'went',\n",
       "  'with',\n",
       "  'teh',\n",
       "  'darkler',\n",
       "  'color',\n",
       "  'only',\n",
       "  'because',\n",
       "  'i',\n",
       "  'am',\n",
       "  'so',\n",
       "  'pale',\n",
       "  'hte',\n",
       "  'color',\n",
       "  'is',\n",
       "  'really',\n",
       "  'gorgeous',\n",
       "  'and',\n",
       "  'turns',\n",
       "  'out',\n",
       "  'it',\n",
       "  'mathced',\n",
       "  'everythiing',\n",
       "  'i',\n",
       "  'was',\n",
       "  'trying',\n",
       "  'on',\n",
       "  'with',\n",
       "  'it',\n",
       "  'prefectly',\n",
       "  'it',\n",
       "  'is',\n",
       "  'a',\n",
       "  'little',\n",
       "  'baggy',\n",
       "  'on',\n",
       "  'me',\n",
       "  'and',\n",
       "  'hte',\n",
       "  'xs',\n",
       "  'is',\n",
       "  'hte',\n",
       "  'msallet',\n",
       "  'size',\n",
       "  'bummer',\n",
       "  'no',\n",
       "  'petite',\n",
       "  'i',\n",
       "  'decided',\n",
       "  'to',\n",
       "  'jkeep',\n",
       "  'it',\n",
       "  'though',\n",
       "  'because',\n",
       "  'as',\n",
       "  'i',\n",
       "  'said',\n",
       "  'it',\n",
       "  'matvehd',\n",
       "  'everything',\n",
       "  'my',\n",
       "  'ejans',\n",
       "  'pants',\n",
       "  'and',\n",
       "  'the',\n",
       "  '3',\n",
       "  'skirts',\n",
       "  'i',\n",
       "  'waas',\n",
       "  'trying',\n",
       "  'on',\n",
       "  'of',\n",
       "  'which',\n",
       "  'i',\n",
       "  'kept',\n",
       "  'all',\n",
       "  'oops'],\n",
       " ['i',\n",
       "  'ordered',\n",
       "  'this',\n",
       "  'in',\n",
       "  'carbon',\n",
       "  'for',\n",
       "  'store',\n",
       "  'pick',\n",
       "  'up',\n",
       "  'and',\n",
       "  'had',\n",
       "  'a',\n",
       "  'ton',\n",
       "  'of',\n",
       "  'stuff',\n",
       "  'as',\n",
       "  'always',\n",
       "  'to',\n",
       "  'try',\n",
       "  'on',\n",
       "  'and',\n",
       "  'used',\n",
       "  'this',\n",
       "  'top',\n",
       "  'to',\n",
       "  'pair',\n",
       "  'skirts',\n",
       "  'and',\n",
       "  'pants',\n",
       "  'everything',\n",
       "  'went',\n",
       "  'with',\n",
       "  'it',\n",
       "  'the',\n",
       "  'color',\n",
       "  'is',\n",
       "  'really',\n",
       "  'nice',\n",
       "  'charcoal',\n",
       "  'with',\n",
       "  'shimmer',\n",
       "  'and',\n",
       "  'went',\n",
       "  'well',\n",
       "  'with',\n",
       "  'pencil',\n",
       "  'skirts',\n",
       "  'flare',\n",
       "  'pants',\n",
       "  'etc',\n",
       "  'my',\n",
       "  'only',\n",
       "  'compaint',\n",
       "  'is',\n",
       "  'it',\n",
       "  'is',\n",
       "  'a',\n",
       "  'bit',\n",
       "  'big',\n",
       "  'sleeves',\n",
       "  'are',\n",
       "  'long',\n",
       "  'and',\n",
       "  'it',\n",
       "  'doesn',\n",
       "  't',\n",
       "  'go',\n",
       "  'in',\n",
       "  'petite',\n",
       "  'also',\n",
       "  'a',\n",
       "  'bit',\n",
       "  'loose',\n",
       "  'for',\n",
       "  'me',\n",
       "  'but',\n",
       "  'no',\n",
       "  'xxs',\n",
       "  'so',\n",
       "  'i',\n",
       "  'kept',\n",
       "  'it',\n",
       "  'and',\n",
       "  'wil',\n",
       "  'ldecide',\n",
       "  'later',\n",
       "  'since',\n",
       "  'the',\n",
       "  'light',\n",
       "  'color',\n",
       "  'is',\n",
       "  'already',\n",
       "  'sold',\n",
       "  'out',\n",
       "  'in',\n",
       "  'hte',\n",
       "  'smallest',\n",
       "  'size'],\n",
       " ['i',\n",
       "  'love',\n",
       "  'this',\n",
       "  'dress',\n",
       "  'i',\n",
       "  'usually',\n",
       "  'get',\n",
       "  'an',\n",
       "  'xs',\n",
       "  'but',\n",
       "  'it',\n",
       "  'runs',\n",
       "  'a',\n",
       "  'little',\n",
       "  'snug',\n",
       "  'in',\n",
       "  'bust',\n",
       "  'so',\n",
       "  'i',\n",
       "  'ordered',\n",
       "  'up',\n",
       "  'a',\n",
       "  'size',\n",
       "  'very',\n",
       "  'flattering',\n",
       "  'and',\n",
       "  'feminine',\n",
       "  'with',\n",
       "  'the',\n",
       "  'usual',\n",
       "  'retailer',\n",
       "  'flair',\n",
       "  'for',\n",
       "  'style'],\n",
       " ['i',\n",
       "  'm',\n",
       "  '5',\n",
       "  '5',\n",
       "  'and',\n",
       "  '125',\n",
       "  'lbs',\n",
       "  'i',\n",
       "  'ordered',\n",
       "  'the',\n",
       "  's',\n",
       "  'petite',\n",
       "  'to',\n",
       "  'make',\n",
       "  'sure',\n",
       "  'the',\n",
       "  'length',\n",
       "  'wasn',\n",
       "  't',\n",
       "  'too',\n",
       "  'long',\n",
       "  'i',\n",
       "  'typically',\n",
       "  'wear',\n",
       "  'an',\n",
       "  'xs',\n",
       "  'regular',\n",
       "  'in',\n",
       "  'retailer',\n",
       "  'dresses',\n",
       "  'if',\n",
       "  'you',\n",
       "  're',\n",
       "  'less',\n",
       "  'busty',\n",
       "  '34b',\n",
       "  'cup',\n",
       "  'or',\n",
       "  'smaller',\n",
       "  'a',\n",
       "  's',\n",
       "  'petite',\n",
       "  'will',\n",
       "  'fit',\n",
       "  'you',\n",
       "  'perfectly',\n",
       "  'snug',\n",
       "  'but',\n",
       "  'not',\n",
       "  'tight',\n",
       "  'i',\n",
       "  'love',\n",
       "  'that',\n",
       "  'i',\n",
       "  'could',\n",
       "  'dress',\n",
       "  'it',\n",
       "  'up',\n",
       "  'for',\n",
       "  'a',\n",
       "  'party',\n",
       "  'or',\n",
       "  'down',\n",
       "  'for',\n",
       "  'work',\n",
       "  'i',\n",
       "  'love',\n",
       "  'that',\n",
       "  'the',\n",
       "  'tulle',\n",
       "  'is',\n",
       "  'longer',\n",
       "  'then',\n",
       "  'the',\n",
       "  'fabric',\n",
       "  'underneath'],\n",
       " ['dress',\n",
       "  'runs',\n",
       "  'small',\n",
       "  'esp',\n",
       "  'where',\n",
       "  'the',\n",
       "  'zipper',\n",
       "  'area',\n",
       "  'runs',\n",
       "  'i',\n",
       "  'ordered',\n",
       "  'the',\n",
       "  'sp',\n",
       "  'which',\n",
       "  'typically',\n",
       "  'fits',\n",
       "  'me',\n",
       "  'and',\n",
       "  'it',\n",
       "  'was',\n",
       "  'very',\n",
       "  'tight',\n",
       "  'the',\n",
       "  'material',\n",
       "  'on',\n",
       "  'the',\n",
       "  'top',\n",
       "  'looks',\n",
       "  'and',\n",
       "  'feels',\n",
       "  'very',\n",
       "  'cheap',\n",
       "  'that',\n",
       "  'even',\n",
       "  'just',\n",
       "  'pulling',\n",
       "  'on',\n",
       "  'it',\n",
       "  'will',\n",
       "  'cause',\n",
       "  'it',\n",
       "  'to',\n",
       "  'rip',\n",
       "  'the',\n",
       "  'fabric',\n",
       "  'pretty',\n",
       "  'disappointed',\n",
       "  'as',\n",
       "  'it',\n",
       "  'was',\n",
       "  'going',\n",
       "  'to',\n",
       "  'be',\n",
       "  'my',\n",
       "  'christmas',\n",
       "  'dress',\n",
       "  'this',\n",
       "  'year',\n",
       "  'needless',\n",
       "  'to',\n",
       "  'say',\n",
       "  'it',\n",
       "  'will',\n",
       "  'be',\n",
       "  'going',\n",
       "  'back'],\n",
       " ['more',\n",
       "  'and',\n",
       "  'more',\n",
       "  'i',\n",
       "  'find',\n",
       "  'myself',\n",
       "  'reliant',\n",
       "  'on',\n",
       "  'the',\n",
       "  'reviews',\n",
       "  'written',\n",
       "  'by',\n",
       "  'savvy',\n",
       "  'shoppers',\n",
       "  'before',\n",
       "  'me',\n",
       "  'and',\n",
       "  'for',\n",
       "  'the',\n",
       "  'most',\n",
       "  'past',\n",
       "  'they',\n",
       "  'are',\n",
       "  'right',\n",
       "  'on',\n",
       "  'in',\n",
       "  'their',\n",
       "  'estimation',\n",
       "  'of',\n",
       "  'the',\n",
       "  'product',\n",
       "  'in',\n",
       "  'the',\n",
       "  'case',\n",
       "  'of',\n",
       "  'this',\n",
       "  'dress',\n",
       "  'if',\n",
       "  'it',\n",
       "  'had',\n",
       "  'not',\n",
       "  'been',\n",
       "  'for',\n",
       "  'the',\n",
       "  'reveiws',\n",
       "  'i',\n",
       "  'doubt',\n",
       "  'i',\n",
       "  'would',\n",
       "  'have',\n",
       "  'even',\n",
       "  'tried',\n",
       "  'this',\n",
       "  'the',\n",
       "  'dress',\n",
       "  'is',\n",
       "  'beautifully',\n",
       "  'made',\n",
       "  'lined',\n",
       "  'and',\n",
       "  'reminiscent',\n",
       "  'of',\n",
       "  'the',\n",
       "  'old',\n",
       "  'retailer',\n",
       "  'quality',\n",
       "  'it',\n",
       "  'is',\n",
       "  'lined',\n",
       "  'in',\n",
       "  'the',\n",
       "  'solid',\n",
       "  'periwinkle',\n",
       "  'colored',\n",
       "  'fabric',\n",
       "  'that',\n",
       "  'matches',\n",
       "  'the',\n",
       "  'outer',\n",
       "  'fabric',\n",
       "  'print',\n",
       "  'tts',\n",
       "  'and',\n",
       "  'very',\n",
       "  'form',\n",
       "  'fitting',\n",
       "  'falls',\n",
       "  'just',\n",
       "  'above',\n",
       "  'the',\n",
       "  'knee',\n",
       "  'and',\n",
       "  'does',\n",
       "  'not',\n",
       "  'rid']]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer('\\w+')\n",
    "liste_avis_clean = [tokenizer.tokenize(str(avis)) for avis in liste_avis_clean]\n",
    "liste_avis_clean[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, on supprime la ponctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i',\n",
       "  'had',\n",
       "  'such',\n",
       "  'high',\n",
       "  'hopes',\n",
       "  'for',\n",
       "  'this',\n",
       "  'dress',\n",
       "  'and',\n",
       "  'really',\n",
       "  'wanted',\n",
       "  'it',\n",
       "  'to',\n",
       "  'work',\n",
       "  'for',\n",
       "  'me',\n",
       "  'i',\n",
       "  'initially',\n",
       "  'ordered',\n",
       "  'the',\n",
       "  'petite',\n",
       "  'small',\n",
       "  'my',\n",
       "  'usual',\n",
       "  'size',\n",
       "  'but',\n",
       "  'i',\n",
       "  'found',\n",
       "  'this',\n",
       "  'to',\n",
       "  'be',\n",
       "  'outrageously',\n",
       "  'small',\n",
       "  'so',\n",
       "  'small',\n",
       "  'in',\n",
       "  'fact',\n",
       "  'that',\n",
       "  'i',\n",
       "  'could',\n",
       "  'not',\n",
       "  'zip',\n",
       "  'it',\n",
       "  'up',\n",
       "  'i',\n",
       "  'reordered',\n",
       "  'it',\n",
       "  'in',\n",
       "  'petite',\n",
       "  'medium',\n",
       "  'which',\n",
       "  'was',\n",
       "  'just',\n",
       "  'ok',\n",
       "  'overall',\n",
       "  'the',\n",
       "  'top',\n",
       "  'half',\n",
       "  'was',\n",
       "  'comfortable',\n",
       "  'and',\n",
       "  'fit',\n",
       "  'nicely',\n",
       "  'but',\n",
       "  'the',\n",
       "  'bottom',\n",
       "  'half',\n",
       "  'had',\n",
       "  'a',\n",
       "  'very',\n",
       "  'tight',\n",
       "  'under',\n",
       "  'layer',\n",
       "  'and',\n",
       "  'several',\n",
       "  'somewhat',\n",
       "  'cheap',\n",
       "  'net',\n",
       "  'over',\n",
       "  'layers',\n",
       "  'imo',\n",
       "  'a',\n",
       "  'major',\n",
       "  'design',\n",
       "  'flaw',\n",
       "  'was',\n",
       "  'the',\n",
       "  'net',\n",
       "  'over',\n",
       "  'layer',\n",
       "  'sewn',\n",
       "  'directly',\n",
       "  'into',\n",
       "  'the',\n",
       "  'zipper',\n",
       "  'it',\n",
       "  'c'],\n",
       " ['i',\n",
       "  'love',\n",
       "  'love',\n",
       "  'love',\n",
       "  'this',\n",
       "  'jumpsuit',\n",
       "  'it',\n",
       "  's',\n",
       "  'fun',\n",
       "  'flirty',\n",
       "  'and',\n",
       "  'fabulous',\n",
       "  'every',\n",
       "  'time',\n",
       "  'i',\n",
       "  'wear',\n",
       "  'it',\n",
       "  'i',\n",
       "  'get',\n",
       "  'nothing',\n",
       "  'but',\n",
       "  'great',\n",
       "  'compliments'],\n",
       " ['this',\n",
       "  'shirt',\n",
       "  'is',\n",
       "  'very',\n",
       "  'flattering',\n",
       "  'to',\n",
       "  'all',\n",
       "  'due',\n",
       "  'to',\n",
       "  'the',\n",
       "  'adjustable',\n",
       "  'front',\n",
       "  'tie',\n",
       "  'it',\n",
       "  'is',\n",
       "  'the',\n",
       "  'perfect',\n",
       "  'length',\n",
       "  'to',\n",
       "  'wear',\n",
       "  'with',\n",
       "  'leggings',\n",
       "  'and',\n",
       "  'it',\n",
       "  'is',\n",
       "  'sleeveless',\n",
       "  'so',\n",
       "  'it',\n",
       "  'pairs',\n",
       "  'well',\n",
       "  'with',\n",
       "  'any',\n",
       "  'cardigan',\n",
       "  'love',\n",
       "  'this',\n",
       "  'shirt'],\n",
       " ['i',\n",
       "  'love',\n",
       "  'tracy',\n",
       "  'reese',\n",
       "  'dresses',\n",
       "  'but',\n",
       "  'this',\n",
       "  'one',\n",
       "  'is',\n",
       "  'not',\n",
       "  'for',\n",
       "  'the',\n",
       "  'very',\n",
       "  'petite',\n",
       "  'i',\n",
       "  'am',\n",
       "  'just',\n",
       "  'under',\n",
       "  '5',\n",
       "  'feet',\n",
       "  'tall',\n",
       "  'and',\n",
       "  'usually',\n",
       "  'wear',\n",
       "  'a',\n",
       "  '0p',\n",
       "  'in',\n",
       "  'this',\n",
       "  'brand',\n",
       "  'this',\n",
       "  'dress',\n",
       "  'was',\n",
       "  'very',\n",
       "  'pretty',\n",
       "  'out',\n",
       "  'of',\n",
       "  'the',\n",
       "  'package',\n",
       "  'but',\n",
       "  'its',\n",
       "  'a',\n",
       "  'lot',\n",
       "  'of',\n",
       "  'dress',\n",
       "  'the',\n",
       "  'skirt',\n",
       "  'is',\n",
       "  'long',\n",
       "  'and',\n",
       "  'very',\n",
       "  'full',\n",
       "  'so',\n",
       "  'it',\n",
       "  'overwhelmed',\n",
       "  'my',\n",
       "  'small',\n",
       "  'frame',\n",
       "  'not',\n",
       "  'a',\n",
       "  'stranger',\n",
       "  'to',\n",
       "  'alterations',\n",
       "  'shortening',\n",
       "  'and',\n",
       "  'narrowing',\n",
       "  'the',\n",
       "  'skirt',\n",
       "  'would',\n",
       "  'take',\n",
       "  'away',\n",
       "  'from',\n",
       "  'the',\n",
       "  'embellishment',\n",
       "  'of',\n",
       "  'the',\n",
       "  'garment',\n",
       "  'i',\n",
       "  'love',\n",
       "  'the',\n",
       "  'color',\n",
       "  'and',\n",
       "  'the',\n",
       "  'idea',\n",
       "  'of',\n",
       "  'the',\n",
       "  'style',\n",
       "  'but',\n",
       "  'it',\n",
       "  'just',\n",
       "  'did',\n",
       "  'not',\n",
       "  'work',\n",
       "  'on',\n",
       "  'me',\n",
       "  'i',\n",
       "  'returned',\n",
       "  'this',\n",
       "  'dress'],\n",
       " ['i',\n",
       "  'aded',\n",
       "  'this',\n",
       "  'in',\n",
       "  'my',\n",
       "  'basket',\n",
       "  'at',\n",
       "  'hte',\n",
       "  'last',\n",
       "  'mintue',\n",
       "  'to',\n",
       "  'see',\n",
       "  'what',\n",
       "  'it',\n",
       "  'would',\n",
       "  'look',\n",
       "  'like',\n",
       "  'in',\n",
       "  'person',\n",
       "  'store',\n",
       "  'pick',\n",
       "  'up',\n",
       "  'i',\n",
       "  'went',\n",
       "  'with',\n",
       "  'teh',\n",
       "  'darkler',\n",
       "  'color',\n",
       "  'only',\n",
       "  'because',\n",
       "  'i',\n",
       "  'am',\n",
       "  'so',\n",
       "  'pale',\n",
       "  'hte',\n",
       "  'color',\n",
       "  'is',\n",
       "  'really',\n",
       "  'gorgeous',\n",
       "  'and',\n",
       "  'turns',\n",
       "  'out',\n",
       "  'it',\n",
       "  'mathced',\n",
       "  'everythiing',\n",
       "  'i',\n",
       "  'was',\n",
       "  'trying',\n",
       "  'on',\n",
       "  'with',\n",
       "  'it',\n",
       "  'prefectly',\n",
       "  'it',\n",
       "  'is',\n",
       "  'a',\n",
       "  'little',\n",
       "  'baggy',\n",
       "  'on',\n",
       "  'me',\n",
       "  'and',\n",
       "  'hte',\n",
       "  'xs',\n",
       "  'is',\n",
       "  'hte',\n",
       "  'msallet',\n",
       "  'size',\n",
       "  'bummer',\n",
       "  'no',\n",
       "  'petite',\n",
       "  'i',\n",
       "  'decided',\n",
       "  'to',\n",
       "  'jkeep',\n",
       "  'it',\n",
       "  'though',\n",
       "  'because',\n",
       "  'as',\n",
       "  'i',\n",
       "  'said',\n",
       "  'it',\n",
       "  'matvehd',\n",
       "  'everything',\n",
       "  'my',\n",
       "  'ejans',\n",
       "  'pants',\n",
       "  'and',\n",
       "  'the',\n",
       "  '3',\n",
       "  'skirts',\n",
       "  'i',\n",
       "  'waas',\n",
       "  'trying',\n",
       "  'on',\n",
       "  'of',\n",
       "  'which',\n",
       "  'i',\n",
       "  'kept',\n",
       "  'all',\n",
       "  'oops'],\n",
       " ['i',\n",
       "  'ordered',\n",
       "  'this',\n",
       "  'in',\n",
       "  'carbon',\n",
       "  'for',\n",
       "  'store',\n",
       "  'pick',\n",
       "  'up',\n",
       "  'and',\n",
       "  'had',\n",
       "  'a',\n",
       "  'ton',\n",
       "  'of',\n",
       "  'stuff',\n",
       "  'as',\n",
       "  'always',\n",
       "  'to',\n",
       "  'try',\n",
       "  'on',\n",
       "  'and',\n",
       "  'used',\n",
       "  'this',\n",
       "  'top',\n",
       "  'to',\n",
       "  'pair',\n",
       "  'skirts',\n",
       "  'and',\n",
       "  'pants',\n",
       "  'everything',\n",
       "  'went',\n",
       "  'with',\n",
       "  'it',\n",
       "  'the',\n",
       "  'color',\n",
       "  'is',\n",
       "  'really',\n",
       "  'nice',\n",
       "  'charcoal',\n",
       "  'with',\n",
       "  'shimmer',\n",
       "  'and',\n",
       "  'went',\n",
       "  'well',\n",
       "  'with',\n",
       "  'pencil',\n",
       "  'skirts',\n",
       "  'flare',\n",
       "  'pants',\n",
       "  'etc',\n",
       "  'my',\n",
       "  'only',\n",
       "  'compaint',\n",
       "  'is',\n",
       "  'it',\n",
       "  'is',\n",
       "  'a',\n",
       "  'bit',\n",
       "  'big',\n",
       "  'sleeves',\n",
       "  'are',\n",
       "  'long',\n",
       "  'and',\n",
       "  'it',\n",
       "  'doesn',\n",
       "  't',\n",
       "  'go',\n",
       "  'in',\n",
       "  'petite',\n",
       "  'also',\n",
       "  'a',\n",
       "  'bit',\n",
       "  'loose',\n",
       "  'for',\n",
       "  'me',\n",
       "  'but',\n",
       "  'no',\n",
       "  'xxs',\n",
       "  'so',\n",
       "  'i',\n",
       "  'kept',\n",
       "  'it',\n",
       "  'and',\n",
       "  'wil',\n",
       "  'ldecide',\n",
       "  'later',\n",
       "  'since',\n",
       "  'the',\n",
       "  'light',\n",
       "  'color',\n",
       "  'is',\n",
       "  'already',\n",
       "  'sold',\n",
       "  'out',\n",
       "  'in',\n",
       "  'hte',\n",
       "  'smallest',\n",
       "  'size'],\n",
       " ['i',\n",
       "  'love',\n",
       "  'this',\n",
       "  'dress',\n",
       "  'i',\n",
       "  'usually',\n",
       "  'get',\n",
       "  'an',\n",
       "  'xs',\n",
       "  'but',\n",
       "  'it',\n",
       "  'runs',\n",
       "  'a',\n",
       "  'little',\n",
       "  'snug',\n",
       "  'in',\n",
       "  'bust',\n",
       "  'so',\n",
       "  'i',\n",
       "  'ordered',\n",
       "  'up',\n",
       "  'a',\n",
       "  'size',\n",
       "  'very',\n",
       "  'flattering',\n",
       "  'and',\n",
       "  'feminine',\n",
       "  'with',\n",
       "  'the',\n",
       "  'usual',\n",
       "  'retailer',\n",
       "  'flair',\n",
       "  'for',\n",
       "  'style'],\n",
       " ['i',\n",
       "  'm',\n",
       "  '5',\n",
       "  '5',\n",
       "  'and',\n",
       "  '125',\n",
       "  'lbs',\n",
       "  'i',\n",
       "  'ordered',\n",
       "  'the',\n",
       "  's',\n",
       "  'petite',\n",
       "  'to',\n",
       "  'make',\n",
       "  'sure',\n",
       "  'the',\n",
       "  'length',\n",
       "  'wasn',\n",
       "  't',\n",
       "  'too',\n",
       "  'long',\n",
       "  'i',\n",
       "  'typically',\n",
       "  'wear',\n",
       "  'an',\n",
       "  'xs',\n",
       "  'regular',\n",
       "  'in',\n",
       "  'retailer',\n",
       "  'dresses',\n",
       "  'if',\n",
       "  'you',\n",
       "  're',\n",
       "  'less',\n",
       "  'busty',\n",
       "  '34b',\n",
       "  'cup',\n",
       "  'or',\n",
       "  'smaller',\n",
       "  'a',\n",
       "  's',\n",
       "  'petite',\n",
       "  'will',\n",
       "  'fit',\n",
       "  'you',\n",
       "  'perfectly',\n",
       "  'snug',\n",
       "  'but',\n",
       "  'not',\n",
       "  'tight',\n",
       "  'i',\n",
       "  'love',\n",
       "  'that',\n",
       "  'i',\n",
       "  'could',\n",
       "  'dress',\n",
       "  'it',\n",
       "  'up',\n",
       "  'for',\n",
       "  'a',\n",
       "  'party',\n",
       "  'or',\n",
       "  'down',\n",
       "  'for',\n",
       "  'work',\n",
       "  'i',\n",
       "  'love',\n",
       "  'that',\n",
       "  'the',\n",
       "  'tulle',\n",
       "  'is',\n",
       "  'longer',\n",
       "  'then',\n",
       "  'the',\n",
       "  'fabric',\n",
       "  'underneath'],\n",
       " ['dress',\n",
       "  'runs',\n",
       "  'small',\n",
       "  'esp',\n",
       "  'where',\n",
       "  'the',\n",
       "  'zipper',\n",
       "  'area',\n",
       "  'runs',\n",
       "  'i',\n",
       "  'ordered',\n",
       "  'the',\n",
       "  'sp',\n",
       "  'which',\n",
       "  'typically',\n",
       "  'fits',\n",
       "  'me',\n",
       "  'and',\n",
       "  'it',\n",
       "  'was',\n",
       "  'very',\n",
       "  'tight',\n",
       "  'the',\n",
       "  'material',\n",
       "  'on',\n",
       "  'the',\n",
       "  'top',\n",
       "  'looks',\n",
       "  'and',\n",
       "  'feels',\n",
       "  'very',\n",
       "  'cheap',\n",
       "  'that',\n",
       "  'even',\n",
       "  'just',\n",
       "  'pulling',\n",
       "  'on',\n",
       "  'it',\n",
       "  'will',\n",
       "  'cause',\n",
       "  'it',\n",
       "  'to',\n",
       "  'rip',\n",
       "  'the',\n",
       "  'fabric',\n",
       "  'pretty',\n",
       "  'disappointed',\n",
       "  'as',\n",
       "  'it',\n",
       "  'was',\n",
       "  'going',\n",
       "  'to',\n",
       "  'be',\n",
       "  'my',\n",
       "  'christmas',\n",
       "  'dress',\n",
       "  'this',\n",
       "  'year',\n",
       "  'needless',\n",
       "  'to',\n",
       "  'say',\n",
       "  'it',\n",
       "  'will',\n",
       "  'be',\n",
       "  'going',\n",
       "  'back'],\n",
       " ['more',\n",
       "  'and',\n",
       "  'more',\n",
       "  'i',\n",
       "  'find',\n",
       "  'myself',\n",
       "  'reliant',\n",
       "  'on',\n",
       "  'the',\n",
       "  'reviews',\n",
       "  'written',\n",
       "  'by',\n",
       "  'savvy',\n",
       "  'shoppers',\n",
       "  'before',\n",
       "  'me',\n",
       "  'and',\n",
       "  'for',\n",
       "  'the',\n",
       "  'most',\n",
       "  'past',\n",
       "  'they',\n",
       "  'are',\n",
       "  'right',\n",
       "  'on',\n",
       "  'in',\n",
       "  'their',\n",
       "  'estimation',\n",
       "  'of',\n",
       "  'the',\n",
       "  'product',\n",
       "  'in',\n",
       "  'the',\n",
       "  'case',\n",
       "  'of',\n",
       "  'this',\n",
       "  'dress',\n",
       "  'if',\n",
       "  'it',\n",
       "  'had',\n",
       "  'not',\n",
       "  'been',\n",
       "  'for',\n",
       "  'the',\n",
       "  'reveiws',\n",
       "  'i',\n",
       "  'doubt',\n",
       "  'i',\n",
       "  'would',\n",
       "  'have',\n",
       "  'even',\n",
       "  'tried',\n",
       "  'this',\n",
       "  'the',\n",
       "  'dress',\n",
       "  'is',\n",
       "  'beautifully',\n",
       "  'made',\n",
       "  'lined',\n",
       "  'and',\n",
       "  'reminiscent',\n",
       "  'of',\n",
       "  'the',\n",
       "  'old',\n",
       "  'retailer',\n",
       "  'quality',\n",
       "  'it',\n",
       "  'is',\n",
       "  'lined',\n",
       "  'in',\n",
       "  'the',\n",
       "  'solid',\n",
       "  'periwinkle',\n",
       "  'colored',\n",
       "  'fabric',\n",
       "  'that',\n",
       "  'matches',\n",
       "  'the',\n",
       "  'outer',\n",
       "  'fabric',\n",
       "  'print',\n",
       "  'tts',\n",
       "  'and',\n",
       "  'very',\n",
       "  'form',\n",
       "  'fitting',\n",
       "  'falls',\n",
       "  'just',\n",
       "  'above',\n",
       "  'the',\n",
       "  'knee',\n",
       "  'and',\n",
       "  'does',\n",
       "  'not',\n",
       "  'rid']]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "punct = string.punctuation\n",
    "\n",
    "# Pour chaque token dans chaque avis, si le token n'est pas dans la liste des ponctuations, on le garde\n",
    "liste_avis_clean = [[token for token in avis if token not in punct] for avis in liste_avis_clean]\n",
    "liste_avis_clean[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traitement et séparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie, nous allons chercher à classifier les avis en fonction de leur note. \n",
    "\n",
    "Nous allons utiliser la colonne \"Rating\" comme étiquettes et \"Review Text\" comme valeurs. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sélection des informations dans notre dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons sélectionner les trois colonnes qui vont nous servir pour la classification dans un objectif d'optimiser les temps de calculs et de ne pas avoir d'informations superflus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>I love tracy reese dresses, but this one is no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>I aded this in my basket at hte last mintue to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  Rating                                        Review Text\n",
       "2   2       3  I had such high hopes for this dress and reall...\n",
       "3   3       5  I love, love, love this jumpsuit. it's fun, fl...\n",
       "4   4       5  This shirt is very flattering to all due to th...\n",
       "5   5       2  I love tracy reese dresses, but this one is no...\n",
       "6   6       5  I aded this in my basket at hte last mintue to..."
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On récupère la colonne id, Rating et Review Text\n",
    "data = data[[\"id\", \"Rating\", \"Review Text\"]]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse de la colonne \"Rating\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rating\n",
       "5    10858\n",
       "4     4289\n",
       "3     2464\n",
       "2     1360\n",
       "1      691\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyse de la colonne \"Rating\"\n",
    "data[\"Rating\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons ici des notes allant de 1 à 5. Nous allons diviser ces valeurs en 3 catégories : \n",
    "- -1 pour les notes allant de 1 à 2\n",
    "- 0 pour les notes égales à 3 \n",
    "- 1 pour les plus élevées (4 et 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse de la colonne \"Review Text\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre colonne correspondant aux valeurs est \"Review Text\". \\\n",
    "Cette colonne contient tous les avis laissés par les internautes sur les différents vêtements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changement des étiquettes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour réaliser notre classification, nous allons donc modifier les étiquettes comme précisé ci-dessus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_label_to_numeric(label):\n",
    "    return 1 if label == 5 else 0 if label == 3 or label == 4 else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(data):\n",
    "    labels = data[[\"id\",\"Rating\"]]\n",
    "    labels['Rating'] = labels['Rating'].apply(map_label_to_numeric)\n",
    "    labels.set_index('id', inplace=True)\n",
    "    \n",
    "    # ajouter les labels dans data selon l'id\n",
    "    data['score_avis'] = labels\n",
    "\n",
    "    # data['score_avis'] = labels\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_7908\\29256686.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labels['Rating'] = labels['Rating'].apply(map_label_to_numeric)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>score_avis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>I love tracy reese dresses, but this one is no...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>I aded this in my basket at hte last mintue to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  Rating                                        Review Text  score_avis\n",
       "2   2       3  I had such high hopes for this dress and reall...           0\n",
       "3   3       5  I love, love, love this jumpsuit. it's fun, fl...           1\n",
       "4   4       5  This shirt is very flattering to all due to th...           1\n",
       "5   5       2  I love tracy reese dresses, but this one is no...          -1\n",
       "6   6       5  I aded this in my basket at hte last mintue to...           1"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = get_labels(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score_avis\n",
       " 1    10858\n",
       " 0     6753\n",
       "-1     2051\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On analyse la nouvelle colonne \"score_avis\"\n",
    "data[\"score_avis\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grâce à cette manipulation, nous pouvons observer que les avis ayant la note de 5 sont majoritaires dans notre jeu de données puisque cela correspond à la note de 1. Les avis compris entre 1 et 2 ont une proportion plus faible (-1). \n",
    "\n",
    "A présent, nous n'avons plus besoin de la colonne Rating, nous pouvons donc la supprimer du dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>score_avis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>I love tracy reese dresses, but this one is no...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>I aded this in my basket at hte last mintue to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                        Review Text  score_avis\n",
       "2   2  I had such high hopes for this dress and reall...           0\n",
       "3   3  I love, love, love this jumpsuit. it's fun, fl...           1\n",
       "4   4  This shirt is very flattering to all due to th...           1\n",
       "5   5  I love tracy reese dresses, but this one is no...          -1\n",
       "6   6  I aded this in my basket at hte last mintue to...           1"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[[\"id\", \"Review Text\", \"score_avis\"]]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Division de notre dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour réaliser notre classification, nous avons besoin de séparer notre jeu de données en un jeu d'apprentissage, de validation et de test.\n",
    "\n",
    "Note :\n",
    "Les données en apprentissage automatique sont généralement séparées en trois jeux :\n",
    "+ **entraînement** : données destinées à l'apprentissage du modèle ;\n",
    "+ **validation** : données destinées à une évaluation intermédiaire du modèle pour permettre l'ajustement de ses hyperparamètres. Une fois les hyperparamètres du modèle arrêtés, on peut le ré-entraîner sur l'ensemble des données (entraînement + validation) avant de le tester sur le jeu de test ;\n",
    "+ **test** : données destinées EXCLUSIVEMENT à l'évaluation FINALE (à réaliser une fois uniquement !) du modèle choisi finalement. Elles ne doivent sous aucune forme servir à la conception du modèle. Il est donc interdit aussi bien de les examiner que d'évaluer le modèle en cours de développement sur ce jeu de données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour créer l'ensemble de validation, nous allons effectuer la manipulation à la fin du pré-traitement réalisé lors de la classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour diviser de notre jeu de données en 2 : train et test\n",
    "def split_data(data, train_ratio):\n",
    "    data_train = data.sample(frac = train_ratio)\n",
    "    data_test = data.drop(data_train.index)\n",
    "    return data_train, data_test\n",
    "\n",
    "# Diviser notre jeu de données en 2 : train et test\n",
    "data_train, data_test = split_data(data, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11797, 3), (5243, 3), (2622, 3))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape, data_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans notre cas :\n",
    "+ entraînement (appelé *Train*) contenant 11797 observations ;\n",
    "+ validation (appelé *Validation*) contenant 5243 observations ;\n",
    "+ test (appelé *Test*), contenant 2622 observations, soit environ 22% de la taille du jeu d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>score_avis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5179</th>\n",
       "      <td>5179</td>\n",
       "      <td>I have loved ag stevie ankle denim for awhile ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12420</th>\n",
       "      <td>12420</td>\n",
       "      <td>So this top is kind of short, but that is pict...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20169</th>\n",
       "      <td>20169</td>\n",
       "      <td>I tried this on in-store in white and in grey....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20161</th>\n",
       "      <td>20161</td>\n",
       "      <td>Lovely color and sweet top, but the fabric was...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5969</th>\n",
       "      <td>5969</td>\n",
       "      <td>Definitely size down, the waist and chest were...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                        Review Text  score_avis\n",
       "5179    5179  I have loved ag stevie ankle denim for awhile ...           1\n",
       "12420  12420  So this top is kind of short, but that is pict...           1\n",
       "20169  20169  I tried this on in-store in white and in grey....           1\n",
       "20161  20161  Lovely color and sweet top, but the fabric was...           0\n",
       "5969    5969  Definitely size down, the waist and chest were...           0"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>score_avis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>I love this dress. i usually get an xs but it ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>I love this shirt because when i first saw it,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>Loved the material, but i didnt really look at...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>This poncho is so cute i love the plaid check ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>This sweater is perfect for fall...it's roomy,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                        Review Text  score_avis\n",
       "8    8  I love this dress. i usually get an xs but it ...           1\n",
       "24  24  I love this shirt because when i first saw it,...           1\n",
       "25  25  Loved the material, but i didnt really look at...           0\n",
       "42  42  This poncho is so cute i love the plaid check ...           1\n",
       "48  48  This sweater is perfect for fall...it's roomy,...           1"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution des classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est important de connaître la répartition des classes dans les données d'entraînement pour pouvoir procéder à notre classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_avis\n",
      " 1    6492\n",
      " 0    4076\n",
      "-1    1229\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "score_avis\n",
       " 1    0.550309\n",
       " 0    0.345512\n",
       "-1    0.104179\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyse de la colonne \"score_avis\" de notre jeu de données d'entrainement\n",
    "print(data_train[\"score_avis\"].value_counts())\n",
    "\n",
    "# Calcul des proportions de chaque classe dans notre jeu de données d'entrainement\n",
    "data_train[\"score_avis\"].value_counts()/len(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons observer que les classes de scores sont réparties de manière aléatoire dans notre jeu d'apprentissage. Nous pouvons noter plus de 50% d'avis très favorables, correspondant à la note de 5/5. Les avis négatifs sont en minorité dans notre jeu d'entraînement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration du texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour se faire une idée des textes auxquels nous avons affaire, nous allons les afficher pour savoir quels pré-traitements sont nécessaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['I have loved ag stevie ankle denim for awhile now and have never been disappointed with the fit or quality. this pair is no exception to that. i am 5\\'5\" and about 113, slender with athletic thighs and the 26 fit great. buying them on-line, i wasn\\'t able to check how distressed the denim was. the pair i received in the mail was indeed slightly different in the distressed locations from the on-line image. mine had no distressing on the upper thigh and one medium and one small spot on each leg. onl',\n",
       "       \"So this top is kind of short, but that is pictured. it hits right where it does in the photo, which i like. it is a sheer material, not terribly so, but you will most likely need a cami, which you could pair a long one with to offset the shorter top if you don't like the shortness. i love the top, it's very pretty detailing, cute, just as pictured. i ordered a size 2 and it fits great. i usually wear petites, but you really don't need to get a petite because it's short as it is. i have broad sho\",\n",
       "       'I tried this on in-store in white and in grey. it is so super comfy and really cute. i love the wide straps! i ended up getting it in white and black. the grey looked a tad bit more casual than the white and black tanks.',\n",
       "       \"Lovely color and sweet top, but the fabric was so itchy i couldn't wear it.\",\n",
       "       'Definitely size down, the waist and chest were baggy in my normal size s.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Affichage des 5 premiers avis\n",
    "data_train[\"Review Text\"].values[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Représentation des textes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sélection de descripteurs : prétraitements textuels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemple sur un avis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'So, i\\'m really surprised that reviewers said the fit was too small and too tight in the arms. i\\'m 5\\'7\" and 183lbs and my husband (god bless his soul) bought me an 8 for my birthday. i tried it on just to see and was pretty impressed i could button it across my chest and get it over my arms as those are common problem areas for me. regardless i need to exchange the shirt for a larger size because even though i got it on its not how is supposed to look. the shirt is loose around the midsection whi'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw = data_train['Review Text'].iloc[100]\n",
    "tw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour effectuer nos tâches de traitements de langage, nous allons transformer le 1er avis en utilisant spacy. En effet, spacy a plusieurs fonctionnalités : \n",
    "- permet de tokeniser directement notre texte\n",
    "- peut lemmatiser les mots\n",
    "- identifier et classer les entités nommées\n",
    "- analyser les dépendances syntaxiques entre les mots\n",
    "- représenter les mots sous forme de vecteurs (embedding)\n",
    "- etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\asus\\anaconda3\\lib\\site-packages (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (0.3.3)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "\n",
      "⠙ Loading compatibility table...\n",
      "⠹ Loading compatibility table...\n",
      "⠸ Loading compatibility table...\n",
      "⠼ Loading compatibility table...\n",
      "\u001b[2K\u001b[38;5;2m✔ Loaded compatibility table\u001b[0m\n",
      "\u001b[1m\n",
      "================= Installed pipeline packages (spaCy v3.7.2) =================\u001b[0m\n",
      "\u001b[38;5;4mℹ spaCy installation:\n",
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\spacy\u001b[0m\n",
      "\n",
      "NAME              SPACY            VERSION                            \n",
      "en_core_web_sm    >=3.7.2,<3.8.0   \u001b[38;5;2m3.7.1\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "fr_core_news_sm   >=3.7.0,<3.8.0   \u001b[38;5;2m3.7.0\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.2/12.8 MB 5.3 MB/s eta 0:00:03\n",
      "     - -------------------------------------- 0.3/12.8 MB 3.4 MB/s eta 0:00:04\n",
      "     - -------------------------------------- 0.5/12.8 MB 3.7 MB/s eta 0:00:04\n",
      "     - -------------------------------------- 0.5/12.8 MB 3.2 MB/s eta 0:00:04\n",
      "     -- ------------------------------------- 0.7/12.8 MB 3.4 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 1.3/12.8 MB 4.6 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 5.4 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 2.4/12.8 MB 6.3 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 2.9/12.8 MB 6.9 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 7.3 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 3.8/12.8 MB 7.2 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 4.3/12.8 MB 7.2 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 4.8/12.8 MB 7.4 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 7.6 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 5.7/12.8 MB 7.7 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 6.1/12.8 MB 7.8 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 6.5/12.8 MB 7.8 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 7.0/12.8 MB 7.9 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 7.5/12.8 MB 7.9 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 8.1/12.8 MB 8.1 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 8.5/12.8 MB 8.1 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 9.3/12.8 MB 8.1 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.9/12.8 MB 8.4 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.3/12.8 MB 8.4 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.4/12.8 MB 8.5 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.4/12.8 MB 8.3 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 10.9/12.8 MB 9.0 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 8.8 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.1/12.8 MB 8.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.5/12.8 MB 8.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.1/12.8 MB 8.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.5/12.8 MB 8.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.7/12.8 MB 8.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 8.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 8.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 8.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 7.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.3)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Pour la langue anglaise\n",
    "!pip install -U spacy\n",
    "! python -m spacy validate\n",
    "\n",
    "import spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "This dress in a lovely platinum is feminine and fits perfectly, easy to wear and comfy, too! highly recommend!"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformation du premier avis en objet spacy\n",
    "avis_nlp = nlp(avis) \n",
    "avis_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(avis_nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a bien un objet de type spacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On affiche chaque token de notre objet spacy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "dress\n",
      "in\n",
      "a\n",
      "lovely\n",
      "platinum\n",
      "is\n",
      "feminine\n",
      "and\n",
      "fits\n",
      "perfectly\n",
      ",\n",
      "easy\n",
      "to\n",
      "wear\n",
      "and\n",
      "comfy\n",
      ",\n",
      "too\n",
      "!\n",
      "highly\n",
      "recommend\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for token in avis_nlp:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous allons pouvoir afficher les lemmes de chaque token. Grâce à cette étape, nous allons pouvoir simplifier les mots pour faciliter notre analyse textuelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this\n",
      "dress\n",
      "in\n",
      "a\n",
      "lovely\n",
      "platinum\n",
      "be\n",
      "feminine\n",
      "and\n",
      "fit\n",
      "perfectly\n",
      ",\n",
      "easy\n",
      "to\n",
      "wear\n",
      "and\n",
      "comfy\n",
      ",\n",
      "too\n",
      "!\n",
      "highly\n",
      "recommend\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "# Lemmatisation\n",
    "for token in avis_nlp:\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Généralisation de la lemmatisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour lemmatiser notre texte, nous allons définir une fonction. Cette étape est indispensable pour récupérer les lemmes des mots de notre texte d'origine. Nous allons simplifier notre texte grâce à cette fonction.\n",
    "\n",
    "Cette fonction va nous permettre de généraliser par la suite nos manipulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatise_text(text):\n",
    "    text = nlp(text) # on transforme le texte en objet spacy\n",
    "    lemmas = [token.lemma_ for token in text] # on récupère les lemmes\n",
    "    return ' '.join(lemmas) # on retourne les lemmes sous forme de texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this dress in a lovely platinum be feminine and fit perfectly , easy to wear and comfy , too ! highly recommend !'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatise_text(avis) # On applique la fonction à notre avis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons ensuite ajouter une colonne avec la fonction de **lemmatisation** appliquée à nos avis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['lemmas'] = data_train['Review Text'].apply(lemmatise_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>score_avis</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5179</th>\n",
       "      <td>5179</td>\n",
       "      <td>I have loved ag stevie ankle denim for awhile ...</td>\n",
       "      <td>1</td>\n",
       "      <td>I have love ag stevie ankle denim for awhile n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12420</th>\n",
       "      <td>12420</td>\n",
       "      <td>So this top is kind of short, but that is pict...</td>\n",
       "      <td>1</td>\n",
       "      <td>so this top be kind of short , but that be pic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20169</th>\n",
       "      <td>20169</td>\n",
       "      <td>I tried this on in-store in white and in grey....</td>\n",
       "      <td>1</td>\n",
       "      <td>I try this on in - store in white and in grey ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20161</th>\n",
       "      <td>20161</td>\n",
       "      <td>Lovely color and sweet top, but the fabric was...</td>\n",
       "      <td>0</td>\n",
       "      <td>lovely color and sweet top , but the fabric be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5969</th>\n",
       "      <td>5969</td>\n",
       "      <td>Definitely size down, the waist and chest were...</td>\n",
       "      <td>0</td>\n",
       "      <td>definitely size down , the waist and chest be ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                        Review Text  score_avis  \\\n",
       "5179    5179  I have loved ag stevie ankle denim for awhile ...           1   \n",
       "12420  12420  So this top is kind of short, but that is pict...           1   \n",
       "20169  20169  I tried this on in-store in white and in grey....           1   \n",
       "20161  20161  Lovely color and sweet top, but the fabric was...           0   \n",
       "5969    5969  Definitely size down, the waist and chest were...           0   \n",
       "\n",
       "                                                  lemmas  \n",
       "5179   I have love ag stevie ankle denim for awhile n...  \n",
       "12420  so this top be kind of short , but that be pic...  \n",
       "20169  I try this on in - store in white and in grey ...  \n",
       "20161  lovely color and sweet top , but the fabric be...  \n",
       "5969   definitely size down , the waist and chest be ...  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On effectue la même manipulation sur l'ensemble de test ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[133], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemmas\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata_test\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mReview Text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlemmatise_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4529\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[129], line 2\u001b[0m, in \u001b[0;36mlemmatise_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatise_text\u001b[39m(text):\n\u001b[1;32m----> 2\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# on transforme le texte en objet spacy\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m [token\u001b[38;5;241m.\u001b[39mlemma_ \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m text] \u001b[38;5;66;03m# on récupère les lemmes\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(lemmas)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\spacy\\language.py:1049\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1047\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1049\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1051\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx:264\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.predict\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx:285\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.greedy_parse\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\thinc\\model.py:334\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutT:\n\u001b[0;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\spacy\\ml\\tb_framework.py:34\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(model, X, is_train):\n\u001b[1;32m---> 34\u001b[0m     step_model \u001b[38;5;241m=\u001b[39m \u001b[43mParserStepModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43munseen_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munseen_classes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_upper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhas_upper\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m step_model, step_model\u001b[38;5;241m.\u001b[39mfinish_steps\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\spacy\\ml\\parser_model.pyx:250\u001b[0m, in \u001b[0;36mspacy.ml.parser_model.ParserStepModel.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\thinc\\layers\\with_array.py:36\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, Xseq, is_train)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m     33\u001b[0m     model: Model[SeqT, SeqT], Xseq: SeqT, is_train: \u001b[38;5;28mbool\u001b[39m\n\u001b[0;32m     34\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[SeqT, Callable]:\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(Xseq, Ragged):\n\u001b[1;32m---> 36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], \u001b[43m_ragged_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(Xseq, Padded):\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], _padded_forward(model, Xseq, is_train))\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\thinc\\layers\\with_array.py:91\u001b[0m, in \u001b[0;36m_ragged_forward\u001b[1;34m(model, Xr, is_train)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ragged_forward\u001b[39m(\n\u001b[0;32m     88\u001b[0m     model: Model[SeqT, SeqT], Xr: Ragged, is_train: \u001b[38;5;28mbool\u001b[39m\n\u001b[0;32m     89\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Ragged, Callable]:\n\u001b[0;32m     90\u001b[0m     layer: Model[ArrayXd, ArrayXd] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 91\u001b[0m     Y, get_dX \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataXd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackprop\u001b[39m(dYr: Ragged) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Ragged:\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Ragged(get_dX(dYr\u001b[38;5;241m.\u001b[39mdataXd), dYr\u001b[38;5;241m.\u001b[39mlengths)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\thinc\\layers\\concatenate.py:65\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(OutT, data_r), backprop\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     data_a, backprop \u001b[38;5;241m=\u001b[39m \u001b[43m_array_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(OutT, data_a), backprop\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\thinc\\layers\\concatenate.py:73\u001b[0m, in \u001b[0;36m_array_forward\u001b[1;34m(model, X, Ys, callbacks, is_train)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_array_forward\u001b[39m(\n\u001b[0;32m     70\u001b[0m     model: Model[InT, OutT], X, Ys: List, callbacks, is_train: \u001b[38;5;28mbool\u001b[39m\n\u001b[0;32m     71\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Array2d, Callable]:\n\u001b[0;32m     72\u001b[0m     widths \u001b[38;5;241m=\u001b[39m [Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m Y \u001b[38;5;129;01min\u001b[39;00m Ys]\n\u001b[1;32m---> 73\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mYs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackprop\u001b[39m(d_output: Array2d) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m InT:\n\u001b[0;32m     76\u001b[0m         dY \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mas_contig(d_output[:, : widths[\u001b[38;5;241m0\u001b[39m]])\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mhstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\numpy\\core\\shape_base.py:370\u001b[0m, in \u001b[0;36mhstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, casting\u001b[38;5;241m=\u001b[39mcasting)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_test['lemmas'] = data_test['Review Text'].apply(lemmatise_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2622, 4)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde\n",
    "data_train.to_pickle('train.pkl')\n",
    "data_test.to_pickle('test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Racines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour réduire les mots à leur forme de base, nous allons utiliser SnowballStemmer sur notre texte. Pour cela, nous allons créer une fonction et l'appliquer à nos avis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'cannot',\n",
       " 'say',\n",
       " 'enough',\n",
       " 'about',\n",
       " 'these',\n",
       " 'pajama',\n",
       " 'pants',\n",
       " '.',\n",
       " \"they're\",\n",
       " 'beautiful',\n",
       " 'and',\n",
       " 'crazy',\n",
       " 'comfortable',\n",
       " '.',\n",
       " \"it's\",\n",
       " 'a',\n",
       " 'nice',\n",
       " 'change',\n",
       " 'from',\n",
       " 'black',\n",
       " 'or',\n",
       " 'grey',\n",
       " '.',\n",
       " 'i',\n",
       " 'also',\n",
       " 'love',\n",
       " 'that',\n",
       " 'there',\n",
       " 'are',\n",
       " 'no',\n",
       " 'pockets',\n",
       " 'because',\n",
       " 'i',\n",
       " 'hate',\n",
       " 'how',\n",
       " 'they',\n",
       " 'jut',\n",
       " 'out',\n",
       " 'on',\n",
       " 'me',\n",
       " '.',\n",
       " 'i',\n",
       " 'wanted',\n",
       " 'a',\n",
       " 'petite',\n",
       " 'l',\n",
       " 'because',\n",
       " 'i',\n",
       " 'am',\n",
       " 'short',\n",
       " ',',\n",
       " 'but',\n",
       " 'the',\n",
       " 'regular',\n",
       " 'large',\n",
       " 'is',\n",
       " 'fine',\n",
       " '.',\n",
       " 'i',\n",
       " 'just',\n",
       " 'wear',\n",
       " 'them',\n",
       " 'higher',\n",
       " 'up',\n",
       " 'on',\n",
       " 'my',\n",
       " 'hips',\n",
       " '.',\n",
       " 'i',\n",
       " 'normally',\n",
       " 'wait',\n",
       " 'for',\n",
       " 'sales',\n",
       " 'on',\n",
       " 'sleepwear',\n",
       " ',',\n",
       " 'but',\n",
       " 'i',\n",
       " \"couldn't\",\n",
       " 'resist',\n",
       " 'on',\n",
       " 'these',\n",
       " '.',\n",
       " \"they're\",\n",
       " 'well',\n",
       " 'worth',\n",
       " 'the',\n",
       " 'investment',\n",
       " '!']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "stemmer.tokenize(avis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text(text):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stems = [stemmer.stem(token) for token in stemmer.tokenize(text)]\n",
    "    return ' '.join(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this dress in a love platinum is feminin and fit perfect , easi to wear and comfi , too ! high recommend !'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_text(avis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On applique la fonction à notre dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['stems'] = data_train['Review Text'].apply(stem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>score_avis</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>stems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13301</th>\n",
       "      <td>13301</td>\n",
       "      <td>This top is so pretty and easy to wear. the ma...</td>\n",
       "      <td>1</td>\n",
       "      <td>this top be so pretty and easy to wear . the m...</td>\n",
       "      <td>this top is so pretti and easi to wear . the m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6121</th>\n",
       "      <td>6121</td>\n",
       "      <td>I've been looking for a new winter dress and t...</td>\n",
       "      <td>1</td>\n",
       "      <td>I have be look for a new winter dress and this...</td>\n",
       "      <td>i'v been look for a new winter dress and this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14393</th>\n",
       "      <td>14393</td>\n",
       "      <td>The material is so soft and i really like the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>the material be so soft and I really like the ...</td>\n",
       "      <td>the materi is so soft and i realli like the de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6804</th>\n",
       "      <td>6804</td>\n",
       "      <td>This hoodie has a great fit! it is slightly ta...</td>\n",
       "      <td>1</td>\n",
       "      <td>this hoodie have a great fit ! it be slightly ...</td>\n",
       "      <td>this hoodi has a great fit ! it is slight tape...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2048</th>\n",
       "      <td>2048</td>\n",
       "      <td>I'm 5'6 and between 100-105lbs. buying clothes...</td>\n",
       "      <td>1</td>\n",
       "      <td>I be 5'6 and between 100 - 105lbs . buy clothe...</td>\n",
       "      <td>i'm 5 ' 6 and between 100-105 lbs . buy cloth ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                        Review Text  score_avis  \\\n",
       "13301  13301  This top is so pretty and easy to wear. the ma...           1   \n",
       "6121    6121  I've been looking for a new winter dress and t...           1   \n",
       "14393  14393  The material is so soft and i really like the ...           0   \n",
       "6804    6804  This hoodie has a great fit! it is slightly ta...           1   \n",
       "2048    2048  I'm 5'6 and between 100-105lbs. buying clothes...           1   \n",
       "\n",
       "                                                  lemmas  \\\n",
       "13301  this top be so pretty and easy to wear . the m...   \n",
       "6121   I have be look for a new winter dress and this...   \n",
       "14393  the material be so soft and I really like the ...   \n",
       "6804   this hoodie have a great fit ! it be slightly ...   \n",
       "2048   I be 5'6 and between 100 - 105lbs . buy clothe...   \n",
       "\n",
       "                                                   stems  \n",
       "13301  this top is so pretti and easi to wear . the m...  \n",
       "6121   i'v been look for a new winter dress and this ...  \n",
       "14393  the materi is so soft and i realli like the de...  \n",
       "6804   this hoodi has a great fit ! it is slight tape...  \n",
       "2048   i'm 5 ' 6 and between 100-105 lbs . buy cloth ...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fait de même sur l'ensemble de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['stems'] = data_test['Review Text'].apply(stem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2622, 5)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde\n",
    "data_train.to_pickle('train.pkl')\n",
    "data_test.to_pickle('test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Étiquettes morphosyntaxiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis, nous allons analyser notre texte et renvoyer chaque mot remplacé par sa catégorie grammaticale pour continuer l'étude des avis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_words_with_pos_tag(text):\n",
    "    text = nlp(text)\n",
    "    return ' '.join([token.pos_ for token in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On teste la fonction sur un avis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DET NOUN ADP DET ADJ NOUN AUX ADJ CCONJ VERB ADV PUNCT ADJ PART VERB CCONJ ADJ PUNCT ADV PUNCT ADV VERB PUNCT'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_words_with_pos_tag(avis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On applique la fonction à notre ensemble d'entrainement et on ajoute une colonne à notre dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['pos'] = data_train['Review Text'].apply(replace_words_with_pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>score_avis</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>stems</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13301</th>\n",
       "      <td>13301</td>\n",
       "      <td>This top is so pretty and easy to wear. the ma...</td>\n",
       "      <td>1</td>\n",
       "      <td>this top be so pretty and easy to wear . the m...</td>\n",
       "      <td>this top is so pretti and easi to wear . the m...</td>\n",
       "      <td>DET NOUN AUX ADV ADJ CCONJ ADJ PART VERB PUNCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6121</th>\n",
       "      <td>6121</td>\n",
       "      <td>I've been looking for a new winter dress and t...</td>\n",
       "      <td>1</td>\n",
       "      <td>I have be look for a new winter dress and this...</td>\n",
       "      <td>i'v been look for a new winter dress and this ...</td>\n",
       "      <td>PRON AUX AUX VERB ADP DET ADJ NOUN NOUN CCONJ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14393</th>\n",
       "      <td>14393</td>\n",
       "      <td>The material is so soft and i really like the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>the material be so soft and I really like the ...</td>\n",
       "      <td>the materi is so soft and i realli like the de...</td>\n",
       "      <td>DET NOUN AUX ADV ADJ CCONJ PRON ADV VERB DET N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6804</th>\n",
       "      <td>6804</td>\n",
       "      <td>This hoodie has a great fit! it is slightly ta...</td>\n",
       "      <td>1</td>\n",
       "      <td>this hoodie have a great fit ! it be slightly ...</td>\n",
       "      <td>this hoodi has a great fit ! it is slight tape...</td>\n",
       "      <td>DET NOUN VERB DET ADJ NOUN PUNCT PRON AUX ADV ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2048</th>\n",
       "      <td>2048</td>\n",
       "      <td>I'm 5'6 and between 100-105lbs. buying clothes...</td>\n",
       "      <td>1</td>\n",
       "      <td>I be 5'6 and between 100 - 105lbs . buy clothe...</td>\n",
       "      <td>i'm 5 ' 6 and between 100-105 lbs . buy cloth ...</td>\n",
       "      <td>PRON AUX NUM CCONJ ADP NUM PUNCT NUM PUNCT VER...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                        Review Text  score_avis  \\\n",
       "13301  13301  This top is so pretty and easy to wear. the ma...           1   \n",
       "6121    6121  I've been looking for a new winter dress and t...           1   \n",
       "14393  14393  The material is so soft and i really like the ...           0   \n",
       "6804    6804  This hoodie has a great fit! it is slightly ta...           1   \n",
       "2048    2048  I'm 5'6 and between 100-105lbs. buying clothes...           1   \n",
       "\n",
       "                                                  lemmas  \\\n",
       "13301  this top be so pretty and easy to wear . the m...   \n",
       "6121   I have be look for a new winter dress and this...   \n",
       "14393  the material be so soft and I really like the ...   \n",
       "6804   this hoodie have a great fit ! it be slightly ...   \n",
       "2048   I be 5'6 and between 100 - 105lbs . buy clothe...   \n",
       "\n",
       "                                                   stems  \\\n",
       "13301  this top is so pretti and easi to wear . the m...   \n",
       "6121   i'v been look for a new winter dress and this ...   \n",
       "14393  the materi is so soft and i realli like the de...   \n",
       "6804   this hoodi has a great fit ! it is slight tape...   \n",
       "2048   i'm 5 ' 6 and between 100-105 lbs . buy cloth ...   \n",
       "\n",
       "                                                     pos  \n",
       "13301  DET NOUN AUX ADV ADJ CCONJ ADJ PART VERB PUNCT...  \n",
       "6121   PRON AUX AUX VERB ADP DET ADJ NOUN NOUN CCONJ ...  \n",
       "14393  DET NOUN AUX ADV ADJ CCONJ PRON ADV VERB DET N...  \n",
       "6804   DET NOUN VERB DET ADJ NOUN PUNCT PRON AUX ADV ...  \n",
       "2048   PRON AUX NUM CCONJ ADP NUM PUNCT NUM PUNCT VER...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On effectue la même manipulation sur notre ensemble de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['pos'] = data_test['Review Text'].apply(replace_words_with_pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2622, 6)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde\n",
    "data_train.to_pickle('train.pkl')\n",
    "data_test.to_pickle('test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Classe d'appartenance des entités nommées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour pouvoir effectuer la reconnaissances d'entités nommées sur nos avis, nous allons retourner une version de notre avis ou chaque entité nommée est remplacée par son type d'entité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner(text):\n",
    "\n",
    "    text = nlp(text) # on transforme le texte en objet spacy\n",
    "    \n",
    "    new_text = [] # on crée une liste vide\n",
    "\n",
    "    for token in text: # pour chaque token dans l'avis\n",
    "        if token.ent_iob_ == \"O\": # si l'entité ne fait pas partie d'une entité nommée\n",
    "            new_text.append(token.text) # on ajoute le texte du token à la liste\n",
    "        elif token.ent_iob_ == \"B\": # si l'entité fait partie d'une entité nommée\n",
    "            new_text.append(token.ent_type_) # on ajoute le type de l'entité à la liste\n",
    "\n",
    "        # Si l'entité comprend plusieurs mot on ne répète pas l'étiquette\n",
    "        else:\n",
    "            continue\n",
    "    return ' '.join(new_text) # on retourne les étiquettes sous forme de texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On applique la fonction sur un avis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dress in a lovely platinum is feminine and fits perfectly, easy to wear and comfy, too! highly recommend!\n",
      "Avec les entités nommées :  This dress in a lovely platinum is feminine and fits perfectly , easy to wear and comfy , too ! highly recommend !\n"
     ]
    }
   ],
   "source": [
    "print(avis)\n",
    "print(\"Avec les entités nommées : \", ner(avis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On applique ensuite notre fonction à notre dataframe d'entrainement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['entites_nommees'] = data_train['Review Text'].apply(ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>score_avis</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>stems</th>\n",
       "      <th>pos</th>\n",
       "      <th>entites_nommees</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13301</th>\n",
       "      <td>13301</td>\n",
       "      <td>This top is so pretty and easy to wear. the ma...</td>\n",
       "      <td>1</td>\n",
       "      <td>this top be so pretty and easy to wear . the m...</td>\n",
       "      <td>this top is so pretti and easi to wear . the m...</td>\n",
       "      <td>DET NOUN AUX ADV ADJ CCONJ ADJ PART VERB PUNCT...</td>\n",
       "      <td>This top is so pretty and easy to wear . the m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6121</th>\n",
       "      <td>6121</td>\n",
       "      <td>I've been looking for a new winter dress and t...</td>\n",
       "      <td>1</td>\n",
       "      <td>I have be look for a new winter dress and this...</td>\n",
       "      <td>i'v been look for a new winter dress and this ...</td>\n",
       "      <td>PRON AUX AUX VERB ADP DET ADJ NOUN NOUN CCONJ ...</td>\n",
       "      <td>I 've been looking for a new DATE dress and th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14393</th>\n",
       "      <td>14393</td>\n",
       "      <td>The material is so soft and i really like the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>the material be so soft and I really like the ...</td>\n",
       "      <td>the materi is so soft and i realli like the de...</td>\n",
       "      <td>DET NOUN AUX ADV ADJ CCONJ PRON ADV VERB DET N...</td>\n",
       "      <td>The material is so soft and i really like the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6804</th>\n",
       "      <td>6804</td>\n",
       "      <td>This hoodie has a great fit! it is slightly ta...</td>\n",
       "      <td>1</td>\n",
       "      <td>this hoodie have a great fit ! it be slightly ...</td>\n",
       "      <td>this hoodi has a great fit ! it is slight tape...</td>\n",
       "      <td>DET NOUN VERB DET ADJ NOUN PUNCT PRON AUX ADV ...</td>\n",
       "      <td>This ORG has a great fit ! it is slightly tape...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2048</th>\n",
       "      <td>2048</td>\n",
       "      <td>I'm 5'6 and between 100-105lbs. buying clothes...</td>\n",
       "      <td>1</td>\n",
       "      <td>I be 5'6 and between 100 - 105lbs . buy clothe...</td>\n",
       "      <td>i'm 5 ' 6 and between 100-105 lbs . buy cloth ...</td>\n",
       "      <td>PRON AUX NUM CCONJ ADP NUM PUNCT NUM PUNCT VER...</td>\n",
       "      <td>I 'm CARDINAL and CARDINAL . buying clothes fr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                        Review Text  score_avis  \\\n",
       "13301  13301  This top is so pretty and easy to wear. the ma...           1   \n",
       "6121    6121  I've been looking for a new winter dress and t...           1   \n",
       "14393  14393  The material is so soft and i really like the ...           0   \n",
       "6804    6804  This hoodie has a great fit! it is slightly ta...           1   \n",
       "2048    2048  I'm 5'6 and between 100-105lbs. buying clothes...           1   \n",
       "\n",
       "                                                  lemmas  \\\n",
       "13301  this top be so pretty and easy to wear . the m...   \n",
       "6121   I have be look for a new winter dress and this...   \n",
       "14393  the material be so soft and I really like the ...   \n",
       "6804   this hoodie have a great fit ! it be slightly ...   \n",
       "2048   I be 5'6 and between 100 - 105lbs . buy clothe...   \n",
       "\n",
       "                                                   stems  \\\n",
       "13301  this top is so pretti and easi to wear . the m...   \n",
       "6121   i'v been look for a new winter dress and this ...   \n",
       "14393  the materi is so soft and i realli like the de...   \n",
       "6804   this hoodi has a great fit ! it is slight tape...   \n",
       "2048   i'm 5 ' 6 and between 100-105 lbs . buy cloth ...   \n",
       "\n",
       "                                                     pos  \\\n",
       "13301  DET NOUN AUX ADV ADJ CCONJ ADJ PART VERB PUNCT...   \n",
       "6121   PRON AUX AUX VERB ADP DET ADJ NOUN NOUN CCONJ ...   \n",
       "14393  DET NOUN AUX ADV ADJ CCONJ PRON ADV VERB DET N...   \n",
       "6804   DET NOUN VERB DET ADJ NOUN PUNCT PRON AUX ADV ...   \n",
       "2048   PRON AUX NUM CCONJ ADP NUM PUNCT NUM PUNCT VER...   \n",
       "\n",
       "                                         entites_nommees  \n",
       "13301  This top is so pretty and easy to wear . the m...  \n",
       "6121   I 've been looking for a new DATE dress and th...  \n",
       "14393  The material is so soft and i really like the ...  \n",
       "6804   This ORG has a great fit ! it is slightly tape...  \n",
       "2048   I 'm CARDINAL and CARDINAL . buying clothes fr...  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aussi, on applique à notre ensemble de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['entites_nommees'] = data_test['Review Text'].apply(ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2622, 7)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.to_pickle('train.pkl')\n",
    "data_test.to_pickle('test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Calcul des valeurs des descripteurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour procéder aux calculs, nous allons séparer notre jeu de données d'entraînement pour avoir un jeu de données de validation. Les données test nou servirons pour l'évaluation finale des modèles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(data_train['Review Text'],\n",
    "                                                      data_train['score_avis'],\n",
    "                                                      train_size=0.75,\n",
    "                                                      random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8847,), (2950,))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a donc 8 847 lignes dans notre jeu d'entrainement et 2 950 dans celui de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20304    1\n",
       "13051    0\n",
       "10628   -1\n",
       "4648     0\n",
       "19493    0\n",
       "        ..\n",
       "19357    1\n",
       "9412     0\n",
       "20438    0\n",
       "22105    1\n",
       "11296    1\n",
       "Name: score_avis, Length: 8847, dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons observer que les sorties à prédire correspondent aux trois étiquettes que nous avons défini plus haut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour évaluer notre modèle, nous initialisons les ensembles de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère les avis et les labels du jeu de données de test\n",
    "X_test, y_test = data_test['Review Text'], data_test['score_avis'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binaire : présence/absence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bin_count = CountVectorizer(binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(binary=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(binary=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(binary=True)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_count.fit(X_train)\n",
    "bin_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[141], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train_vectorized_bin \u001b[38;5;241m=\u001b[39m bin_count\u001b[38;5;241m.\u001b[39mtransform(\u001b[43mX_train\u001b[49m)\n\u001b[0;32m      2\u001b[0m X_train_vectorized_bin\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_vectorized_bin = bin_count.transform(X_train)\n",
    "X_train_vectorized_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_vectorized_bin = bin_count.transform(X_valid)\n",
    "X_test_vectorized_bin = bin_count.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2950x9677 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 128245 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid_vectorized_bin # MEME NOMBRE DE COLONNES QUE X_train_vectorized_bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Numérique discret : décomptes d'occurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons calculer les fréquences d'occurence des termes dans nos avis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_count = CountVectorizer().fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons examiner le vocabulaire de nos avis : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '00p', '03', '03dd', '0in', '0p', '0petite', '0r', '0verall',\n",
       "       '0xs', '10', '100', '1000', '100lb', '100lbs', '102', '102lbs',\n",
       "       '103', '103lb', '103lbs', '104', '105', '105lbs', '106', '107',\n",
       "       '107lb', '107pound', '108', '108lbs', '109', '109lbs', '10lbs',\n",
       "       '10p', '10x', '11', '110', '110lbs', '111', '111lbs', '112',\n",
       "       '112lbs', '113', '114', '114lb', '115', '115lbs', '116', '116lb',\n",
       "       '116lbs', '117'], dtype=object)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_count.get_feature_names_out()[:50] # 50 premiers mots (\"types\" du vocabulaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['yep', 'yes', 'yest', 'yesterday', 'yet', 'yey', 'yfit', 'yiddish',\n",
       "       'yield', 'yikes', 'yippee', 'yo', 'yoga', 'yogini', 'yogis',\n",
       "       'yoke', 'yolk', 'york', 'you', 'young', 'younger', 'your', 'youre',\n",
       "       'yours', 'yourself', 'yourselves', 'youthful', 'youthfull', 'yr',\n",
       "       'yrs', 'yuck', 'yucky', 'yummiest', 'yummy', 'zag', 'zermatt',\n",
       "       'zero', 'zig', 'zigzag', 'zip', 'zipped', 'zipper', 'zippered',\n",
       "       'zippers', 'zipping', 'zips', 'zombie', 'zone', 'zoom', 'zuma'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_count.get_feature_names_out()[-50:] # 50 derniers mots (\"types\" du vocabulaire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taille de notre vocabulaire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vect_count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[142], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mvect_count\u001b[49m\u001b[38;5;241m.\u001b[39mget_feature_names_out()) \n",
      "\u001b[1;31mNameError\u001b[0m: name 'vect_count' is not defined"
     ]
    }
   ],
   "source": [
    "len(vect_count.get_feature_names_out()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création matrice document-termes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons créer la matrice document-termes avec le même vectoriseur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8847x9677 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 388140 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized_count = vect_count.transform(X_train)\n",
    "X_train_vectorized_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_vectorized_count = vect_count.transform(X_valid)\n",
    "X_test_vectorized_count = vect_count.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A présent, nous allons prendre en compte les bi-grammes dans notre vocabulaire. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_count_bigrams = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\n",
    "X_train_vectorized_count_bigrams = vect_count_bigrams.transform(X_train)\n",
    "X_valid_vectorized_count_bigrams = vect_count_bigrams.transform(X_valid)\n",
    "X_test_vectorized_count_bigrams = vect_count_bigrams.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17411"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect_count_bigrams.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons presque 2 fois plus de vocabulaire avec inclusion des bigrammes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Numérique continu : TF-IDF (ou autres pondérations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons limiter le vocabulaire à des termes qui apparaissent au moins 5 fois dans le document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_tfidf = TfidfVectorizer(min_df=5).fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9677, 3260)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect_count.get_feature_names_out()), len(vect_tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La réduction de la taille du vocadulaire est importante et est due au paramètre min_df=5 : on a quasiment 3 fois moins de termes !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons vectoriser les jeux de données. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorisation des corpus d'entrainement, de validation et de test\n",
    "X_train_vectorized_tfidf = vect_tfidf.transform(X_train)\n",
    "X_valid_vectorized_tfidf = vect_tfidf.transform(X_valid)\n",
    "X_test_vectorized_tfidf = vect_tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification des textes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons réaliser une classification en utilisant plusieurs modèles afin de comparer les performances. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
