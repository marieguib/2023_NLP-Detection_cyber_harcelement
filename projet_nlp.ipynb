{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Classification des avis sur des vêtements de femmes vendus dans le e-commerce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Est ce que les avis que l'on a des vêtements sont représentatifs de la note qui est attribuée ?\n",
    "\n",
    "Idées :\n",
    "- visualisation de données : quels types de vêtements ont les notes les plus élevées ?\n",
    "- nb d'avis donné selon l'âge des clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Import des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps, nous allons importer nos données. Notre base de données contient des informations sur des avis de vêtements femmes vendus sur internet. Ces données sont issus d'un processus de webscrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>767</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Absolutely wonderful - silky and sexy and comf...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Initmates</td>\n",
       "      <td>Intimate</td>\n",
       "      <td>Intimates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1080</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1077</td>\n",
       "      <td>60</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1049</td>\n",
       "      <td>50</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>847</td>\n",
       "      <td>47</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  Clothing ID  Age                    Title  \\\n",
       "0   0          767   33                      NaN   \n",
       "1   1         1080   34                      NaN   \n",
       "2   2         1077   60  Some major design flaws   \n",
       "3   3         1049   50         My favorite buy!   \n",
       "4   4          847   47         Flattering shirt   \n",
       "\n",
       "                                         Review Text  Rating  Recommended IND  \\\n",
       "0  Absolutely wonderful - silky and sexy and comf...       4                1   \n",
       "1  Love this dress!  it's sooo pretty.  i happene...       5                1   \n",
       "2  I had such high hopes for this dress and reall...       3                0   \n",
       "3  I love, love, love this jumpsuit. it's fun, fl...       5                1   \n",
       "4  This shirt is very flattering to all due to th...       5                1   \n",
       "\n",
       "   Positive Feedback Count   Division Name Department Name Class Name  \n",
       "0                        0       Initmates        Intimate  Intimates  \n",
       "1                        4         General         Dresses    Dresses  \n",
       "2                        0         General         Dresses    Dresses  \n",
       "3                        0  General Petite         Bottoms      Pants  \n",
       "4                        6         General            Tops    Blouses  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import des données\n",
    "data = pd.read_csv(\"Womens Clothing E-Commerce Reviews.csv\", sep = \",\")\n",
    "\n",
    "# Renomage première colonne pour pouvoir l'utiliser comme id par la suite\n",
    "data = data.rename(columns = {\"Unnamed: 0\" : \"id\"})\n",
    "\n",
    "# Affichage des 5 premières lignes\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23486, 11)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre jeu de données contient 23 486 avis et 11 colonnes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Pré-traitement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour avoir des données plus propres, nous allons effectuer divers pré-traitements.\n",
    "\n",
    "Pour commencer, nous allons supprimer les lignes où nous avons des valeurs manquantes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19662, 11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Suppression des valeurs manquantes\n",
    "data = data.dropna()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suite à cette manipulation, nous supprimons environ 4 000 lignes pour pouvoir avoir des données complètes pour poursuivre notre analyse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération des données pour notre étude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traitement de la casse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- suppression des valeurs manquantes => pas d'avis : inutile\n",
    "- suppression des caractères spéciaux \n",
    "- suppression des majuscules\n",
    "- suppression des mots vides\n",
    "- lemmatisation \n",
    "- affiche du nombre de mots par étiquette grammaticale\n",
    "- extraction des mots (groupes de mots) les plus fréquents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps, nous allons récupérer les avis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2        I had such high hopes for this dress and reall...\n",
       "3        I love, love, love this jumpsuit. it's fun, fl...\n",
       "4        This shirt is very flattering to all due to th...\n",
       "5        I love tracy reese dresses, but this one is no...\n",
       "6        I aded this in my basket at hte last mintue to...\n",
       "                               ...                        \n",
       "23481    I was very happy to snag this dress at such a ...\n",
       "23482    It reminds me of maternity clothes. soft, stre...\n",
       "23483    This fit well, but the top was very see throug...\n",
       "23484    I bought this dress for a wedding i have this ...\n",
       "23485    This dress in a lovely platinum is feminine an...\n",
       "Name: Review Text, Length: 19662, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avis = data[\"Review Text\"]\n",
    "avis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(avis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons récupérer seulement la partie textuelle de l'avis, cela nous permet de ne avoir un objet Pandas.Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I had such high hopes for this dress and really wanted it to work for me. i '\n",
      " 'initially ordered the petite small (my usual size) but i found this to be '\n",
      " 'outrageously small. so small in fact that i could not zip it up! i reordered '\n",
      " 'it in petite medium, which was just ok. overall, the top half was '\n",
      " 'comfortable and fit nicely, but the bottom half had a very tight under layer '\n",
      " 'and several somewhat cheap (net) over layers. imo, a major design flaw was '\n",
      " 'the net over layer sewn directly into the zipper - it c',\n",
      " \"I love, love, love this jumpsuit. it's fun, flirty, and fabulous! every time \"\n",
      " 'i wear it, i get nothing but great compliments!',\n",
      " 'This shirt is very flattering to all due to the adjustable front tie. it is '\n",
      " 'the perfect length to wear with leggings and it is sleeveless so it pairs '\n",
      " 'well with any cardigan. love this shirt!!!',\n",
      " 'I love tracy reese dresses, but this one is not for the very petite. i am '\n",
      " 'just under 5 feet tall and usually wear a 0p in this brand. this dress was '\n",
      " 'very pretty out of the package but its a lot of dress. the skirt is long and '\n",
      " 'very full so it overwhelmed my small frame. not a stranger to alterations, '\n",
      " 'shortening and narrowing the skirt would take away from the embellishment of '\n",
      " 'the garment. i love the color and the idea of the style but it just did not '\n",
      " 'work on me. i returned this dress.',\n",
      " 'I aded this in my basket at hte last mintue to see what it would look like '\n",
      " 'in person. (store pick up). i went with teh darkler color only because i am '\n",
      " 'so pale :-) hte color is really gorgeous, and turns out it mathced '\n",
      " 'everythiing i was trying on with it prefectly. it is a little baggy on me '\n",
      " 'and hte xs is hte msallet size (bummer, no petite). i decided to jkeep it '\n",
      " 'though, because as i said, it matvehd everything. my ejans, pants, and the 3 '\n",
      " 'skirts i waas trying on (of which i ]kept all ) oops.',\n",
      " 'I ordered this in carbon for store pick up, and had a ton of stuff (as '\n",
      " 'always) to try on and used this top to pair (skirts and pants). everything '\n",
      " 'went with it. the color is really nice charcoal with shimmer, and went well '\n",
      " 'with pencil skirts, flare pants, etc. my only compaint is it is a bit big, '\n",
      " \"sleeves are long and it doesn't go in petite. also a bit loose for me, but \"\n",
      " 'no xxs... so i kept it and wil ldecide later since the light color is '\n",
      " 'already sold out in hte smallest size...',\n",
      " 'I love this dress. i usually get an xs but it runs a little snug in bust so '\n",
      " 'i ordered up a size. very flattering and feminine with the usual retailer '\n",
      " 'flair for style.',\n",
      " 'I\\'m 5\"5\\' and 125 lbs. i ordered the s petite to make sure the length '\n",
      " \"wasn't too long. i typically wear an xs regular in retailer dresses. if \"\n",
      " \"you're less busty (34b cup or smaller), a s petite will fit you perfectly \"\n",
      " '(snug, but not tight). i love that i could dress it up for a party, or down '\n",
      " 'for work. i love that the tulle is longer then the fabric underneath.',\n",
      " 'Dress runs small esp where the zipper area runs. i ordered the sp which '\n",
      " 'typically fits me and it was very tight! the material on the top looks and '\n",
      " 'feels very cheap that even just pulling on it will cause it to rip the '\n",
      " 'fabric. pretty disappointed as it was going to be my christmas dress this '\n",
      " 'year! needless to say it will be going back.',\n",
      " 'More and more i find myself reliant on the reviews written by savvy shoppers '\n",
      " 'before me and for the most past, they are right on in their estimation of '\n",
      " 'the product. in the case of this dress-if it had not been for the reveiws-i '\n",
      " 'doubt i would have even tried this. the dress is beautifully made, lined and '\n",
      " 'reminiscent of the old retailer quality. it is lined in the solid '\n",
      " 'periwinkle-colored fabric that matches the outer fabric print. tts and very '\n",
      " 'form-fitting. falls just above the knee and does not rid']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "liste_avis = data[\"Review Text\"].values.tolist()\n",
    "pprint(liste_avis[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grâce à cette manipulation, chaque avis est élément d'une liste d'avis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous allons découper les avis en liste de mots et les mettre en minuscules pour pouvoir les analyser plus facilement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i had such high hopes for this dress and really wanted it to work for me. i initially ordered the petite small (my usual size) but i found this to be outrageously small. so small in fact that i could not zip it up! i reordered it in petite medium, which was just ok. overall, the top half was comfortable and fit nicely, but the bottom half had a very tight under layer and several somewhat cheap (net) over layers. imo, a major design flaw was the net over layer sewn directly into the zipper - it c',\n",
       " \"i love, love, love this jumpsuit. it's fun, flirty, and fabulous! every time i wear it, i get nothing but great compliments!\",\n",
       " 'this shirt is very flattering to all due to the adjustable front tie. it is the perfect length to wear with leggings and it is sleeveless so it pairs well with any cardigan. love this shirt!!!',\n",
       " 'i love tracy reese dresses, but this one is not for the very petite. i am just under 5 feet tall and usually wear a 0p in this brand. this dress was very pretty out of the package but its a lot of dress. the skirt is long and very full so it overwhelmed my small frame. not a stranger to alterations, shortening and narrowing the skirt would take away from the embellishment of the garment. i love the color and the idea of the style but it just did not work on me. i returned this dress.',\n",
       " 'i aded this in my basket at hte last mintue to see what it would look like in person. (store pick up). i went with teh darkler color only because i am so pale :-) hte color is really gorgeous, and turns out it mathced everythiing i was trying on with it prefectly. it is a little baggy on me and hte xs is hte msallet size (bummer, no petite). i decided to jkeep it though, because as i said, it matvehd everything. my ejans, pants, and the 3 skirts i waas trying on (of which i ]kept all ) oops.',\n",
       " \"i ordered this in carbon for store pick up, and had a ton of stuff (as always) to try on and used this top to pair (skirts and pants). everything went with it. the color is really nice charcoal with shimmer, and went well with pencil skirts, flare pants, etc. my only compaint is it is a bit big, sleeves are long and it doesn't go in petite. also a bit loose for me, but no xxs... so i kept it and wil ldecide later since the light color is already sold out in hte smallest size...\",\n",
       " 'i love this dress. i usually get an xs but it runs a little snug in bust so i ordered up a size. very flattering and feminine with the usual retailer flair for style.',\n",
       " 'i\\'m 5\"5\\' and 125 lbs. i ordered the s petite to make sure the length wasn\\'t too long. i typically wear an xs regular in retailer dresses. if you\\'re less busty (34b cup or smaller), a s petite will fit you perfectly (snug, but not tight). i love that i could dress it up for a party, or down for work. i love that the tulle is longer then the fabric underneath.',\n",
       " 'dress runs small esp where the zipper area runs. i ordered the sp which typically fits me and it was very tight! the material on the top looks and feels very cheap that even just pulling on it will cause it to rip the fabric. pretty disappointed as it was going to be my christmas dress this year! needless to say it will be going back.',\n",
       " 'more and more i find myself reliant on the reviews written by savvy shoppers before me and for the most past, they are right on in their estimation of the product. in the case of this dress-if it had not been for the reveiws-i doubt i would have even tried this. the dress is beautifully made, lined and reminiscent of the old retailer quality. it is lined in the solid periwinkle-colored fabric that matches the outer fabric print. tts and very form-fitting. falls just above the knee and does not rid']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Découpage des avis en mots\n",
    "\n",
    "liste_avis_clean = []\n",
    "\n",
    "for avis in liste_avis : \n",
    "    avis = str(avis)\n",
    "    avis_clean = avis.split()\n",
    "    avis_clean = avis.lower()\n",
    "    liste_avis_clean.append(avis_clean)\n",
    "\n",
    "liste_avis_clean[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis, nous allons tockeniser notre texte. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i',\n",
       "  'had',\n",
       "  'such',\n",
       "  'high',\n",
       "  'hopes',\n",
       "  'for',\n",
       "  'this',\n",
       "  'dress',\n",
       "  'and',\n",
       "  'really',\n",
       "  'wanted',\n",
       "  'it',\n",
       "  'to',\n",
       "  'work',\n",
       "  'for',\n",
       "  'me',\n",
       "  'i',\n",
       "  'initially',\n",
       "  'ordered',\n",
       "  'the',\n",
       "  'petite',\n",
       "  'small',\n",
       "  'my',\n",
       "  'usual',\n",
       "  'size',\n",
       "  'but',\n",
       "  'i',\n",
       "  'found',\n",
       "  'this',\n",
       "  'to',\n",
       "  'be',\n",
       "  'outrageously',\n",
       "  'small',\n",
       "  'so',\n",
       "  'small',\n",
       "  'in',\n",
       "  'fact',\n",
       "  'that',\n",
       "  'i',\n",
       "  'could',\n",
       "  'not',\n",
       "  'zip',\n",
       "  'it',\n",
       "  'up',\n",
       "  'i',\n",
       "  'reordered',\n",
       "  'it',\n",
       "  'in',\n",
       "  'petite',\n",
       "  'medium',\n",
       "  'which',\n",
       "  'was',\n",
       "  'just',\n",
       "  'ok',\n",
       "  'overall',\n",
       "  'the',\n",
       "  'top',\n",
       "  'half',\n",
       "  'was',\n",
       "  'comfortable',\n",
       "  'and',\n",
       "  'fit',\n",
       "  'nicely',\n",
       "  'but',\n",
       "  'the',\n",
       "  'bottom',\n",
       "  'half',\n",
       "  'had',\n",
       "  'a',\n",
       "  'very',\n",
       "  'tight',\n",
       "  'under',\n",
       "  'layer',\n",
       "  'and',\n",
       "  'several',\n",
       "  'somewhat',\n",
       "  'cheap',\n",
       "  'net',\n",
       "  'over',\n",
       "  'layers',\n",
       "  'imo',\n",
       "  'a',\n",
       "  'major',\n",
       "  'design',\n",
       "  'flaw',\n",
       "  'was',\n",
       "  'the',\n",
       "  'net',\n",
       "  'over',\n",
       "  'layer',\n",
       "  'sewn',\n",
       "  'directly',\n",
       "  'into',\n",
       "  'the',\n",
       "  'zipper',\n",
       "  'it',\n",
       "  'c'],\n",
       " ['i',\n",
       "  'love',\n",
       "  'love',\n",
       "  'love',\n",
       "  'this',\n",
       "  'jumpsuit',\n",
       "  'it',\n",
       "  's',\n",
       "  'fun',\n",
       "  'flirty',\n",
       "  'and',\n",
       "  'fabulous',\n",
       "  'every',\n",
       "  'time',\n",
       "  'i',\n",
       "  'wear',\n",
       "  'it',\n",
       "  'i',\n",
       "  'get',\n",
       "  'nothing',\n",
       "  'but',\n",
       "  'great',\n",
       "  'compliments'],\n",
       " ['this',\n",
       "  'shirt',\n",
       "  'is',\n",
       "  'very',\n",
       "  'flattering',\n",
       "  'to',\n",
       "  'all',\n",
       "  'due',\n",
       "  'to',\n",
       "  'the',\n",
       "  'adjustable',\n",
       "  'front',\n",
       "  'tie',\n",
       "  'it',\n",
       "  'is',\n",
       "  'the',\n",
       "  'perfect',\n",
       "  'length',\n",
       "  'to',\n",
       "  'wear',\n",
       "  'with',\n",
       "  'leggings',\n",
       "  'and',\n",
       "  'it',\n",
       "  'is',\n",
       "  'sleeveless',\n",
       "  'so',\n",
       "  'it',\n",
       "  'pairs',\n",
       "  'well',\n",
       "  'with',\n",
       "  'any',\n",
       "  'cardigan',\n",
       "  'love',\n",
       "  'this',\n",
       "  'shirt'],\n",
       " ['i',\n",
       "  'love',\n",
       "  'tracy',\n",
       "  'reese',\n",
       "  'dresses',\n",
       "  'but',\n",
       "  'this',\n",
       "  'one',\n",
       "  'is',\n",
       "  'not',\n",
       "  'for',\n",
       "  'the',\n",
       "  'very',\n",
       "  'petite',\n",
       "  'i',\n",
       "  'am',\n",
       "  'just',\n",
       "  'under',\n",
       "  '5',\n",
       "  'feet',\n",
       "  'tall',\n",
       "  'and',\n",
       "  'usually',\n",
       "  'wear',\n",
       "  'a',\n",
       "  '0p',\n",
       "  'in',\n",
       "  'this',\n",
       "  'brand',\n",
       "  'this',\n",
       "  'dress',\n",
       "  'was',\n",
       "  'very',\n",
       "  'pretty',\n",
       "  'out',\n",
       "  'of',\n",
       "  'the',\n",
       "  'package',\n",
       "  'but',\n",
       "  'its',\n",
       "  'a',\n",
       "  'lot',\n",
       "  'of',\n",
       "  'dress',\n",
       "  'the',\n",
       "  'skirt',\n",
       "  'is',\n",
       "  'long',\n",
       "  'and',\n",
       "  'very',\n",
       "  'full',\n",
       "  'so',\n",
       "  'it',\n",
       "  'overwhelmed',\n",
       "  'my',\n",
       "  'small',\n",
       "  'frame',\n",
       "  'not',\n",
       "  'a',\n",
       "  'stranger',\n",
       "  'to',\n",
       "  'alterations',\n",
       "  'shortening',\n",
       "  'and',\n",
       "  'narrowing',\n",
       "  'the',\n",
       "  'skirt',\n",
       "  'would',\n",
       "  'take',\n",
       "  'away',\n",
       "  'from',\n",
       "  'the',\n",
       "  'embellishment',\n",
       "  'of',\n",
       "  'the',\n",
       "  'garment',\n",
       "  'i',\n",
       "  'love',\n",
       "  'the',\n",
       "  'color',\n",
       "  'and',\n",
       "  'the',\n",
       "  'idea',\n",
       "  'of',\n",
       "  'the',\n",
       "  'style',\n",
       "  'but',\n",
       "  'it',\n",
       "  'just',\n",
       "  'did',\n",
       "  'not',\n",
       "  'work',\n",
       "  'on',\n",
       "  'me',\n",
       "  'i',\n",
       "  'returned',\n",
       "  'this',\n",
       "  'dress'],\n",
       " ['i',\n",
       "  'aded',\n",
       "  'this',\n",
       "  'in',\n",
       "  'my',\n",
       "  'basket',\n",
       "  'at',\n",
       "  'hte',\n",
       "  'last',\n",
       "  'mintue',\n",
       "  'to',\n",
       "  'see',\n",
       "  'what',\n",
       "  'it',\n",
       "  'would',\n",
       "  'look',\n",
       "  'like',\n",
       "  'in',\n",
       "  'person',\n",
       "  'store',\n",
       "  'pick',\n",
       "  'up',\n",
       "  'i',\n",
       "  'went',\n",
       "  'with',\n",
       "  'teh',\n",
       "  'darkler',\n",
       "  'color',\n",
       "  'only',\n",
       "  'because',\n",
       "  'i',\n",
       "  'am',\n",
       "  'so',\n",
       "  'pale',\n",
       "  'hte',\n",
       "  'color',\n",
       "  'is',\n",
       "  'really',\n",
       "  'gorgeous',\n",
       "  'and',\n",
       "  'turns',\n",
       "  'out',\n",
       "  'it',\n",
       "  'mathced',\n",
       "  'everythiing',\n",
       "  'i',\n",
       "  'was',\n",
       "  'trying',\n",
       "  'on',\n",
       "  'with',\n",
       "  'it',\n",
       "  'prefectly',\n",
       "  'it',\n",
       "  'is',\n",
       "  'a',\n",
       "  'little',\n",
       "  'baggy',\n",
       "  'on',\n",
       "  'me',\n",
       "  'and',\n",
       "  'hte',\n",
       "  'xs',\n",
       "  'is',\n",
       "  'hte',\n",
       "  'msallet',\n",
       "  'size',\n",
       "  'bummer',\n",
       "  'no',\n",
       "  'petite',\n",
       "  'i',\n",
       "  'decided',\n",
       "  'to',\n",
       "  'jkeep',\n",
       "  'it',\n",
       "  'though',\n",
       "  'because',\n",
       "  'as',\n",
       "  'i',\n",
       "  'said',\n",
       "  'it',\n",
       "  'matvehd',\n",
       "  'everything',\n",
       "  'my',\n",
       "  'ejans',\n",
       "  'pants',\n",
       "  'and',\n",
       "  'the',\n",
       "  '3',\n",
       "  'skirts',\n",
       "  'i',\n",
       "  'waas',\n",
       "  'trying',\n",
       "  'on',\n",
       "  'of',\n",
       "  'which',\n",
       "  'i',\n",
       "  'kept',\n",
       "  'all',\n",
       "  'oops'],\n",
       " ['i',\n",
       "  'ordered',\n",
       "  'this',\n",
       "  'in',\n",
       "  'carbon',\n",
       "  'for',\n",
       "  'store',\n",
       "  'pick',\n",
       "  'up',\n",
       "  'and',\n",
       "  'had',\n",
       "  'a',\n",
       "  'ton',\n",
       "  'of',\n",
       "  'stuff',\n",
       "  'as',\n",
       "  'always',\n",
       "  'to',\n",
       "  'try',\n",
       "  'on',\n",
       "  'and',\n",
       "  'used',\n",
       "  'this',\n",
       "  'top',\n",
       "  'to',\n",
       "  'pair',\n",
       "  'skirts',\n",
       "  'and',\n",
       "  'pants',\n",
       "  'everything',\n",
       "  'went',\n",
       "  'with',\n",
       "  'it',\n",
       "  'the',\n",
       "  'color',\n",
       "  'is',\n",
       "  'really',\n",
       "  'nice',\n",
       "  'charcoal',\n",
       "  'with',\n",
       "  'shimmer',\n",
       "  'and',\n",
       "  'went',\n",
       "  'well',\n",
       "  'with',\n",
       "  'pencil',\n",
       "  'skirts',\n",
       "  'flare',\n",
       "  'pants',\n",
       "  'etc',\n",
       "  'my',\n",
       "  'only',\n",
       "  'compaint',\n",
       "  'is',\n",
       "  'it',\n",
       "  'is',\n",
       "  'a',\n",
       "  'bit',\n",
       "  'big',\n",
       "  'sleeves',\n",
       "  'are',\n",
       "  'long',\n",
       "  'and',\n",
       "  'it',\n",
       "  'doesn',\n",
       "  't',\n",
       "  'go',\n",
       "  'in',\n",
       "  'petite',\n",
       "  'also',\n",
       "  'a',\n",
       "  'bit',\n",
       "  'loose',\n",
       "  'for',\n",
       "  'me',\n",
       "  'but',\n",
       "  'no',\n",
       "  'xxs',\n",
       "  'so',\n",
       "  'i',\n",
       "  'kept',\n",
       "  'it',\n",
       "  'and',\n",
       "  'wil',\n",
       "  'ldecide',\n",
       "  'later',\n",
       "  'since',\n",
       "  'the',\n",
       "  'light',\n",
       "  'color',\n",
       "  'is',\n",
       "  'already',\n",
       "  'sold',\n",
       "  'out',\n",
       "  'in',\n",
       "  'hte',\n",
       "  'smallest',\n",
       "  'size'],\n",
       " ['i',\n",
       "  'love',\n",
       "  'this',\n",
       "  'dress',\n",
       "  'i',\n",
       "  'usually',\n",
       "  'get',\n",
       "  'an',\n",
       "  'xs',\n",
       "  'but',\n",
       "  'it',\n",
       "  'runs',\n",
       "  'a',\n",
       "  'little',\n",
       "  'snug',\n",
       "  'in',\n",
       "  'bust',\n",
       "  'so',\n",
       "  'i',\n",
       "  'ordered',\n",
       "  'up',\n",
       "  'a',\n",
       "  'size',\n",
       "  'very',\n",
       "  'flattering',\n",
       "  'and',\n",
       "  'feminine',\n",
       "  'with',\n",
       "  'the',\n",
       "  'usual',\n",
       "  'retailer',\n",
       "  'flair',\n",
       "  'for',\n",
       "  'style'],\n",
       " ['i',\n",
       "  'm',\n",
       "  '5',\n",
       "  '5',\n",
       "  'and',\n",
       "  '125',\n",
       "  'lbs',\n",
       "  'i',\n",
       "  'ordered',\n",
       "  'the',\n",
       "  's',\n",
       "  'petite',\n",
       "  'to',\n",
       "  'make',\n",
       "  'sure',\n",
       "  'the',\n",
       "  'length',\n",
       "  'wasn',\n",
       "  't',\n",
       "  'too',\n",
       "  'long',\n",
       "  'i',\n",
       "  'typically',\n",
       "  'wear',\n",
       "  'an',\n",
       "  'xs',\n",
       "  'regular',\n",
       "  'in',\n",
       "  'retailer',\n",
       "  'dresses',\n",
       "  'if',\n",
       "  'you',\n",
       "  're',\n",
       "  'less',\n",
       "  'busty',\n",
       "  '34b',\n",
       "  'cup',\n",
       "  'or',\n",
       "  'smaller',\n",
       "  'a',\n",
       "  's',\n",
       "  'petite',\n",
       "  'will',\n",
       "  'fit',\n",
       "  'you',\n",
       "  'perfectly',\n",
       "  'snug',\n",
       "  'but',\n",
       "  'not',\n",
       "  'tight',\n",
       "  'i',\n",
       "  'love',\n",
       "  'that',\n",
       "  'i',\n",
       "  'could',\n",
       "  'dress',\n",
       "  'it',\n",
       "  'up',\n",
       "  'for',\n",
       "  'a',\n",
       "  'party',\n",
       "  'or',\n",
       "  'down',\n",
       "  'for',\n",
       "  'work',\n",
       "  'i',\n",
       "  'love',\n",
       "  'that',\n",
       "  'the',\n",
       "  'tulle',\n",
       "  'is',\n",
       "  'longer',\n",
       "  'then',\n",
       "  'the',\n",
       "  'fabric',\n",
       "  'underneath'],\n",
       " ['dress',\n",
       "  'runs',\n",
       "  'small',\n",
       "  'esp',\n",
       "  'where',\n",
       "  'the',\n",
       "  'zipper',\n",
       "  'area',\n",
       "  'runs',\n",
       "  'i',\n",
       "  'ordered',\n",
       "  'the',\n",
       "  'sp',\n",
       "  'which',\n",
       "  'typically',\n",
       "  'fits',\n",
       "  'me',\n",
       "  'and',\n",
       "  'it',\n",
       "  'was',\n",
       "  'very',\n",
       "  'tight',\n",
       "  'the',\n",
       "  'material',\n",
       "  'on',\n",
       "  'the',\n",
       "  'top',\n",
       "  'looks',\n",
       "  'and',\n",
       "  'feels',\n",
       "  'very',\n",
       "  'cheap',\n",
       "  'that',\n",
       "  'even',\n",
       "  'just',\n",
       "  'pulling',\n",
       "  'on',\n",
       "  'it',\n",
       "  'will',\n",
       "  'cause',\n",
       "  'it',\n",
       "  'to',\n",
       "  'rip',\n",
       "  'the',\n",
       "  'fabric',\n",
       "  'pretty',\n",
       "  'disappointed',\n",
       "  'as',\n",
       "  'it',\n",
       "  'was',\n",
       "  'going',\n",
       "  'to',\n",
       "  'be',\n",
       "  'my',\n",
       "  'christmas',\n",
       "  'dress',\n",
       "  'this',\n",
       "  'year',\n",
       "  'needless',\n",
       "  'to',\n",
       "  'say',\n",
       "  'it',\n",
       "  'will',\n",
       "  'be',\n",
       "  'going',\n",
       "  'back'],\n",
       " ['more',\n",
       "  'and',\n",
       "  'more',\n",
       "  'i',\n",
       "  'find',\n",
       "  'myself',\n",
       "  'reliant',\n",
       "  'on',\n",
       "  'the',\n",
       "  'reviews',\n",
       "  'written',\n",
       "  'by',\n",
       "  'savvy',\n",
       "  'shoppers',\n",
       "  'before',\n",
       "  'me',\n",
       "  'and',\n",
       "  'for',\n",
       "  'the',\n",
       "  'most',\n",
       "  'past',\n",
       "  'they',\n",
       "  'are',\n",
       "  'right',\n",
       "  'on',\n",
       "  'in',\n",
       "  'their',\n",
       "  'estimation',\n",
       "  'of',\n",
       "  'the',\n",
       "  'product',\n",
       "  'in',\n",
       "  'the',\n",
       "  'case',\n",
       "  'of',\n",
       "  'this',\n",
       "  'dress',\n",
       "  'if',\n",
       "  'it',\n",
       "  'had',\n",
       "  'not',\n",
       "  'been',\n",
       "  'for',\n",
       "  'the',\n",
       "  'reveiws',\n",
       "  'i',\n",
       "  'doubt',\n",
       "  'i',\n",
       "  'would',\n",
       "  'have',\n",
       "  'even',\n",
       "  'tried',\n",
       "  'this',\n",
       "  'the',\n",
       "  'dress',\n",
       "  'is',\n",
       "  'beautifully',\n",
       "  'made',\n",
       "  'lined',\n",
       "  'and',\n",
       "  'reminiscent',\n",
       "  'of',\n",
       "  'the',\n",
       "  'old',\n",
       "  'retailer',\n",
       "  'quality',\n",
       "  'it',\n",
       "  'is',\n",
       "  'lined',\n",
       "  'in',\n",
       "  'the',\n",
       "  'solid',\n",
       "  'periwinkle',\n",
       "  'colored',\n",
       "  'fabric',\n",
       "  'that',\n",
       "  'matches',\n",
       "  'the',\n",
       "  'outer',\n",
       "  'fabric',\n",
       "  'print',\n",
       "  'tts',\n",
       "  'and',\n",
       "  'very',\n",
       "  'form',\n",
       "  'fitting',\n",
       "  'falls',\n",
       "  'just',\n",
       "  'above',\n",
       "  'the',\n",
       "  'knee',\n",
       "  'and',\n",
       "  'does',\n",
       "  'not',\n",
       "  'rid']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer('\\w+')\n",
    "liste_avis_clean = [tokenizer.tokenize(str(avis)) for avis in liste_avis_clean]\n",
    "liste_avis_clean[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, on supprime la ponctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i',\n",
       "  'had',\n",
       "  'such',\n",
       "  'high',\n",
       "  'hopes',\n",
       "  'for',\n",
       "  'this',\n",
       "  'dress',\n",
       "  'and',\n",
       "  'really',\n",
       "  'wanted',\n",
       "  'it',\n",
       "  'to',\n",
       "  'work',\n",
       "  'for',\n",
       "  'me',\n",
       "  'i',\n",
       "  'initially',\n",
       "  'ordered',\n",
       "  'the',\n",
       "  'petite',\n",
       "  'small',\n",
       "  'my',\n",
       "  'usual',\n",
       "  'size',\n",
       "  'but',\n",
       "  'i',\n",
       "  'found',\n",
       "  'this',\n",
       "  'to',\n",
       "  'be',\n",
       "  'outrageously',\n",
       "  'small',\n",
       "  'so',\n",
       "  'small',\n",
       "  'in',\n",
       "  'fact',\n",
       "  'that',\n",
       "  'i',\n",
       "  'could',\n",
       "  'not',\n",
       "  'zip',\n",
       "  'it',\n",
       "  'up',\n",
       "  'i',\n",
       "  'reordered',\n",
       "  'it',\n",
       "  'in',\n",
       "  'petite',\n",
       "  'medium',\n",
       "  'which',\n",
       "  'was',\n",
       "  'just',\n",
       "  'ok',\n",
       "  'overall',\n",
       "  'the',\n",
       "  'top',\n",
       "  'half',\n",
       "  'was',\n",
       "  'comfortable',\n",
       "  'and',\n",
       "  'fit',\n",
       "  'nicely',\n",
       "  'but',\n",
       "  'the',\n",
       "  'bottom',\n",
       "  'half',\n",
       "  'had',\n",
       "  'a',\n",
       "  'very',\n",
       "  'tight',\n",
       "  'under',\n",
       "  'layer',\n",
       "  'and',\n",
       "  'several',\n",
       "  'somewhat',\n",
       "  'cheap',\n",
       "  'net',\n",
       "  'over',\n",
       "  'layers',\n",
       "  'imo',\n",
       "  'a',\n",
       "  'major',\n",
       "  'design',\n",
       "  'flaw',\n",
       "  'was',\n",
       "  'the',\n",
       "  'net',\n",
       "  'over',\n",
       "  'layer',\n",
       "  'sewn',\n",
       "  'directly',\n",
       "  'into',\n",
       "  'the',\n",
       "  'zipper',\n",
       "  'it',\n",
       "  'c'],\n",
       " ['i',\n",
       "  'love',\n",
       "  'love',\n",
       "  'love',\n",
       "  'this',\n",
       "  'jumpsuit',\n",
       "  'it',\n",
       "  's',\n",
       "  'fun',\n",
       "  'flirty',\n",
       "  'and',\n",
       "  'fabulous',\n",
       "  'every',\n",
       "  'time',\n",
       "  'i',\n",
       "  'wear',\n",
       "  'it',\n",
       "  'i',\n",
       "  'get',\n",
       "  'nothing',\n",
       "  'but',\n",
       "  'great',\n",
       "  'compliments'],\n",
       " ['this',\n",
       "  'shirt',\n",
       "  'is',\n",
       "  'very',\n",
       "  'flattering',\n",
       "  'to',\n",
       "  'all',\n",
       "  'due',\n",
       "  'to',\n",
       "  'the',\n",
       "  'adjustable',\n",
       "  'front',\n",
       "  'tie',\n",
       "  'it',\n",
       "  'is',\n",
       "  'the',\n",
       "  'perfect',\n",
       "  'length',\n",
       "  'to',\n",
       "  'wear',\n",
       "  'with',\n",
       "  'leggings',\n",
       "  'and',\n",
       "  'it',\n",
       "  'is',\n",
       "  'sleeveless',\n",
       "  'so',\n",
       "  'it',\n",
       "  'pairs',\n",
       "  'well',\n",
       "  'with',\n",
       "  'any',\n",
       "  'cardigan',\n",
       "  'love',\n",
       "  'this',\n",
       "  'shirt'],\n",
       " ['i',\n",
       "  'love',\n",
       "  'tracy',\n",
       "  'reese',\n",
       "  'dresses',\n",
       "  'but',\n",
       "  'this',\n",
       "  'one',\n",
       "  'is',\n",
       "  'not',\n",
       "  'for',\n",
       "  'the',\n",
       "  'very',\n",
       "  'petite',\n",
       "  'i',\n",
       "  'am',\n",
       "  'just',\n",
       "  'under',\n",
       "  '5',\n",
       "  'feet',\n",
       "  'tall',\n",
       "  'and',\n",
       "  'usually',\n",
       "  'wear',\n",
       "  'a',\n",
       "  '0p',\n",
       "  'in',\n",
       "  'this',\n",
       "  'brand',\n",
       "  'this',\n",
       "  'dress',\n",
       "  'was',\n",
       "  'very',\n",
       "  'pretty',\n",
       "  'out',\n",
       "  'of',\n",
       "  'the',\n",
       "  'package',\n",
       "  'but',\n",
       "  'its',\n",
       "  'a',\n",
       "  'lot',\n",
       "  'of',\n",
       "  'dress',\n",
       "  'the',\n",
       "  'skirt',\n",
       "  'is',\n",
       "  'long',\n",
       "  'and',\n",
       "  'very',\n",
       "  'full',\n",
       "  'so',\n",
       "  'it',\n",
       "  'overwhelmed',\n",
       "  'my',\n",
       "  'small',\n",
       "  'frame',\n",
       "  'not',\n",
       "  'a',\n",
       "  'stranger',\n",
       "  'to',\n",
       "  'alterations',\n",
       "  'shortening',\n",
       "  'and',\n",
       "  'narrowing',\n",
       "  'the',\n",
       "  'skirt',\n",
       "  'would',\n",
       "  'take',\n",
       "  'away',\n",
       "  'from',\n",
       "  'the',\n",
       "  'embellishment',\n",
       "  'of',\n",
       "  'the',\n",
       "  'garment',\n",
       "  'i',\n",
       "  'love',\n",
       "  'the',\n",
       "  'color',\n",
       "  'and',\n",
       "  'the',\n",
       "  'idea',\n",
       "  'of',\n",
       "  'the',\n",
       "  'style',\n",
       "  'but',\n",
       "  'it',\n",
       "  'just',\n",
       "  'did',\n",
       "  'not',\n",
       "  'work',\n",
       "  'on',\n",
       "  'me',\n",
       "  'i',\n",
       "  'returned',\n",
       "  'this',\n",
       "  'dress'],\n",
       " ['i',\n",
       "  'aded',\n",
       "  'this',\n",
       "  'in',\n",
       "  'my',\n",
       "  'basket',\n",
       "  'at',\n",
       "  'hte',\n",
       "  'last',\n",
       "  'mintue',\n",
       "  'to',\n",
       "  'see',\n",
       "  'what',\n",
       "  'it',\n",
       "  'would',\n",
       "  'look',\n",
       "  'like',\n",
       "  'in',\n",
       "  'person',\n",
       "  'store',\n",
       "  'pick',\n",
       "  'up',\n",
       "  'i',\n",
       "  'went',\n",
       "  'with',\n",
       "  'teh',\n",
       "  'darkler',\n",
       "  'color',\n",
       "  'only',\n",
       "  'because',\n",
       "  'i',\n",
       "  'am',\n",
       "  'so',\n",
       "  'pale',\n",
       "  'hte',\n",
       "  'color',\n",
       "  'is',\n",
       "  'really',\n",
       "  'gorgeous',\n",
       "  'and',\n",
       "  'turns',\n",
       "  'out',\n",
       "  'it',\n",
       "  'mathced',\n",
       "  'everythiing',\n",
       "  'i',\n",
       "  'was',\n",
       "  'trying',\n",
       "  'on',\n",
       "  'with',\n",
       "  'it',\n",
       "  'prefectly',\n",
       "  'it',\n",
       "  'is',\n",
       "  'a',\n",
       "  'little',\n",
       "  'baggy',\n",
       "  'on',\n",
       "  'me',\n",
       "  'and',\n",
       "  'hte',\n",
       "  'xs',\n",
       "  'is',\n",
       "  'hte',\n",
       "  'msallet',\n",
       "  'size',\n",
       "  'bummer',\n",
       "  'no',\n",
       "  'petite',\n",
       "  'i',\n",
       "  'decided',\n",
       "  'to',\n",
       "  'jkeep',\n",
       "  'it',\n",
       "  'though',\n",
       "  'because',\n",
       "  'as',\n",
       "  'i',\n",
       "  'said',\n",
       "  'it',\n",
       "  'matvehd',\n",
       "  'everything',\n",
       "  'my',\n",
       "  'ejans',\n",
       "  'pants',\n",
       "  'and',\n",
       "  'the',\n",
       "  '3',\n",
       "  'skirts',\n",
       "  'i',\n",
       "  'waas',\n",
       "  'trying',\n",
       "  'on',\n",
       "  'of',\n",
       "  'which',\n",
       "  'i',\n",
       "  'kept',\n",
       "  'all',\n",
       "  'oops'],\n",
       " ['i',\n",
       "  'ordered',\n",
       "  'this',\n",
       "  'in',\n",
       "  'carbon',\n",
       "  'for',\n",
       "  'store',\n",
       "  'pick',\n",
       "  'up',\n",
       "  'and',\n",
       "  'had',\n",
       "  'a',\n",
       "  'ton',\n",
       "  'of',\n",
       "  'stuff',\n",
       "  'as',\n",
       "  'always',\n",
       "  'to',\n",
       "  'try',\n",
       "  'on',\n",
       "  'and',\n",
       "  'used',\n",
       "  'this',\n",
       "  'top',\n",
       "  'to',\n",
       "  'pair',\n",
       "  'skirts',\n",
       "  'and',\n",
       "  'pants',\n",
       "  'everything',\n",
       "  'went',\n",
       "  'with',\n",
       "  'it',\n",
       "  'the',\n",
       "  'color',\n",
       "  'is',\n",
       "  'really',\n",
       "  'nice',\n",
       "  'charcoal',\n",
       "  'with',\n",
       "  'shimmer',\n",
       "  'and',\n",
       "  'went',\n",
       "  'well',\n",
       "  'with',\n",
       "  'pencil',\n",
       "  'skirts',\n",
       "  'flare',\n",
       "  'pants',\n",
       "  'etc',\n",
       "  'my',\n",
       "  'only',\n",
       "  'compaint',\n",
       "  'is',\n",
       "  'it',\n",
       "  'is',\n",
       "  'a',\n",
       "  'bit',\n",
       "  'big',\n",
       "  'sleeves',\n",
       "  'are',\n",
       "  'long',\n",
       "  'and',\n",
       "  'it',\n",
       "  'doesn',\n",
       "  't',\n",
       "  'go',\n",
       "  'in',\n",
       "  'petite',\n",
       "  'also',\n",
       "  'a',\n",
       "  'bit',\n",
       "  'loose',\n",
       "  'for',\n",
       "  'me',\n",
       "  'but',\n",
       "  'no',\n",
       "  'xxs',\n",
       "  'so',\n",
       "  'i',\n",
       "  'kept',\n",
       "  'it',\n",
       "  'and',\n",
       "  'wil',\n",
       "  'ldecide',\n",
       "  'later',\n",
       "  'since',\n",
       "  'the',\n",
       "  'light',\n",
       "  'color',\n",
       "  'is',\n",
       "  'already',\n",
       "  'sold',\n",
       "  'out',\n",
       "  'in',\n",
       "  'hte',\n",
       "  'smallest',\n",
       "  'size'],\n",
       " ['i',\n",
       "  'love',\n",
       "  'this',\n",
       "  'dress',\n",
       "  'i',\n",
       "  'usually',\n",
       "  'get',\n",
       "  'an',\n",
       "  'xs',\n",
       "  'but',\n",
       "  'it',\n",
       "  'runs',\n",
       "  'a',\n",
       "  'little',\n",
       "  'snug',\n",
       "  'in',\n",
       "  'bust',\n",
       "  'so',\n",
       "  'i',\n",
       "  'ordered',\n",
       "  'up',\n",
       "  'a',\n",
       "  'size',\n",
       "  'very',\n",
       "  'flattering',\n",
       "  'and',\n",
       "  'feminine',\n",
       "  'with',\n",
       "  'the',\n",
       "  'usual',\n",
       "  'retailer',\n",
       "  'flair',\n",
       "  'for',\n",
       "  'style'],\n",
       " ['i',\n",
       "  'm',\n",
       "  '5',\n",
       "  '5',\n",
       "  'and',\n",
       "  '125',\n",
       "  'lbs',\n",
       "  'i',\n",
       "  'ordered',\n",
       "  'the',\n",
       "  's',\n",
       "  'petite',\n",
       "  'to',\n",
       "  'make',\n",
       "  'sure',\n",
       "  'the',\n",
       "  'length',\n",
       "  'wasn',\n",
       "  't',\n",
       "  'too',\n",
       "  'long',\n",
       "  'i',\n",
       "  'typically',\n",
       "  'wear',\n",
       "  'an',\n",
       "  'xs',\n",
       "  'regular',\n",
       "  'in',\n",
       "  'retailer',\n",
       "  'dresses',\n",
       "  'if',\n",
       "  'you',\n",
       "  're',\n",
       "  'less',\n",
       "  'busty',\n",
       "  '34b',\n",
       "  'cup',\n",
       "  'or',\n",
       "  'smaller',\n",
       "  'a',\n",
       "  's',\n",
       "  'petite',\n",
       "  'will',\n",
       "  'fit',\n",
       "  'you',\n",
       "  'perfectly',\n",
       "  'snug',\n",
       "  'but',\n",
       "  'not',\n",
       "  'tight',\n",
       "  'i',\n",
       "  'love',\n",
       "  'that',\n",
       "  'i',\n",
       "  'could',\n",
       "  'dress',\n",
       "  'it',\n",
       "  'up',\n",
       "  'for',\n",
       "  'a',\n",
       "  'party',\n",
       "  'or',\n",
       "  'down',\n",
       "  'for',\n",
       "  'work',\n",
       "  'i',\n",
       "  'love',\n",
       "  'that',\n",
       "  'the',\n",
       "  'tulle',\n",
       "  'is',\n",
       "  'longer',\n",
       "  'then',\n",
       "  'the',\n",
       "  'fabric',\n",
       "  'underneath'],\n",
       " ['dress',\n",
       "  'runs',\n",
       "  'small',\n",
       "  'esp',\n",
       "  'where',\n",
       "  'the',\n",
       "  'zipper',\n",
       "  'area',\n",
       "  'runs',\n",
       "  'i',\n",
       "  'ordered',\n",
       "  'the',\n",
       "  'sp',\n",
       "  'which',\n",
       "  'typically',\n",
       "  'fits',\n",
       "  'me',\n",
       "  'and',\n",
       "  'it',\n",
       "  'was',\n",
       "  'very',\n",
       "  'tight',\n",
       "  'the',\n",
       "  'material',\n",
       "  'on',\n",
       "  'the',\n",
       "  'top',\n",
       "  'looks',\n",
       "  'and',\n",
       "  'feels',\n",
       "  'very',\n",
       "  'cheap',\n",
       "  'that',\n",
       "  'even',\n",
       "  'just',\n",
       "  'pulling',\n",
       "  'on',\n",
       "  'it',\n",
       "  'will',\n",
       "  'cause',\n",
       "  'it',\n",
       "  'to',\n",
       "  'rip',\n",
       "  'the',\n",
       "  'fabric',\n",
       "  'pretty',\n",
       "  'disappointed',\n",
       "  'as',\n",
       "  'it',\n",
       "  'was',\n",
       "  'going',\n",
       "  'to',\n",
       "  'be',\n",
       "  'my',\n",
       "  'christmas',\n",
       "  'dress',\n",
       "  'this',\n",
       "  'year',\n",
       "  'needless',\n",
       "  'to',\n",
       "  'say',\n",
       "  'it',\n",
       "  'will',\n",
       "  'be',\n",
       "  'going',\n",
       "  'back'],\n",
       " ['more',\n",
       "  'and',\n",
       "  'more',\n",
       "  'i',\n",
       "  'find',\n",
       "  'myself',\n",
       "  'reliant',\n",
       "  'on',\n",
       "  'the',\n",
       "  'reviews',\n",
       "  'written',\n",
       "  'by',\n",
       "  'savvy',\n",
       "  'shoppers',\n",
       "  'before',\n",
       "  'me',\n",
       "  'and',\n",
       "  'for',\n",
       "  'the',\n",
       "  'most',\n",
       "  'past',\n",
       "  'they',\n",
       "  'are',\n",
       "  'right',\n",
       "  'on',\n",
       "  'in',\n",
       "  'their',\n",
       "  'estimation',\n",
       "  'of',\n",
       "  'the',\n",
       "  'product',\n",
       "  'in',\n",
       "  'the',\n",
       "  'case',\n",
       "  'of',\n",
       "  'this',\n",
       "  'dress',\n",
       "  'if',\n",
       "  'it',\n",
       "  'had',\n",
       "  'not',\n",
       "  'been',\n",
       "  'for',\n",
       "  'the',\n",
       "  'reveiws',\n",
       "  'i',\n",
       "  'doubt',\n",
       "  'i',\n",
       "  'would',\n",
       "  'have',\n",
       "  'even',\n",
       "  'tried',\n",
       "  'this',\n",
       "  'the',\n",
       "  'dress',\n",
       "  'is',\n",
       "  'beautifully',\n",
       "  'made',\n",
       "  'lined',\n",
       "  'and',\n",
       "  'reminiscent',\n",
       "  'of',\n",
       "  'the',\n",
       "  'old',\n",
       "  'retailer',\n",
       "  'quality',\n",
       "  'it',\n",
       "  'is',\n",
       "  'lined',\n",
       "  'in',\n",
       "  'the',\n",
       "  'solid',\n",
       "  'periwinkle',\n",
       "  'colored',\n",
       "  'fabric',\n",
       "  'that',\n",
       "  'matches',\n",
       "  'the',\n",
       "  'outer',\n",
       "  'fabric',\n",
       "  'print',\n",
       "  'tts',\n",
       "  'and',\n",
       "  'very',\n",
       "  'form',\n",
       "  'fitting',\n",
       "  'falls',\n",
       "  'just',\n",
       "  'above',\n",
       "  'the',\n",
       "  'knee',\n",
       "  'and',\n",
       "  'does',\n",
       "  'not',\n",
       "  'rid']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "punct = string.punctuation\n",
    "\n",
    "# Pour chaque token dans chaque avis, si le token n'est pas dans la liste des ponctuations, on le garde\n",
    "liste_avis_clean = [[token for token in avis if token not in punct] for avis in liste_avis_clean]\n",
    "liste_avis_clean[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traitement et séparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie, nous allons chercher à classifier les avis en fonction de leur note. \n",
    "\n",
    "Nous allons utiliser la colonne \"Rating\" comme étiquettes et \"Review Text\" comme valeurs. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sélection des informations dans notre dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons sélectionner les trois colonnes qui vont nous servir pour la classification dans un objectif d'optimiser les temps de calculs et de ne pas avoir d'informations superflus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>I love tracy reese dresses, but this one is no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>I aded this in my basket at hte last mintue to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  Rating                                        Review Text\n",
       "2   2       3  I had such high hopes for this dress and reall...\n",
       "3   3       5  I love, love, love this jumpsuit. it's fun, fl...\n",
       "4   4       5  This shirt is very flattering to all due to th...\n",
       "5   5       2  I love tracy reese dresses, but this one is no...\n",
       "6   6       5  I aded this in my basket at hte last mintue to..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On récupère la colonne id, Rating et Review Text\n",
    "data = data[[\"id\", \"Rating\", \"Review Text\"]]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse de la colonne \"Rating\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    10858\n",
       "4     4289\n",
       "3     2464\n",
       "2     1360\n",
       "1      691\n",
       "Name: Rating, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyse de la colonne \"Rating\"\n",
    "data[\"Rating\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons ici des notes allant de 1 à 5. Nous allons diviser ces valeurs en 3 catégories : \n",
    "- -1 pour les notes allant de 1 à 2\n",
    "- 0 pour les notes égales à 3 \n",
    "- 1 pour les plus élevées (4 et 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse de la colonne \"Review Text\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre colonne correspondant aux valeurs est \"Review Text\". \\\n",
    "Cette colonne contient tous les avis laissés par les internautes sur les différents vêtements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changement des étiquettes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour réaliser notre classification, nous allons donc modifier les étiquettes comme précisé ci-dessus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_label_to_numeric(label):\n",
    "    return 1 if label == 5 else 0 if label == 3 or label == 4 else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(data):\n",
    "    labels = data[[\"id\",\"Rating\"]]\n",
    "    labels['Rating'] = labels['Rating'].apply(map_label_to_numeric)\n",
    "    labels.set_index('id', inplace=True)\n",
    "    \n",
    "    # ajouter les labels dans data selon l'id\n",
    "    data['score_avis'] = labels\n",
    "\n",
    "    # data['score_avis'] = labels\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\orian\\AppData\\Local\\Temp\\ipykernel_36456\\29256686.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labels['Rating'] = labels['Rating'].apply(map_label_to_numeric)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>score_avis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>I love tracy reese dresses, but this one is no...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>I aded this in my basket at hte last mintue to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  Rating                                        Review Text  score_avis\n",
       "2   2       3  I had such high hopes for this dress and reall...           0\n",
       "3   3       5  I love, love, love this jumpsuit. it's fun, fl...           1\n",
       "4   4       5  This shirt is very flattering to all due to th...           1\n",
       "5   5       2  I love tracy reese dresses, but this one is no...          -1\n",
       "6   6       5  I aded this in my basket at hte last mintue to...           1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = get_labels(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    10858\n",
       " 0     6753\n",
       "-1     2051\n",
       "Name: score_avis, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On analyse la nouvelle colonne \"score_avis\"\n",
    "data[\"score_avis\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grâce à cette manipulation, nous pouvons observer que les avis ayant la note de 5 sont majoritaires dans notre jeu de données puisque cela correspond à la note de 1. Les avis compris entre 1 et 2 ont une proportion plus faible (-1). \n",
    "\n",
    "A présent, nous n'avons plus besoin de la colonne Rating, nous pouvons donc la supprimer du dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>score_avis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>I love tracy reese dresses, but this one is no...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>I aded this in my basket at hte last mintue to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                        Review Text  score_avis\n",
       "2   2  I had such high hopes for this dress and reall...           0\n",
       "3   3  I love, love, love this jumpsuit. it's fun, fl...           1\n",
       "4   4  This shirt is very flattering to all due to th...           1\n",
       "5   5  I love tracy reese dresses, but this one is no...          -1\n",
       "6   6  I aded this in my basket at hte last mintue to...           1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[[\"id\", \"Review Text\", \"score_avis\"]]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Division de notre dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour réaliser notre classification, nous avons besoin de séparer notre jeu de données en un jeu d'apprentissage, de validation et de test.\n",
    "\n",
    "Note :\n",
    "Les données en apprentissage automatique sont généralement séparées en trois jeux :\n",
    "+ **entraînement** : données destinées à l'apprentissage du modèle ;\n",
    "+ **validation** : données destinées à une évaluation intermédiaire du modèle pour permettre l'ajustement de ses hyperparamètres. Une fois les hyperparamètres du modèle arrêtés, on peut le ré-entraîner sur l'ensemble des données (entraînement + validation) avant de le tester sur le jeu de test ;\n",
    "+ **test** : données destinées EXCLUSIVEMENT à l'évaluation FINALE (à réaliser une fois uniquement !) du modèle choisi finalement. Elles ne doivent sous aucune forme servir à la conception du modèle. Il est donc interdit aussi bien de les examiner que d'évaluer le modèle en cours de développement sur ce jeu de données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour créer l'ensemble de validation, nous allons effectuer la manipulation à la fin du pré-traitement réalisé lors de la classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour diviser de notre jeu de données en 2 : train et test\n",
    "def split_data(data, train_ratio):\n",
    "    data_train = data.sample(frac = train_ratio)\n",
    "    data_test = data.drop(data_train.index)\n",
    "    return data_train, data_test\n",
    "\n",
    "# Diviser notre jeu de données en 2 : train et test\n",
    "data_train, data_test = split_data(data, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11797, 3), (7865, 3))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape, data_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans notre cas :\n",
    "+ entraînement (appelé *Train*) contenant 11797 observations ;\n",
    "+ validation (appelé *Validation*) contenant 5243 observations ;\n",
    "+ test (appelé *Test*), contenant 2622 observations, soit environ 22% de la taille du jeu d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>score_avis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23321</th>\n",
       "      <td>23321</td>\n",
       "      <td>I got this in the blue and love it. tts and th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>401</td>\n",
       "      <td>This dress is beautifully constructed. it is v...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16557</th>\n",
       "      <td>16557</td>\n",
       "      <td>This dress is lovely and easy to wear if you a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5018</th>\n",
       "      <td>5018</td>\n",
       "      <td>Picked this up end of season sale, but i go to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6552</th>\n",
       "      <td>6552</td>\n",
       "      <td>I really loved this skirt on the model. i orde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                        Review Text  score_avis\n",
       "23321  23321  I got this in the blue and love it. tts and th...           1\n",
       "401      401  This dress is beautifully constructed. it is v...           1\n",
       "16557  16557  This dress is lovely and easy to wear if you a...           1\n",
       "5018    5018  Picked this up end of season sale, but i go to...           1\n",
       "6552    6552  I really loved this skirt on the model. i orde...           1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>score_avis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>I love tracy reese dresses, but this one is no...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>I aded this in my basket at hte last mintue to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>I love this dress. i usually get an xs but it ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                        Review Text  score_avis\n",
       "3   3  I love, love, love this jumpsuit. it's fun, fl...           1\n",
       "4   4  This shirt is very flattering to all due to th...           1\n",
       "5   5  I love tracy reese dresses, but this one is no...          -1\n",
       "6   6  I aded this in my basket at hte last mintue to...           1\n",
       "8   8  I love this dress. i usually get an xs but it ...           1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution des classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est important de connaître la répartition des classes dans les données d'entraînement pour pouvoir procéder à notre classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1    6497\n",
      " 0    4077\n",
      "-1    1223\n",
      "Name: score_avis, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " 1    0.550733\n",
       " 0    0.345596\n",
       "-1    0.103670\n",
       "Name: score_avis, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyse de la colonne \"score_avis\" de notre jeu de données d'entrainement\n",
    "print(data_train[\"score_avis\"].value_counts())\n",
    "\n",
    "# Calcul des proportions de chaque classe dans notre jeu de données d'entrainement\n",
    "data_train[\"score_avis\"].value_counts()/len(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons observer que les classes de scores sont réparties de manière aléatoire dans notre jeu d'apprentissage. Nous pouvons noter plus de 50% d'avis très favorables, correspondant à la note de 5/5. Les avis négatifs sont en minorité dans notre jeu d'entraînement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration du texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour se faire une idée des textes auxquels nous avons affaire, nous allons les afficher pour savoir quels pré-traitements sont nécessaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['I got this in the blue and love it. tts and the shape is very unique and classy- better than shown on the model. it can be worn alone or with a long sleeve underneath as a layer. considering the neutral as well.',\n",
       "       'This dress is beautifully constructed. it is very fitted and does run on the small side. it will be a perfect derby dress. the lace is lovely and the pearl buttons are great touch. age appropriate for anyone..a very \\r\\npretty dress. simple, classic and gorgeous!',\n",
       "       \"This dress is lovely and easy to wear if you are short and you buy regular size (not petite). i am 5'1 and i bought the regular medium so the length of the front falls just above my knees. i can wear it to work without leggings. the other thing i love is it has pockets! and it fits generously - i weigh 140 lbs and will usually fit in a medium (if i don't breathe) and a large (if i use a belt). but in this size (medium regular - i can breathe! and i need a belt. ergo - i don't feel like i need to\",\n",
       "       \"Picked this up end of season sale, but i go to florida a lot so i will still be able to use it. it runs a bit large. i'm usually between a s and m at retailer and i'm a bit heavy right now but the small fit loose and comfortably. i would recommend sizing down if you are between sizes. not itchy but i would wear a cami underneath because it is white and could start to itch after a while i suppose. it's definitely not a soft flowy t. mix of t-shirt material and linen. the main part of the t is cotto\",\n",
       "       \"I really loved this skirt on the model. i ordered it in my typical 0 and it fit well. the green was adorable, and the buttons are rose gold, which is quite pretty. i did exchange this skirt for a 2, only because the 0 was just too short for me. if the 2 is a bit longer but not terribly huge in the waist, i will probably keep it. i was surprised by how large the a line felt, but i think i can get used to it. it really is very fun, and very easy to dress up or down. i'm really hoping the bigger si\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Affichage des 5 premiers avis\n",
    "data_train[\"Review Text\"].values[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_examples</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>1223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       num_examples\n",
       "class              \n",
       " 1             6497\n",
       " 0             4077\n",
       "-1             1223"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "class_distribution = (pd.DataFrame.from_dict(Counter(data_train.score_avis.values),\n",
    "                                             orient='index')\n",
    "                                  .rename(columns={0: 'num_examples'}))\n",
    "class_distribution.index.name = 'class'\n",
    "class_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_examples</th>\n",
       "      <th>perc_examples</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6497</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4077</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>1223</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       num_examples  perc_examples\n",
       "class                             \n",
       " 1             6497           0.55\n",
       " 0             4077           0.35\n",
       "-1             1223           0.10"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class_distribution['perc_examples'] = np.around(class_distribution.num_examples /\n",
    "                                                np.sum(class_distribution.num_examples),\n",
    "                                                2)\n",
    "class_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Représentation des textes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sélection de descripteurs : prétraitements textuels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemple sur un avis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Luv the loose fit and quality fabric - great piece for layering.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw = data_train['Review Text'].iloc[100]\n",
    "tw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour effectuer nos tâches de traitements de langage, nous allons transformer le 1er avis en utilisant spacy. En effet, spacy a plusieurs fonctionnalités : \n",
    "- permet de tokeniser directement notre texte\n",
    "- peut lemmatiser les mots\n",
    "- identifier et classer les entités nommées\n",
    "- analyser les dépendances syntaxiques entre les mots\n",
    "- représenter les mots sous forme de vecteurs (embedding)\n",
    "- etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\orian\\anaconda3\\lib\\site-packages (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy) (0.3.3)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\orian\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "\n",
      "⠙ Loading compatibility table...\n",
      "⠹ Loading compatibility table...\n",
      "\u001b[2K\u001b[38;5;2m✔ Loaded compatibility table\u001b[0m\n",
      "\u001b[1m\n",
      "================= Installed pipeline packages (spaCy v3.7.2) =================\u001b[0m\n",
      "\u001b[38;5;4mℹ spaCy installation:\n",
      "c:\\Users\\orian\\anaconda3\\Lib\\site-packages\\spacy\u001b[0m\n",
      "\n",
      "NAME              SPACY            VERSION                            \n",
      "en_core_web_sm    >=3.7.0,<3.8.0   \u001b[38;5;2m3.7.0\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "fr_core_news_sm   >=3.7.0,<3.8.0   \u001b[38;5;2m3.7.0\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.1/12.8 MB 2.3 MB/s eta 0:00:06\n",
      "      --------------------------------------- 0.2/12.8 MB 3.0 MB/s eta 0:00:05\n",
      "     - -------------------------------------- 0.4/12.8 MB 3.1 MB/s eta 0:00:04\n",
      "     - -------------------------------------- 0.6/12.8 MB 3.2 MB/s eta 0:00:04\n",
      "     -- ------------------------------------- 0.7/12.8 MB 3.2 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 1.0/12.8 MB 3.7 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 1.1/12.8 MB 3.6 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 1.5/12.8 MB 4.1 MB/s eta 0:00:03\n",
      "     ------ --------------------------------- 1.9/12.8 MB 4.7 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 2.3/12.8 MB 5.0 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 2.5/12.8 MB 5.2 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 2.9/12.8 MB 5.2 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 3.1/12.8 MB 5.2 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 3.3/12.8 MB 5.2 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 3.6/12.8 MB 5.2 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 5.2 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 3.9/12.8 MB 5.0 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 4.1/12.8 MB 4.9 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 4.2/12.8 MB 4.7 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 4.5/12.8 MB 5.0 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 4.8/12.8 MB 5.0 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 5.0/12.8 MB 5.0 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 5.3/12.8 MB 4.9 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 5.4/12.8 MB 4.9 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 5.6/12.8 MB 4.8 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 5.7/12.8 MB 4.7 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 5.9/12.8 MB 4.8 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 6.2/12.8 MB 4.8 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 6.4/12.8 MB 4.8 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 6.6/12.8 MB 4.8 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.9/12.8 MB 4.8 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 7.5/12.8 MB 5.0 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 7.8/12.8 MB 5.1 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 7.9/12.8 MB 5.0 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 8.3/12.8 MB 5.1 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 8.5/12.8 MB 5.0 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.7/12.8 MB 5.0 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.9/12.8 MB 5.0 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 5.1 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.8/12.8 MB 5.2 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.2/12.8 MB 5.3 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.5/12.8 MB 5.5 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.8/12.8 MB 5.6 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.3/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.7/12.8 MB 5.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.2/12.8 MB 5.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.5/12.8 MB 5.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 5.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.3)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\orian\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\orian\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Attempting uninstall: en-core-web-sm\n",
      "    Found existing installation: en-core-web-sm 3.7.0\n",
      "    Uninstalling en-core-web-sm-3.7.0:\n",
      "      Successfully uninstalled en-core-web-sm-3.7.0\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Pour la langue anglaise\n",
    "!pip install -U spacy\n",
    "! python -m spacy validate\n",
    "\n",
    "import spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "This dress in a lovely platinum is feminine and fits perfectly, easy to wear and comfy, too! highly recommend!"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformation du premier avis en objet spacy\n",
    "avis_nlp = nlp(avis) \n",
    "avis_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(avis_nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a bien un objet de type spacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On affiche chaque token de notre objet spacy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "dress\n",
      "in\n",
      "a\n",
      "lovely\n",
      "platinum\n",
      "is\n",
      "feminine\n",
      "and\n",
      "fits\n",
      "perfectly\n",
      ",\n",
      "easy\n",
      "to\n",
      "wear\n",
      "and\n",
      "comfy\n",
      ",\n",
      "too\n",
      "!\n",
      "highly\n",
      "recommend\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for token in avis_nlp:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous allons pouvoir afficher les lemmes de chaque token. Grâce à cette étape, nous allons pouvoir simplifier les mots pour faciliter notre analyse textuelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this\n",
      "dress\n",
      "in\n",
      "a\n",
      "lovely\n",
      "platinum\n",
      "be\n",
      "feminine\n",
      "and\n",
      "fit\n",
      "perfectly\n",
      ",\n",
      "easy\n",
      "to\n",
      "wear\n",
      "and\n",
      "comfy\n",
      ",\n",
      "too\n",
      "!\n",
      "highly\n",
      "recommend\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "# Lemmatisation\n",
    "for token in avis_nlp:\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Généralisation de la lemmatisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour lemmatiser notre texte, nous allons définir une fonction. Cette étape est indispensable pour récupérer les lemmes des mots de notre texte d'origine. Nous allons simplifier notre texte grâce à cette fonction.\n",
    "\n",
    "Cette fonction va nous permettre de généraliser par la suite nos manipulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatise_text(text):\n",
    "    text = nlp(text) # on transforme le texte en objet spacy\n",
    "    lemmas = [token.lemma_ for token in text] # on récupère les lemmes\n",
    "    return ' '.join(lemmas) # on retourne les lemmes sous forme de texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avis initial :  I had such high hopes for this dress and really wanted it to work for me. i initially ordered the petite small (my usual size) but i found this to be outrageously small. so small in fact that i could not zip it up! i reordered it in petite medium, which was just ok. overall, the top half was comfortable and fit nicely, but the bottom half had a very tight under layer and several somewhat cheap (net) over layers. imo, a major design flaw was the net over layer sewn directly into the zipper - it c\n",
      "Avis lemmatisé :  I have such high hope for this dress and really want it to work for I . I initially order the petite small ( my usual size ) but I find this to be outrageously small . so small in fact that I could not zip it up ! I reorder it in petite medium , which be just ok . overall , the top half be comfortable and fit nicely , but the bottom half have a very tight under layer and several somewhat cheap ( net ) over layer . imo , a major design flaw be the net over layer sew directly into the zipper - it c\n",
      "Avis initial :  I love, love, love this jumpsuit. it's fun, flirty, and fabulous! every time i wear it, i get nothing but great compliments!\n",
      "Avis lemmatisé :  I love , love , love this jumpsuit . it be fun , flirty , and fabulous ! every time I wear it , I get nothing but great compliment !\n",
      "Avis initial :  This shirt is very flattering to all due to the adjustable front tie. it is the perfect length to wear with leggings and it is sleeveless so it pairs well with any cardigan. love this shirt!!!\n",
      "Avis lemmatisé :  this shirt be very flattering to all due to the adjustable front tie . it be the perfect length to wear with legging and it be sleeveless so it pair well with any cardigan . love this shirt ! ! !\n"
     ]
    }
   ],
   "source": [
    "# On teste sur 3 avis \n",
    "for avis in liste_avis[:3]:\n",
    "    print(\"Avis initial : \", avis) # On affiche l'avis initial\n",
    "    print(\"Avis lemmatisé : \", lemmatise_text(avis)) # On applique la fonction à notre avis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons ensuite ajouter une colonne avec la fonction de **lemmatisation** appliquée à nos avis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['lemmas'] = data_train['Review Text'].apply(lemmatise_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>score_avis</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23321</th>\n",
       "      <td>23321</td>\n",
       "      <td>I got this in the blue and love it. tts and th...</td>\n",
       "      <td>1</td>\n",
       "      <td>I get this in the blue and love it . tts and t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>401</td>\n",
       "      <td>This dress is beautifully constructed. it is v...</td>\n",
       "      <td>1</td>\n",
       "      <td>this dress be beautifully construct . it be ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16557</th>\n",
       "      <td>16557</td>\n",
       "      <td>This dress is lovely and easy to wear if you a...</td>\n",
       "      <td>1</td>\n",
       "      <td>this dress be lovely and easy to wear if you b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5018</th>\n",
       "      <td>5018</td>\n",
       "      <td>Picked this up end of season sale, but i go to...</td>\n",
       "      <td>1</td>\n",
       "      <td>pick this up end of season sale , but I go to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6552</th>\n",
       "      <td>6552</td>\n",
       "      <td>I really loved this skirt on the model. i orde...</td>\n",
       "      <td>1</td>\n",
       "      <td>I really love this skirt on the model . I orde...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                        Review Text  score_avis  \\\n",
       "23321  23321  I got this in the blue and love it. tts and th...           1   \n",
       "401      401  This dress is beautifully constructed. it is v...           1   \n",
       "16557  16557  This dress is lovely and easy to wear if you a...           1   \n",
       "5018    5018  Picked this up end of season sale, but i go to...           1   \n",
       "6552    6552  I really loved this skirt on the model. i orde...           1   \n",
       "\n",
       "                                                  lemmas  \n",
       "23321  I get this in the blue and love it . tts and t...  \n",
       "401    this dress be beautifully construct . it be ve...  \n",
       "16557  this dress be lovely and easy to wear if you b...  \n",
       "5018   pick this up end of season sale , but I go to ...  \n",
       "6552   I really love this skirt on the model . I orde...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On effectue la même manipulation sur l'ensemble de test ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['lemmas'] = data_test['Review Text'].apply(lemmatise_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7865, 4)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde\n",
    "data_train.to_pickle('train.pkl')\n",
    "data_test.to_pickle('test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Racines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour réduire les mots à leur forme de base, nous allons utiliser SnowballStemmer sur notre texte. Pour cela, nous allons créer une fonction et l'appliquer à nos avis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text(text):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    stems = [stemmer.stem(token) for token in tokenizer.tokenize(text)]\n",
    "    return ' '.join(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this shirt is veri flatter to all due to the adjust front tie it is the perfect length to wear with leg and it is sleeveless so it pair well with ani cardigan love this shirt'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_text(avis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On applique la fonction à notre dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['stems'] = data_train['Review Text'].apply(stem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>score_avis</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>stems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23321</th>\n",
       "      <td>23321</td>\n",
       "      <td>I got this in the blue and love it. tts and th...</td>\n",
       "      <td>1</td>\n",
       "      <td>I get this in the blue and love it . tts and t...</td>\n",
       "      <td>i got this in the blue and love it tts and the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>401</td>\n",
       "      <td>This dress is beautifully constructed. it is v...</td>\n",
       "      <td>1</td>\n",
       "      <td>this dress be beautifully construct . it be ve...</td>\n",
       "      <td>this dress is beauti construct it is veri fit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16557</th>\n",
       "      <td>16557</td>\n",
       "      <td>This dress is lovely and easy to wear if you a...</td>\n",
       "      <td>1</td>\n",
       "      <td>this dress be lovely and easy to wear if you b...</td>\n",
       "      <td>this dress is love and easi to wear if you are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5018</th>\n",
       "      <td>5018</td>\n",
       "      <td>Picked this up end of season sale, but i go to...</td>\n",
       "      <td>1</td>\n",
       "      <td>pick this up end of season sale , but I go to ...</td>\n",
       "      <td>pick this up end of season sale but i go to fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6552</th>\n",
       "      <td>6552</td>\n",
       "      <td>I really loved this skirt on the model. i orde...</td>\n",
       "      <td>1</td>\n",
       "      <td>I really love this skirt on the model . I orde...</td>\n",
       "      <td>i realli love this skirt on the model i order ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                        Review Text  score_avis  \\\n",
       "23321  23321  I got this in the blue and love it. tts and th...           1   \n",
       "401      401  This dress is beautifully constructed. it is v...           1   \n",
       "16557  16557  This dress is lovely and easy to wear if you a...           1   \n",
       "5018    5018  Picked this up end of season sale, but i go to...           1   \n",
       "6552    6552  I really loved this skirt on the model. i orde...           1   \n",
       "\n",
       "                                                  lemmas  \\\n",
       "23321  I get this in the blue and love it . tts and t...   \n",
       "401    this dress be beautifully construct . it be ve...   \n",
       "16557  this dress be lovely and easy to wear if you b...   \n",
       "5018   pick this up end of season sale , but I go to ...   \n",
       "6552   I really love this skirt on the model . I orde...   \n",
       "\n",
       "                                                   stems  \n",
       "23321  i got this in the blue and love it tts and the...  \n",
       "401    this dress is beauti construct it is veri fit ...  \n",
       "16557  this dress is love and easi to wear if you are...  \n",
       "5018   pick this up end of season sale but i go to fl...  \n",
       "6552   i realli love this skirt on the model i order ...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fait de même sur l'ensemble de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['stems'] = data_test['Review Text'].apply(stem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7865, 5)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde\n",
    "data_train.to_pickle('train.pkl')\n",
    "data_test.to_pickle('test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Étiquettes morphosyntaxiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis, nous allons analyser notre texte et renvoyer chaque mot remplacé par sa catégorie grammaticale pour continuer l'étude des avis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_words_with_pos_tag(text):\n",
    "    text = nlp(text)\n",
    "    return ' '.join([token.pos_ for token in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On teste la fonction sur un avis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DET NOUN AUX ADV ADJ ADP PRON ADP ADP DET ADJ ADJ NOUN PUNCT PRON AUX DET ADJ NOUN PART VERB ADP NOUN CCONJ PRON AUX ADJ SCONJ PRON VERB ADV ADP DET NOUN PUNCT VERB DET NOUN PUNCT PUNCT PUNCT'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_words_with_pos_tag(avis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On applique la fonction à notre ensemble d'entrainement et on ajoute une colonne à notre dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['pos'] = data_train['Review Text'].apply(replace_words_with_pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>score_avis</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>stems</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23321</th>\n",
       "      <td>23321</td>\n",
       "      <td>I got this in the blue and love it. tts and th...</td>\n",
       "      <td>1</td>\n",
       "      <td>I get this in the blue and love it . tts and t...</td>\n",
       "      <td>i got this in the blue and love it tts and the...</td>\n",
       "      <td>PRON VERB PRON ADP DET NOUN CCONJ VERB PRON PU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>401</td>\n",
       "      <td>This dress is beautifully constructed. it is v...</td>\n",
       "      <td>1</td>\n",
       "      <td>this dress be beautifully construct . it be ve...</td>\n",
       "      <td>this dress is beauti construct it is veri fit ...</td>\n",
       "      <td>DET NOUN AUX ADV VERB PUNCT PRON AUX ADV VERB ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16557</th>\n",
       "      <td>16557</td>\n",
       "      <td>This dress is lovely and easy to wear if you a...</td>\n",
       "      <td>1</td>\n",
       "      <td>this dress be lovely and easy to wear if you b...</td>\n",
       "      <td>this dress is love and easi to wear if you are...</td>\n",
       "      <td>DET NOUN AUX ADJ CCONJ ADJ PART VERB SCONJ PRO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5018</th>\n",
       "      <td>5018</td>\n",
       "      <td>Picked this up end of season sale, but i go to...</td>\n",
       "      <td>1</td>\n",
       "      <td>pick this up end of season sale , but I go to ...</td>\n",
       "      <td>pick this up end of season sale but i go to fl...</td>\n",
       "      <td>VERB PRON ADP NOUN ADP NOUN NOUN PUNCT CCONJ P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6552</th>\n",
       "      <td>6552</td>\n",
       "      <td>I really loved this skirt on the model. i orde...</td>\n",
       "      <td>1</td>\n",
       "      <td>I really love this skirt on the model . I orde...</td>\n",
       "      <td>i realli love this skirt on the model i order ...</td>\n",
       "      <td>PRON ADV VERB DET NOUN ADP DET NOUN PUNCT PRON...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                        Review Text  score_avis  \\\n",
       "23321  23321  I got this in the blue and love it. tts and th...           1   \n",
       "401      401  This dress is beautifully constructed. it is v...           1   \n",
       "16557  16557  This dress is lovely and easy to wear if you a...           1   \n",
       "5018    5018  Picked this up end of season sale, but i go to...           1   \n",
       "6552    6552  I really loved this skirt on the model. i orde...           1   \n",
       "\n",
       "                                                  lemmas  \\\n",
       "23321  I get this in the blue and love it . tts and t...   \n",
       "401    this dress be beautifully construct . it be ve...   \n",
       "16557  this dress be lovely and easy to wear if you b...   \n",
       "5018   pick this up end of season sale , but I go to ...   \n",
       "6552   I really love this skirt on the model . I orde...   \n",
       "\n",
       "                                                   stems  \\\n",
       "23321  i got this in the blue and love it tts and the...   \n",
       "401    this dress is beauti construct it is veri fit ...   \n",
       "16557  this dress is love and easi to wear if you are...   \n",
       "5018   pick this up end of season sale but i go to fl...   \n",
       "6552   i realli love this skirt on the model i order ...   \n",
       "\n",
       "                                                     pos  \n",
       "23321  PRON VERB PRON ADP DET NOUN CCONJ VERB PRON PU...  \n",
       "401    DET NOUN AUX ADV VERB PUNCT PRON AUX ADV VERB ...  \n",
       "16557  DET NOUN AUX ADJ CCONJ ADJ PART VERB SCONJ PRO...  \n",
       "5018   VERB PRON ADP NOUN ADP NOUN NOUN PUNCT CCONJ P...  \n",
       "6552   PRON ADV VERB DET NOUN ADP DET NOUN PUNCT PRON...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On effectue la même manipulation sur notre ensemble de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['pos'] = data_test['Review Text'].apply(replace_words_with_pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7865, 6)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde\n",
    "data_train.to_pickle('train.pkl')\n",
    "data_test.to_pickle('test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Classe d'appartenance des entités nommées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour pouvoir effectuer la reconnaissances d'entités nommées sur nos avis, nous allons retourner une version de notre avis ou chaque entité nommée est remplacée par son type d'entité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner(text):\n",
    "\n",
    "    text = nlp(text) # on transforme le texte en objet spacy\n",
    "    \n",
    "    new_text = [] # on crée une liste vide\n",
    "\n",
    "    for token in text: # pour chaque token dans l'avis\n",
    "\n",
    "        # print(token.text, token.ent_iob_, token.ent_type_)\n",
    "        \n",
    "        if token.ent_iob_ == \"O\": # si l'entité ne fait pas partie d'une entité nommée\n",
    "            new_text.append(token.text) # on ajoute le texte du token à la liste\n",
    "        elif token.ent_iob_ == \"B\": # si l'entité fait partie d'une entité nommée\n",
    "            new_text.append(token.ent_type_) # on ajoute le type de l'entité à la liste\n",
    "\n",
    "        # Si l'entité comprend plusieurs mot on ne répète pas l'étiquette\n",
    "        else:\n",
    "            continue\n",
    "    return ' '.join(new_text) # on retourne les étiquettes sous forme de texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On applique la fonction sur un avis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avis initiaux :  This shirt is very flattering to all due to the adjustable front tie. it is the perfect length to wear with leggings and it is sleeveless so it pairs well with any cardigan. love this shirt!!!\n",
      "Avec les entités nommées :  This shirt is very flattering to all due to the adjustable front tie . it is the perfect length to wear with leggings and it is sleeveless so it pairs well with any cardigan . love this shirt ! ! !\n"
     ]
    }
   ],
   "source": [
    "print(\"Avis initiaux : \", avis)\n",
    "print(\"Avec les entités nommées : \", ner(avis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On applique ensuite notre fonction à notre dataframe d'entrainement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['entites_nommees'] = data_train['Review Text'].apply(ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>score_avis</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>stems</th>\n",
       "      <th>pos</th>\n",
       "      <th>entites_nommees</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23321</th>\n",
       "      <td>23321</td>\n",
       "      <td>I got this in the blue and love it. tts and th...</td>\n",
       "      <td>1</td>\n",
       "      <td>I get this in the blue and love it . tts and t...</td>\n",
       "      <td>i got this in the blue and love it tts and the...</td>\n",
       "      <td>PRON VERB PRON ADP DET NOUN CCONJ VERB PRON PU...</td>\n",
       "      <td>I got this in the blue and love it . tts and t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>401</td>\n",
       "      <td>This dress is beautifully constructed. it is v...</td>\n",
       "      <td>1</td>\n",
       "      <td>this dress be beautifully construct . it be ve...</td>\n",
       "      <td>this dress is beauti construct it is veri fit ...</td>\n",
       "      <td>DET NOUN AUX ADV VERB PUNCT PRON AUX ADV VERB ...</td>\n",
       "      <td>This dress is beautifully constructed . it is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16557</th>\n",
       "      <td>16557</td>\n",
       "      <td>This dress is lovely and easy to wear if you a...</td>\n",
       "      <td>1</td>\n",
       "      <td>this dress be lovely and easy to wear if you b...</td>\n",
       "      <td>this dress is love and easi to wear if you are...</td>\n",
       "      <td>DET NOUN AUX ADJ CCONJ ADJ PART VERB SCONJ PRO...</td>\n",
       "      <td>This dress is lovely and easy to wear if you a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5018</th>\n",
       "      <td>5018</td>\n",
       "      <td>Picked this up end of season sale, but i go to...</td>\n",
       "      <td>1</td>\n",
       "      <td>pick this up end of season sale , but I go to ...</td>\n",
       "      <td>pick this up end of season sale but i go to fl...</td>\n",
       "      <td>VERB PRON ADP NOUN ADP NOUN NOUN PUNCT CCONJ P...</td>\n",
       "      <td>Picked this up end of season sale , but i go t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6552</th>\n",
       "      <td>6552</td>\n",
       "      <td>I really loved this skirt on the model. i orde...</td>\n",
       "      <td>1</td>\n",
       "      <td>I really love this skirt on the model . I orde...</td>\n",
       "      <td>i realli love this skirt on the model i order ...</td>\n",
       "      <td>PRON ADV VERB DET NOUN ADP DET NOUN PUNCT PRON...</td>\n",
       "      <td>I really loved this skirt on the model . i ord...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                        Review Text  score_avis  \\\n",
       "23321  23321  I got this in the blue and love it. tts and th...           1   \n",
       "401      401  This dress is beautifully constructed. it is v...           1   \n",
       "16557  16557  This dress is lovely and easy to wear if you a...           1   \n",
       "5018    5018  Picked this up end of season sale, but i go to...           1   \n",
       "6552    6552  I really loved this skirt on the model. i orde...           1   \n",
       "\n",
       "                                                  lemmas  \\\n",
       "23321  I get this in the blue and love it . tts and t...   \n",
       "401    this dress be beautifully construct . it be ve...   \n",
       "16557  this dress be lovely and easy to wear if you b...   \n",
       "5018   pick this up end of season sale , but I go to ...   \n",
       "6552   I really love this skirt on the model . I orde...   \n",
       "\n",
       "                                                   stems  \\\n",
       "23321  i got this in the blue and love it tts and the...   \n",
       "401    this dress is beauti construct it is veri fit ...   \n",
       "16557  this dress is love and easi to wear if you are...   \n",
       "5018   pick this up end of season sale but i go to fl...   \n",
       "6552   i realli love this skirt on the model i order ...   \n",
       "\n",
       "                                                     pos  \\\n",
       "23321  PRON VERB PRON ADP DET NOUN CCONJ VERB PRON PU...   \n",
       "401    DET NOUN AUX ADV VERB PUNCT PRON AUX ADV VERB ...   \n",
       "16557  DET NOUN AUX ADJ CCONJ ADJ PART VERB SCONJ PRO...   \n",
       "5018   VERB PRON ADP NOUN ADP NOUN NOUN PUNCT CCONJ P...   \n",
       "6552   PRON ADV VERB DET NOUN ADP DET NOUN PUNCT PRON...   \n",
       "\n",
       "                                         entites_nommees  \n",
       "23321  I got this in the blue and love it . tts and t...  \n",
       "401    This dress is beautifully constructed . it is ...  \n",
       "16557  This dress is lovely and easy to wear if you a...  \n",
       "5018   Picked this up end of season sale , but i go t...  \n",
       "6552   I really loved this skirt on the model . i ord...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aussi, on applique à notre ensemble de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['entites_nommees'] = data_test['Review Text'].apply(ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7865, 7)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.to_pickle('train.pkl')\n",
    "data_test.to_pickle('test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Calcul des valeurs des descripteurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour procéder aux calculs, nous allons séparer notre jeu de données d'entraînement pour avoir un jeu de données de validation. Les données test nou servirons pour l'évaluation finale des modèles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(data_train['Review Text'],\n",
    "                                                      data_train['score_avis'],\n",
    "                                                      train_size=0.75,\n",
    "                                                      random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8847,), (2950,))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a donc 8 847 lignes dans notre jeu d'entrainement et 2 950 dans celui de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14415    0\n",
       "18614    1\n",
       "10119   -1\n",
       "7066     1\n",
       "5533     1\n",
       "        ..\n",
       "12071    1\n",
       "5322     0\n",
       "22105    1\n",
       "18666    1\n",
       "9404     0\n",
       "Name: score_avis, Length: 8847, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons observer que les sorties à prédire correspondent aux trois étiquettes que nous avons défini plus haut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour évaluer notre modèle, nous initialisons les ensembles de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère les avis et les labels du jeu de données de test\n",
    "X_test, y_test = data_test['Review Text'], data_test['score_avis'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binaire : présence/absence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bin_count = CountVectorizer(binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(binary=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(binary=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(binary=True)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_count.fit(X_train)\n",
    "bin_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8847x9755 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 388770 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized_bin = bin_count.transform(X_train)\n",
    "X_train_vectorized_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_vectorized_bin = bin_count.transform(X_valid)\n",
    "X_test_vectorized_bin = bin_count.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2950x9755 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 128506 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid_vectorized_bin # MEME NOMBRE DE COLONNES QUE X_train_vectorized_bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Numérique discret : décomptes d'occurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons calculer les fréquences d'occurence des termes dans nos avis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_count = CountVectorizer().fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons examiner le vocabulaire de nos avis : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '00p', '02', '03', '04', '06', '0in', '0p', '0petite', '0r',\n",
       "       '0verall', '10', '100', '100lb', '100lbs', '101', '102', '102lbs',\n",
       "       '103', '104', '105', '105lb', '105lbs', '106', '106lbs', '107',\n",
       "       '107lbs', '108', '109', '109lbs', '10lbs', '10p', '10s', '10th',\n",
       "       '10x', '11', '110', '110lbs', '111', '111lbs', '112', '112lb',\n",
       "       '112lbs', '113', '113lbs', '114', '114lb', '114lbs', '115',\n",
       "       '115ish'], dtype=object)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_count.get_feature_names_out()[:50] # 50 premiers mots (\"types\" du vocabulaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['yellowish', 'yellows', 'yellowy', 'yep', 'yes', 'yesterday',\n",
       "       'yesteryear', 'yet', 'yey', 'yfit', 'yielded', 'yikes', 'yippee',\n",
       "       'yo', 'yoga', 'yogini', 'yoke', 'york', 'you', 'young', 'younger',\n",
       "       'your', 'yourself', 'youth', 'youthful', 'yr', 'yuck', 'yucky',\n",
       "       'yummy', 'yup', 'zermatt', 'zero', 'zeros', 'zigzag', 'zillion',\n",
       "       'zip', 'zipepr', 'ziploc', 'zipped', 'zipper', 'zippered',\n",
       "       'zippers', 'zipping', 'zips', 'zombie', 'zone', 'zoolander',\n",
       "       'zoom', 'zooming', 'zuma'], dtype=object)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_count.get_feature_names_out()[-50:] # 50 derniers mots (\"types\" du vocabulaire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taille de notre vocabulaire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9755"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect_count.get_feature_names_out()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création matrice document-termes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons créer la matrice document-termes avec le même vectoriseur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8847x9755 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 388770 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized_count = vect_count.transform(X_train)\n",
    "X_train_vectorized_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_vectorized_count = vect_count.transform(X_valid)\n",
    "X_test_vectorized_count = vect_count.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A présent, nous allons prendre en compte les bi-grammes dans notre vocabulaire. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_count_bigrams = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\n",
    "X_train_vectorized_count_bigrams = vect_count_bigrams.transform(X_train)\n",
    "X_valid_vectorized_count_bigrams = vect_count_bigrams.transform(X_valid)\n",
    "X_test_vectorized_count_bigrams = vect_count_bigrams.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17555"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect_count_bigrams.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons presque 2 fois plus de vocabulaire avec inclusion des bigrammes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Numérique continu : TF-IDF (ou autres pondérations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons limiter le vocabulaire à des termes qui apparaissent au moins 5 fois dans le document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_tfidf = TfidfVectorizer(min_df=5).fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9755, 3249)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect_count.get_feature_names_out()), len(vect_tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La réduction de la taille du vocadulaire est importante et est due au paramètre min_df=5 : on a quasiment 3 fois moins de termes !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons vectoriser les jeux de données. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorisation des corpus d'entrainement, de validation et de test\n",
    "X_train_vectorized_tfidf = vect_tfidf.transform(X_train)\n",
    "X_valid_vectorized_tfidf = vect_tfidf.transform(X_valid)\n",
    "X_test_vectorized_tfidf = vect_tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification des textes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons réaliser une classification en utilisant plusieurs modèles afin de comparer les performances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèles de référence faibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choix aléatoire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons d'abord choisir un modèle où toutes les classes ont la même probabilité d'être choisies ou bien le prédicteur respecte la disctribution des classes dans les données d'entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prédiction proportionnelle à la distribution des classes dans les données d'entraînement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_prop_class = DummyClassifier(strategy='stratified').fit(X_train_vectorized_tfidf,\n",
    "                                                               y_train)\n",
    "predictions_valid = random_prop_class.predict(X_valid_vectorized_tfidf)\n",
    "conf_mat = confusion_matrix(y_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 41  93 158]\n",
      " [ 99 346 587]\n",
      " [177 608 841]]\n"
     ]
    }
   ],
   "source": [
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prédiction uniforme : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1, -1, ..., -1, -1,  0], dtype=int64)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_uniform = DummyClassifier(strategy='uniform').fit(X_train_vectorized_tfidf,\n",
    "                                                         y_train)\n",
    "predictions_valid = random_uniform.predict(X_valid_vectorized_tfidf)\n",
    "predictions_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101  91 100]\n",
      " [345 356 331]\n",
      " [571 485 570]]\n"
     ]
    }
   ],
   "source": [
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.348135593220339"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.10      0.35      0.15       292\n",
      "           0       0.38      0.34      0.36      1032\n",
      "           1       0.57      0.35      0.43      1626\n",
      "\n",
      "    accuracy                           0.35      2950\n",
      "   macro avg       0.35      0.35      0.32      2950\n",
      "weighted avg       0.46      0.35      0.38      2950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, predictions_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prédiction constante de la classe majoritaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons d'abord identifier la répartition des classes dans les données d'entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_examples</th>\n",
       "      <th>perc_examples</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6497</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4077</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>1223</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       num_examples  perc_examples\n",
       "class                             \n",
       " 1             6497           0.55\n",
       " 0             4077           0.35\n",
       "-1             1223           0.10"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maj = DummyClassifier(strategy='most_frequent').fit(X_train_vectorized_tfidf, y_train)\n",
    "predictions_valid = maj.predict(X_valid_vectorized_tfidf)\n",
    "predictions_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "maj_class = (class_distribution.index[class_distribution.perc_examples ==\n",
    "                                      np.amax(class_distribution.perc_examples)][0])\n",
    "maj_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(predictions_valid == maj_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5511864406779661"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maj.score(X_valid_vectorized_tfidf, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00       292\n",
      "           0       0.00      0.00      0.00      1032\n",
      "           1       0.55      1.00      0.71      1626\n",
      "\n",
      "    accuracy                           0.55      2950\n",
      "   macro avg       0.18      0.33      0.24      2950\n",
      "weighted avg       0.30      0.55      0.39      2950\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\orian\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\orian\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\orian\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, predictions_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifieur naïf bayésien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nb = MultinomialNB().fit(X_train_vectorized_tfidf, y_train)\n",
    "predictions_valid = model_nb.predict(X_valid_vectorized_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6884745762711865"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.50      0.01      0.02       292\n",
      "           0       0.61      0.49      0.54      1032\n",
      "           1       0.72      0.94      0.81      1626\n",
      "\n",
      "    accuracy                           0.69      2950\n",
      "   macro avg       0.61      0.48      0.46      2950\n",
      "weighted avg       0.66      0.69      0.64      2950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, predictions_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\orian\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model_lr = LogisticRegression(multi_class='multinomial', solver='lbfgs',\n",
    "                              max_iter=200).fit(X_train_vectorized_count, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_valid = model_lr.predict(X_valid_vectorized_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.683728813559322"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.50      0.42      0.46       292\n",
      "           0       0.58      0.55      0.56      1032\n",
      "           1       0.77      0.82      0.79      1626\n",
      "\n",
      "    accuracy                           0.68      2950\n",
      "   macro avg       0.61      0.60      0.60      2950\n",
      "weighted avg       0.68      0.68      0.68      2950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, predictions_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_n_strongly_associated_features(vectoriser, model, n):\n",
    "    feature_names = np.array(vectoriser.get_feature_names_out())\n",
    "\n",
    "    for i in range(3):\n",
    "        class_name = model.classes_[i]\n",
    "        print(\"CLASSE {}\".format(class_name))\n",
    "        idx_coefs_sorted = model.coef_[i].argsort() # ordre croissant\n",
    "        print(\"Les dix variables ayant l'association négative la plus forte \" + \n",
    "              \"avec la classe {} :\\n{}\\n\".format(class_name,\n",
    "                                                 feature_names[idx_coefs_sorted[:n]]))\n",
    "        idx_coefs_sorted = idx_coefs_sorted[::-1] # ordre décroissant\n",
    "        print(\"Les dix variables ayant l'association positive la plus forte \" +\n",
    "              \"avec la classe {} :\\n{}\\n\"\n",
    "              .format(class_name,\n",
    "                      feature_names[idx_coefs_sorted[:n]]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examinons les variables (termes) ayant l'association la plus forte avec chaque classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSE -1\n",
      "Les dix variables ayant l'association négative la plus forte avec la classe -1 :\n",
      "['sold' 'compliments' 'linen' 'sometimes' 'little' 'amazing' 'beautifully'\n",
      " 'dressed' 'detailing' 'worried']\n",
      "\n",
      "Les dix variables ayant l'association positive la plus forte avec la classe -1 :\n",
      "['bummer' 'idea' 'disappointing' 'money' 'nothing' 'horrible' 'shapeless'\n",
      " 'awkward' 'cheap' 'disappointment']\n",
      "\n",
      "\n",
      "CLASSE 0\n",
      "Les dix variables ayant l'association négative la plus forte avec la classe 0 :\n",
      "['thanks' 'transparent' 'happier' 'choose' 'gauze' 'bulges' 'use' '36dd'\n",
      " 'sequins' '110lbs']\n",
      "\n",
      "Les dix variables ayant l'association positive la plus forte avec la classe 0 :\n",
      "['washer' 'sharp' 'complaint' 'lint' 'possible' 'replacement' 'altered'\n",
      " 'suited' 'undergarments' 'display']\n",
      "\n",
      "\n",
      "CLASSE 1\n",
      "Les dix variables ayant l'association négative la plus forte avec la classe 1 :\n",
      "['disappointing' 'returning' 'unlined' 'bummer' 'replacement' 'idea'\n",
      " 'raise' 'unflattering' 'disappointed' 'imagine']\n",
      "\n",
      "Les dix variables ayant l'association positive la plus forte avec la classe 1 :\n",
      "['stunning' 'choose' 'highly' '36dd' 'bulges' 'draped' 'amazing'\n",
      " 'compliments' 'linen' 'concerned']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_n_strongly_associated_features(vect_count, model_lr, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMMENTAIRE A METTRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\orian\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model_lr = LogisticRegression(multi_class='multinomial',\n",
    "                              solver='lbfgs').fit(X_train_vectorized_tfidf, y_train)\n",
    "predictions_valid = model_lr.predict(X_valid_vectorized_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7142372881355932"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.61      0.29      0.40       292\n",
      "           0       0.61      0.59      0.60      1032\n",
      "           1       0.78      0.87      0.82      1626\n",
      "\n",
      "    accuracy                           0.71      2950\n",
      "   macro avg       0.67      0.58      0.61      2950\n",
      "weighted avg       0.70      0.71      0.70      2950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, predictions_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF le moins élevé : ['pros' 'wi' 'pu' 'gauzy' 'computer' 'li' 'secondly' 'complement' 'fo'\n",
      " 'xspetite']\n",
      "TF-IDF le plus élevé : ['birds' 'structure' 'loved' 'wonderful' 'simple' 'exciting' 'awesome'\n",
      " 'royal' 'she' 'cute']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vect_tfidf.get_feature_names_out())\n",
    "idx_tfidf_sorted = X_train_vectorized_tfidf.max(0).toarray()[0].argsort()\n",
    "print(\"TF-IDF le moins élevé : {}\".format(feature_names[idx_tfidf_sorted[:10]]))\n",
    "print(\"TF-IDF le plus élevé : {}\".format(feature_names[idx_tfidf_sorted[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous faisons avec les mêmes paramètres mais avec le vectoriseur à unigrammes et bigrammes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression(multi_class='multinomial', solver='lbfgs',max_iter=500).fit(X_train_vectorized_count_bigrams, y_train)\n",
    "predictions_valid = model_lr.predict(X_valid_vectorized_count_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6996610169491525"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.52      0.40      0.45       292\n",
      "           0       0.60      0.57      0.58      1032\n",
      "           1       0.78      0.84      0.81      1626\n",
      "\n",
      "    accuracy                           0.70      2950\n",
      "   macro avg       0.63      0.60      0.61      2950\n",
      "weighted avg       0.69      0.70      0.69      2950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, predictions_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSE -1\n",
      "Les dix variables ayant l'association négative la plus forte avec la classe -1 :\n",
      "['little' 'beautiful' 'soft' 'comfortable' 'love' 'compliments' 'the back'\n",
      " 'fit and' 'think' 'not too']\n",
      "\n",
      "Les dix variables ayant l'association positive la plus forte avec la classe -1 :\n",
      "['unflattering' 'shapeless' 'top but' 'didn work' 'cheap' 'bummer'\n",
      " 'nothing' 'return' 'huge' 'awful']\n",
      "\n",
      "\n",
      "CLASSE 0\n",
      "Les dix variables ayant l'association négative la plus forte avec la classe 0 :\n",
      "['ordered an' 'cute in' 'baby' 'absolutely' 'big so' 'just the'\n",
      " 'underwear' 'and didn' 'use' 'wish there']\n",
      "\n",
      "Les dix variables ayant l'association positive la plus forte avec la classe 0 :\n",
      "['cute dress' 'not flattering' 'altered' 'this fit' 'little too' 'however'\n",
      " 'sharp' 'torso' 'like the' 'didn fit']\n",
      "\n",
      "\n",
      "CLASSE 1\n",
      "Les dix variables ayant l'association négative la plus forte avec la classe 1 :\n",
      "['to love' 'however' 'returning' 'unflattering' 'not flattering'\n",
      " 'cute dress' 'going back' 'beautiful but' 'disappointing' 'wouldn']\n",
      "\n",
      "Les dix variables ayant l'association positive la plus forte avec la classe 1 :\n",
      "['big so' 'amazing' 'stunning' 'highly' 'even better' 'comfortable'\n",
      " 'beautiful' 'baby' 'but still' 'love this']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_n_strongly_associated_features(vect_count_bigrams, model_lr, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svm = SVC(kernel='linear', C=0.1).fit(X_train_vectorized_count_bigrams, y_train)\n",
    "predictions_valid = model_svm.predict(X_valid_vectorized_count_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6935593220338983"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.46      0.40      0.43       292\n",
      "           0       0.59      0.55      0.57      1032\n",
      "           1       0.79      0.83      0.81      1626\n",
      "\n",
      "    accuracy                           0.69      2950\n",
      "   macro avg       0.61      0.60      0.60      2950\n",
      "weighted avg       0.69      0.69      0.69      2950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, predictions_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
